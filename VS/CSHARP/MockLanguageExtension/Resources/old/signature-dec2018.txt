;--------------------------------------------------------
GENERAL	AAA	ASCII Adjust After Addition	AAA
AAA		8086	AAA	ASCII adjust AL after addition.
;--------------------------------------------------------
GENERAL	AAD	ASCII Adjust AX Before Division	AAD
AAD		8086	AAD	ASCII adjust AX before division.
AAD	IMM8	8086	AAD IMM8	Adjust AX before division to number base imm8.
;--------------------------------------------------------
GENERAL	AAM	ASCII Adjust AX After Multiply	AAM
AAM		8086	AAM	ASCII adjust AX after multiply.
AAM	IMM8	8086	AAM IMM8	Adjust AX after multiply to number base imm8.
;--------------------------------------------------------
GENERAL	AAS	ASCII Adjust AL After Subtraction	AAS
AAS		8086	AAS	ASCII adjust AL after subtraction.
;--------------------------------------------------------
GENERAL	ADC	Add with Carry	ADC
ADC	AL,IMM8	8086	ADC AL,IMM8	Add with carry imm8 to AL.
ADC	AX,IMM16	8086	ADC AX,IMM16	Add with carry imm16 to AX.
ADC	EAX,IMM32	386	ADC EAX,IMM32	Add with carry imm32 to EAX.
ADC	RAX,IMM32	386	ADC RAX,IMM32	Add with carry imm32 sign extended to 64-bits to RAX.
ADC	R/M8,IMM8	8086	ADC R/M8,IMM8	Add with carry imm8 to r/m8.
ADC	R/M8,IMM8	8086	ADC R/M8,IMM8	Add with carry imm8 to r/m8.
ADC	R/M16,IMM16	8086	ADC R/M16,IMM16	Add with carry imm16 to r/m16.
ADC	R/M32,IMM32	386	ADC R/M32,IMM32	Add with CF imm32 to r/m32.
ADC	R/M64,IMM32	X64	ADC R/M64,IMM32	Add with CF imm32 sign extended to 64-bits to r/m64.
ADC	R/M16,IMM8	8086	ADC R/M16,IMM8	Add with CF sign-extended imm8 to r/m16.
ADC	R/M32,IMM8	386	ADC R/M32,IMM8	Add with CF sign-extended imm8 into r/m32.
ADC	R/M64,IMM8	X64	ADC R/M64,IMM8	Add with CF sign-extended imm8 into r/m64.
ADC	R/M8,R8	8086	ADC R/M8,R8	Add with carry byte register to r/m8.
ADC	R/M8,R8	8086	ADC R/M8,R8	Add with carry byte register to r/m64.
ADC	R/M16,R16	8086	ADC R/M16,R16	Add with carry r16 to r/m16.
ADC	R/M32,R32	386	ADC R/M32,R32	Add with CF r32 to r/m32.
ADC	R/M64,R64	X64	ADC R/M64,R64	Add with CF r64 to r/m64.
ADC	R8,R/M8	8086	ADC R8,R/M8	Add with carry r/m8 to byte register.
ADC	R8,R/M8	8086	ADC R8,R/M8	Add with carry r/m64 to byte register.
ADC	R16,R/M16	8086	ADC R16,R/M16	Add with carry r/m16 to r16.
ADC	R32,R/M32	386	ADC R32,R/M32	Add with CF r/m32 to r32.
ADC	R64,R/M64	X64	ADC R64,R/M64	Add with CF r/m64 to r64.
;--------------------------------------------------------
GENERAL	ADCX	Unsigned Integer Addition of Two Operands with Carry Flag	ADCX
ADCX	R32,R/M32	ADX	ADCX R32,R/M32	Unsigned addition of r32 with CF, r/m32 to r32, writes CF.
ADCX	R64,R/M64	ADX	ADCX R64,R/M64	Unsigned addition of r64 with CF, r/m64 to r64, writes CF.
;--------------------------------------------------------
GENERAL	ADD	Add	ADD
ADD	AL,IMM8	8086	ADD AL,IMM8	Add imm8 to AL.
ADD	AX,IMM16	8086	ADD AX,IMM16	Add imm16 to AX.
ADD	EAX,IMM32	386	ADD EAX,IMM32	Add imm32 to EAX.
ADD	RAX,IMM32	386	ADD RAX,IMM32	Add imm32 sign-extended to 64-bits to RAX.
ADD	R/M8,IMM8	8086	ADD R/M8,IMM8	Add imm8 to r/m8.
ADD	R/M8,IMM8	8086	ADD R/M8,IMM8	Add sign-extended imm8 to r/m8.
ADD	R/M16,IMM16	8086	ADD R/M16,IMM16	Add imm16 to r/m16.
ADD	R/M32,IMM32	386	ADD R/M32,IMM32	Add imm32 to r/m32.
ADD	R/M64,IMM32	X64	ADD R/M64,IMM32	Add imm32 sign-extended to 64-bits to r/m64.
ADD	R/M16,IMM8	8086	ADD R/M16,IMM8	Add sign-extended imm8 to r/m16.
ADD	R/M32,IMM8	386	ADD R/M32,IMM8	Add sign-extended imm8 to r/m32.
ADD	R/M64,IMM8	X64	ADD R/M64,IMM8	Add sign-extended imm8 to r/m64.
ADD	R/M8,R8	8086	ADD R/M8,R8	Add r8 to r/m8.
ADD	R/M8,R8	8086	ADD R/M8,R8	Add r8 to r/m8.
ADD	R/M16,R16	8086	ADD R/M16,R16	Add r16 to r/m16.
ADD	R/M32,R32	386	ADD R/M32,R32	Add r32 to r/m32.
ADD	R/M64,R64	X64	ADD R/M64,R64	Add r64 to r/m64.
ADD	R8,R/M8	8086	ADD R8,R/M8	Add r/m8 to r8.
ADD	R8,R/M8	8086	ADD R8,R/M8	Add r/m8 to r8.
ADD	R16,R/M16	8086	ADD R16,R/M16	Add r/m16 to r16.
ADD	R32,R/M32	386	ADD R32,R/M32	Add r/m32 to r32.
ADD	R64,R/M64	X64	ADD R64,R/M64	Add r/m64 to r64.
;--------------------------------------------------------
GENERAL	ADDPD	Add Packed Double-Precision Floating-Point Values	ADDPD
ADDPD	XMM,XMM/M128	SSE2	ADDPD XMM1,XMM2/M128	Add packed DP FP values from xmm2/mem to xmm1 and store result in xmm1.
GENERAL	VADDPD	Add Packed Double-Precision Floating-Point Values	ADDPD
VADDPD	XMM,XMM,XMM/M128	AVX	VADDPD XMM1,XMM2,XMM3/M128	Add packed DP FP values from xmm3/mem to xmm2 and store result in xmm1.
VADDPD	YMM,YMM,YMM/M256	AVX	VADDPD YMM1,YMM2,YMM3/M256	Add packed DP FP values from ymm3/mem to ymm2 and store result in ymm1.
VADDPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VADDPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Add packed DP FP values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1.
VADDPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VADDPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Add packed DP FP values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1.
VADDPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VADDPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Add packed DP FP values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	ADDPS	Add Packed Single-Precision Floating-Point Values	ADDPS
ADDPS	XMM,XMM/M128	SSE	ADDPS XMM1,XMM2/M128	Add packed SP FP values from xmm2/m128 to xmm1 and store result in xmm1.
GENERAL	VADDPS	Add Packed Single-Precision Floating-Point Values	ADDPS
VADDPS	XMM,XMM,XMM/M128	AVX	VADDPS XMM1,XMM2,XMM3/M128	Add packed SP FP values from xmm3/m128 to xmm2 and store result in xmm1.
VADDPS	YMM,YMM,YMM/M256	AVX	VADDPS YMM1,YMM2,YMM3/M256	Add packed SP FP values from ymm3/m256 to ymm2 and store result in ymm1.
VADDPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VADDPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Add packed SP FP values from xmm3/m128/m32bcst to xmm2 and store result in xmm1 with writemask k1.
VADDPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VADDPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Add packed SP FP values from ymm3/m256/m32bcst to ymm2 and store result in ymm1 with writemask k1.
VADDPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VADDPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Add packed SP FP values from zmm3/m512/m32bcst to zmm2 and store result in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	ADDSD	Add Scalar Double-Precision Floating-Point Values	ADDSD
ADDSD	XMM,XMM/M64	SSE2	ADDSD XMM1,XMM2/M64	Add the low DP FP value from xmm2/mem to xmm1 and store the result in xmm1.
GENERAL	VADDSD	Add Scalar Double-Precision Floating-Point Values	ADDSD
VADDSD	XMM,XMM,XMM/M64	AVX	VADDSD XMM1,XMM2,XMM3/M64	Add the low DP FP value from xmm3/mem to xmm2 and store the result in xmm1.
VADDSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VADDSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Add the low DP FP value from xmm3/m64 to xmm2 and store the result in xmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	ADDSS	Add Scalar Single-Precision Floating-Point Values	ADDSS
ADDSS	XMM,XMM/M32	SSE	ADDSS XMM1,XMM2/M32	Add the low SP FP value from xmm2/mem to xmm1 and store the result in xmm1.
GENERAL	VADDSS	Add Scalar Single-Precision Floating-Point Values	ADDSS
VADDSS	XMM,XMM,XMM/M32	AVX	VADDSS XMM1,XMM2,XMM3/M32	Add the low SP FP value from xmm3/mem to xmm2 and store the result in xmm1.
VADDSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VADDSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Add the low SP FP value from xmm3/m32 to xmm2 and store the result in xmm1with writemask k1.
;--------------------------------------------------------
GENERAL	ADDSUBPD	Packed Double-FP Add/Subtract	ADDSUBPD
ADDSUBPD	XMM,XMM/M128	SSE3	ADDSUBPD XMM1,XMM2/M128	Add/subtract DP FP values from xmm2/m128 to xmm1.
GENERAL	VADDSUBPD	Packed Double-FP Add/Subtract	ADDSUBPD
VADDSUBPD	XMM,XMM,XMM/M128	AVX	VADDSUBPD XMM1,XMM2,XMM3/M128	Add/subtract packed DP FP values from xmm3/mem to xmm2 and stores result in xmm1.
VADDSUBPD	YMM,YMM,YMM/M256	AVX	VADDSUBPD YMM1,YMM2,YMM3/M256	Add / subtract packed DP FP values from ymm3/mem to ymm2 and stores result in ymm1.
;--------------------------------------------------------
GENERAL	ADDSUBPS	Packed Single-FP Add/Subtract	ADDSUBPS
ADDSUBPS	XMM,XMM/M128	SSE3	ADDSUBPS XMM1,XMM2/M128	Add/subtract SP FP values from xmm2/m128 to xmm1.
GENERAL	VADDSUBPS	Packed Single-FP Add/Subtract	ADDSUBPS
VADDSUBPS	XMM,XMM,XMM/M128	AVX	VADDSUBPS XMM1,XMM2,XMM3/M128	Add/subtract SP FP values from xmm3/mem to xmm2 and stores result in xmm1.
VADDSUBPS	YMM,YMM,YMM/M256	AVX	VADDSUBPS YMM1,YMM2,YMM3/M256	Add / subtract SP FP values from ymm3/mem to ymm2 and stores result in ymm1.
;--------------------------------------------------------
GENERAL	ADOX	Unsigned Integer Addition of Two Operands with Overflow Flag	ADOX
ADOX	R32,R/M32	ADX	ADOX R32,R/M32	Unsigned addition of r32 with OF, r/m32 to r32, writes OF.
ADOX	R64,R/M64	ADX	ADOX R64,R/M64	Unsigned addition of r64 with OF, r/m64 to r64, writes OF.
;--------------------------------------------------------
GENERAL	AESDEC	Perform One Round of an AES Decryption Flow	AESDEC
AESDEC	XMM,XMM/M128	AES	AESDEC XMM1,XMM2/M128	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
GENERAL	VAESDEC	Perform One Round of an AES Decryption Flow	AESDEC
VAESDEC	XMM,XMM,XMM/M128	AES,AVX	VAESDEC XMM1,XMM2,XMM3/M128	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
;--------------------------------------------------------
GENERAL	AESDECLAST	Perform Last Round of an AES Decryption Flow	AESDECLAST
AESDECLAST	XMM,XMM/M128	AES	AESDECLAST XMM1,XMM2/M128	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
GENERAL	VAESDECLAST	Perform Last Round of an AES Decryption Flow	AESDECLAST
VAESDECLAST	XMM,XMM,XMM/M128	AES,AVX	VAESDECLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
;--------------------------------------------------------
GENERAL	AESENC	Perform One Round of an AES Encryption Flow	AESENC
AESENC	XMM,XMM/M128	AES	AESENC XMM1,XMM2/M128	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
GENERAL	VAESENC	Perform One Round of an AES Encryption Flow	AESENC
VAESENC	XMM,XMM,XMM/M128	AES,AVX	VAESENC XMM1,XMM2,XMM3/M128	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1.
;--------------------------------------------------------
GENERAL	AESENCLAST	Perform Last Round of an AES Encryption Flow	AESENCLAST
AESENCLAST	XMM,XMM/M128	AES	AESENCLAST XMM1,XMM2/M128	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
GENERAL	VAESENCLAST	Perform Last Round of an AES Encryption Flow	AESENCLAST
VAESENCLAST	XMM,XMM,XMM/M128	AES,AVX	VAESENCLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1.
;--------------------------------------------------------
GENERAL	AESIMC	Perform the AES InvMixColumn Transformation	AESIMC
AESIMC	XMM,XMM/M128	AES	AESIMC XMM1,XMM2/M128	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
GENERAL	VAESIMC	Perform the AES InvMixColumn Transformation	AESIMC
VAESIMC	XMM,XMM/M128	AES,AVX	VAESIMC XMM1,XMM2/M128	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
;--------------------------------------------------------
GENERAL	AESKEYGENASSIST	AES Round Key Generation Assist	AESKEYGENASSIST
AESKEYGENASSIST	XMM,XMM/M128,IMM8	AES	AESKEYGENASSIST XMM1,XMM2/M128,IMM8	Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
GENERAL	VAESKEYGENASSIST	AES Round Key Generation Assist	AESKEYGENASSIST
VAESKEYGENASSIST	XMM,XMM/M128,IMM8	AES,AVX	VAESKEYGENASSIST XMM1,XMM2/M128,IMM8	Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
;--------------------------------------------------------
GENERAL	AND	Logical AND	AND
AND	AL,IMM8	8086	AND AL,IMM8	AL AND imm8.
AND	AX,IMM16	8086	AND AX,IMM16	AX AND imm16.
AND	EAX,IMM32	386	AND EAX,IMM32	EAX AND imm32.
AND	RAX,IMM32	386	AND RAX,IMM32	RAX AND imm32 sign-extended to 64-bits.
AND	R/M8,IMM8	8086	AND R/M8,IMM8	r/m8 AND imm8.
AND	R/M8,IMM8	8086	AND R/M8,IMM8	r/m8 AND imm8.
AND	R/M16,IMM16	8086	AND R/M16,IMM16	r/m16 AND imm16.
AND	R/M32,IMM32	386	AND R/M32,IMM32	r/m32 AND imm32.
AND	R/M64,IMM32	X64	AND R/M64,IMM32	r/m64 AND imm32 sign extended to 64-bits.
AND	R/M16,IMM8	8086	AND R/M16,IMM8	r/m16 AND imm8 (sign-extended).
AND	R/M32,IMM8	386	AND R/M32,IMM8	r/m32 AND imm8 (sign-extended).
AND	R/M64,IMM8	X64	AND R/M64,IMM8	r/m64 AND imm8 (sign-extended).
AND	R/M8,R8	8086	AND R/M8,R8	r/m8 AND r8.
AND	R/M8,R8	8086	AND R/M8,R8	r/m64 AND r8 (sign-extended).
AND	R/M16,R16	8086	AND R/M16,R16	r/m16 AND r16.
AND	R/M32,R32	386	AND R/M32,R32	r/m32 AND r32.
AND	R/M64,R64	X64	AND R/M64,R64	r/m64 AND r32.
AND	R8,R/M8	8086	AND R8,R/M8	r8 AND r/m8.
AND	R8,R/M8	8086	AND R8,R/M8	r/m64 AND r8 (sign-extended).
AND	R16,R/M16	8086	AND R16,R/M16	r16 AND r/m16.
AND	R32,R/M32	386	AND R32,R/M32	r32 AND r/m32.
AND	R64,R/M64	X64	AND R64,R/M64	r64 AND r/m64.
;--------------------------------------------------------
GENERAL	ANDN	Logical AND NOT	ANDN
ANDN	R32,R32,R/M32	BMI1	ANDN R32A,R32B,R/M32	Bitwise AND of inverted r32b with r/m32, store result in r32a.
ANDN	R64,R64,R/M64	BMI1	ANDN R64A,R64B,R/M64	Bitwise AND of inverted r64b with r/m64, store result in r64a.
;--------------------------------------------------------
GENERAL	ANDNPD	Bitwise Logical AND NOT of Packed Double Precision Floating-Point Values	ANDNPD
ANDNPD	XMM,XMM/M128	SSE2	ANDNPD XMM1,XMM2/M128	Return the bitwise logical AND NOT of packed DP FP values in xmm1 and xmm2/mem.
GENERAL	VANDNPD	Bitwise Logical AND NOT of Packed Double Precision Floating-Point Values	ANDNPD
VANDNPD	XMM,XMM,XMM/M128	AVX	VANDNPD XMM1,XMM2,XMM3/M128	Return the bitwise logical AND NOT of packed DP FP values in xmm2 and xmm3/mem.
VANDNPD	YMM,YMM,YMM/M256	AVX	VANDNPD YMM1,YMM2,YMM3/M256	Return the bitwise logical AND NOT of packed DP FP values in ymm2 and ymm3/mem.
VANDNPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VANDNPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical AND NOT of packed DP FP values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VANDNPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VANDNPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical AND NOT of packed DP FP values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VANDNPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_DQ	VANDNPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical AND NOT of packed DP FP values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	ANDNPS	Bitwise Logical AND NOT of Packed Single Precision Floating-Point Values	ANDNPS
ANDNPS	XMM,XMM/M128	SSE	ANDNPS XMM1,XMM2/M128	Return the bitwise logical AND NOT of packed SP FP values in xmm1 and xmm2/mem.
GENERAL	VANDNPS	Bitwise Logical AND NOT of Packed Single Precision Floating-Point Values	ANDNPS
VANDNPS	XMM,XMM,XMM/M128	AVX	VANDNPS XMM1,XMM2,XMM3/M128	Return the bitwise logical AND NOT of packed SP FP values in xmm2 and xmm3/mem.
VANDNPS	YMM,YMM,YMM/M256	AVX	VANDNPS YMM1,YMM2,YMM3/M256	Return the bitwise logical AND NOT of packed SP FP values in ymm2 and ymm3/mem.
VANDNPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VANDNPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical AND of packed SP FP values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VANDNPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_DQ	VANDNPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical AND of packed SP FP values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VANDNPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_DQ	VANDNPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical AND of packed SP FP values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	ANDPD	Bitwise Logical AND of Packed Double Precision Floating-Point Values	ANDPD
ANDPD	XMM,XMM/M128	SSE2	ANDPD XMM1,XMM2/M128	Return the bitwise logical AND of packed DP FP values in xmm1 and xmm2/mem.
GENERAL	VANDPD	Bitwise Logical AND of Packed Double Precision Floating-Point Values	ANDPD
VANDPD	XMM,XMM,XMM/M128	AVX	VANDPD XMM1,XMM2,XMM3/M128	Return the bitwise logical AND of packed DP FP values in xmm2 and xmm3/mem.
VANDPD	YMM,YMM,YMM/M256	AVX	VANDPD YMM1,YMM2,YMM3/M256	Return the bitwise logical AND of packed DP FP values in ymm2 and ymm3/mem.
VANDPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VANDPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical AND of packed DP FP values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VANDPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VANDPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical AND of packed DP FP values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VANDPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_DQ	VANDPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical AND of packed DP FP values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	ANDPS	Bitwise Logical AND of Packed Single Precision Floating-Point Values	ANDPS
ANDPS	XMM,XMM/M128	SSE	ANDPS XMM1,XMM2/M128	Return the bitwise logical AND of packed SP FP values in xmm1 and xmm2/mem.
GENERAL	VANDPS	Bitwise Logical AND of Packed Single Precision Floating-Point Values	ANDPS
VANDPS	XMM,XMM,XMM/M128	AVX	VANDPS XMM1,XMM2,XMM3/M128	Return the bitwise logical AND of packed SP FP values in xmm2 and xmm3/mem.
VANDPS	YMM,YMM,YMM/M256	AVX	VANDPS YMM1,YMM2,YMM3/M256	Return the bitwise logical AND of packed SP FP values in ymm2 and ymm3/mem.
VANDPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VANDPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical AND of packed SP FP values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VANDPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_DQ	VANDPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical AND of packed SP FP values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VANDPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_DQ	VANDPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical AND of packed SP FP values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	ARPL	Adjust RPL Field of Segment Selector	ARPL
ARPL	R/M16,R16	8086	ARPL R/M16,R16	Adjust RPL of r/m16 to not less than RPL of r16.
;--------------------------------------------------------
GENERAL	BEXTR	Bit Field Extract	BEXTR
BEXTR	R32,R/M32,R32	BMI1	BEXTR R32A,R/M32,R32B	Contiguous bitwise extract from r/m32 using r32b as control; store result in r32a.
BEXTR	R64,R/M64,R64	BMI1	BEXTR R64A,R/M64,R64B	Contiguous bitwise extract from r/m64 using r64b as control; store result in r64a
;--------------------------------------------------------
GENERAL	BLENDPD	Blend Packed Double Precision Floating-Point Values	BLENDPD
BLENDPD	XMM,XMM/M128,IMM8	SSE4_1	BLENDPD XMM1,XMM2/M128,IMM8	Select packed DP-FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
GENERAL	VBLENDPD	Blend Packed Double Precision Floating-Point Values	BLENDPD
VBLENDPD	XMM,XMM,XMM/M128,IMM8	AVX	VBLENDPD XMM1,XMM2,XMM3/M128,IMM8	Select packed DP FP Values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VBLENDPD	YMM,YMM,YMM/M256,IMM8	AVX	VBLENDPD YMM1,YMM2,YMM3/M256,IMM8	Select packed DP FP Values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
;--------------------------------------------------------
GENERAL	BLENDPS	Blend Packed Single Precision Floating-Point Values	BLENDPS
BLENDPS	XMM,XMM/M128,IMM8	SSE4_1	BLENDPS XMM1,XMM2/M128,IMM8	Select packed single precision FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
GENERAL	VBLENDPS	Blend Packed Single Precision Floating-Point Values	BLENDPS
VBLENDPS	XMM,XMM,XMM/M128,IMM8	AVX	VBLENDPS XMM1,XMM2,XMM3/M128,IMM8	Select packed SP FP values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VBLENDPS	YMM,YMM,YMM/M256,IMM8	AVX	VBLENDPS YMM1,YMM2,YMM3/M256,IMM8	Select packed SP FP values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
;--------------------------------------------------------
GENERAL	BLENDVPD	Variable Blend Packed Double Precision Floating-Point Values	BLENDVPD
BLENDVPD	XMM,XMM/M128,XMM_ZERO	SSE4_1	BLENDVPD XMM1,XMM2/M128,XMM_ZERO	Select packed DP FP values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1.
GENERAL	VBLENDVPD	Variable Blend Packed Double Precision Floating-Point Values	BLENDVPD
VBLENDVPD	XMM,XMM,XMM/M128,XMM	AVX	VBLENDVPD XMM1,XMM2,XMM3/M128,XMM4	Conditionally copy DP FP values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4.
VBLENDVPD	YMM,YMM,YMM/M256,YMM	AVX	VBLENDVPD YMM1,YMM2,YMM3/M256,YMM4	Conditionally copy DP FP values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4.
;--------------------------------------------------------
GENERAL	BLENDVPS	Variable Blend Packed Single Precision Floating-Point Values	BLENDVPS
BLENDVPS	XMM,XMM/M128,XMM_ZERO	SSE4_1	BLENDVPS XMM1,XMM2/M128,XMM_ZERO	Select packed single precision FP values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1.
GENERAL	VBLENDVPS	Variable Blend Packed Single Precision Floating-Point Values	BLENDVPS
VBLENDVPS	XMM,XMM,XMM/M128,XMM	AVX	VBLENDVPS XMM1,XMM2,XMM3/M128,XMM4	Conditionally copy SP FP values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4.
VBLENDVPS	YMM,YMM,YMM/M256,YMM	AVX	VBLENDVPS YMM1,YMM2,YMM3/M256,YMM4	Conditionally copy SP FP values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4.
;--------------------------------------------------------
GENERAL	BLSI	Extract Lowest Set Isolated Bit	BLSI
BLSI	R32,R/M32	BMI1	BLSI R32,R/M32	Extract lowest set bit from r/m32 and set that bit in r32.
BLSI	R64,R/M64	BMI1	BLSI R64,R/M64	Extract lowest set bit from r/m64, and set that bit in r64.
;--------------------------------------------------------
GENERAL	BLSMSK	Get Mask Up to Lowest Set Bit	BLSMSK
BLSMSK	R32,R/M32	BMI1	BLSMSK R32,R/M32	Set all lower bits in r32 to “1” starting from bit 0 to lowest set bit in r/m32.
BLSMSK	R64,R/M64	BMI1	BLSMSK R64,R/M64	Set all lower bits in r64 to “1” starting from bit 0 to lowest set bit in r/m64.
;--------------------------------------------------------
GENERAL	BLSR	Reset Lowest Set Bit	BLSR
BLSR	R32,R/M32	BMI1	BLSR R32,R/M32	Reset lowest set bit of r/m32, keep all other bits of r/m32 and write result to r32.
BLSR	R64,R/M64	BMI1	BLSR R64,R/M64	Reset lowest set bit of r/m64, keep all other bits of r/m64 and write result to r64.
;--------------------------------------------------------
GENERAL	BNDCL	Check Lower Bound	BNDCL
BNDCL	BND,R/M32	MPX	BNDCL BND,R/M32	Generate a #BR if the address in r/m32 is lower than the lower bound in bnd.LB.
BNDCL	BND,R/M64	MPX	BNDCL BND,R/M64	Generate a #BR if the address in r/m64 is lower than the lower bound in bnd.LB.
;--------------------------------------------------------
GENERAL	BNDCU	Check Upper Bound	BNDCU_BNDCN
BNDCU	BND,R/M32	MPX	BNDCU BND,R/M32	Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form).
BNDCU	BND,R/M64	MPX	BNDCU BND,R/M64	Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form).
GENERAL	BNDCN	Check Upper Bound	BNDCU_BNDCN
BNDCN	BND,R/M32	MPX	BNDCN BND,R/M32	Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form).
BNDCN	BND,R/M64	MPX	BNDCN BND,R/M64	Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form).
;--------------------------------------------------------
GENERAL	BNDLDX	Load Extended Bounds Using Address Translation	BNDLDX
BNDLDX	BND,MIB	MPX	BNDLDX BND,MIB	Load the bounds stored in a bound table entry (BTE) into bnd with address translation using the base of mib and conditional on the index of mib matching the pointer value in the BTE.
;--------------------------------------------------------
GENERAL	BNDMK	Make Bounds	BNDMK
BNDMK	BND,M32	MPX	BNDMK BND,M32	Make lower and upper bounds from m32 and store them in bnd.
BNDMK	BND,M64	MPX	BNDMK BND,M64	Make lower and upper bounds from m64 and store them in bnd.
;--------------------------------------------------------
GENERAL	BNDMOV	Move Bounds	BNDMOV
BNDMOV	BND,BND/M64	MPX	BNDMOV BND1,BND2/M64	Move lower and upper bound from bnd2/m64 to bound register bnd1.
BNDMOV	BND,BND/M128	MPX	BNDMOV BND1,BND2/M128	Move lower and upper bound from bnd2/m128 to bound register bnd1.
BNDMOV	BND/M64,BND	MPX	BNDMOV BND1/M64,BND2	Move lower and upper bound from bnd2 to bnd1/m64.
BNDMOV	BND/M128,BND	MPX	BNDMOV BND1/M128,BND2	Move lower and upper bound from bnd2 to bound register bnd1/m128.
;--------------------------------------------------------
GENERAL	BNDSTX	Store Extended Bounds Using Address Translation	BNDSTX
BNDSTX	MIB,BND	MPX	BNDSTX MIB,BND	Store the bounds in bnd and the pointer value in the index regis- ter of mib to a bound table entry (BTE) with address translation using the base of mib.
;--------------------------------------------------------
GENERAL	BOUND	Check Array Index Against Bounds	BOUND
BOUND	R16,M16&16	8086	BOUND R16,M16&16	Check if r16 (array index) is within bounds specified by m16&16.
BOUND	R32,M32&32	386	BOUND R32,M32&32	Check if r32 (array index) is within bounds specified by m32&32.
;--------------------------------------------------------
GENERAL	BSF	Bit Scan Forward	BSF
BSF	R16,R/M16	8086	BSF R16,R/M16	Bit scan forward on r/m16.
BSF	R32,R/M32	386	BSF R32,R/M32	Bit scan forward on r/m32.
BSF	R64,R/M64	X64	BSF R64,R/M64	Bit scan forward on r/m64.
;--------------------------------------------------------
GENERAL	BSR	Bit Scan Reverse	BSR
BSR	R16,R/M16	8086	BSR R16,R/M16	Bit scan reverse on r/m16.
BSR	R32,R/M32	386	BSR R32,R/M32	Bit scan reverse on r/m32.
BSR	R64,R/M64	X64	BSR R64,R/M64	Bit scan reverse on r/m64.
;--------------------------------------------------------
GENERAL	BSWAP	Byte Swap	BSWAP
BSWAP	R32	386	BSWAP R32	Reverses the byte order of a 32-bit register.
BSWAP	R64	X64	BSWAP R64	Reverses the byte order of a 64-bit register.
;--------------------------------------------------------
GENERAL	BT	Bit Test	BT
BT	R/M16,R16	8086	BT R/M16,R16	Store selected bit in CF flag.
BT	R/M32,R32	386	BT R/M32,R32	Store selected bit in CF flag.
BT	R/M64,R64	X64	BT R/M64,R64	Store selected bit in CF flag.
BT	R/M16,IMM8	8086	BT R/M16,IMM8	Store selected bit in CF flag.
BT	R/M32,IMM8	386	BT R/M32,IMM8	Store selected bit in CF flag.
BT	R/M64,IMM8	X64	BT R/M64,IMM8	Store selected bit in CF flag.
;--------------------------------------------------------
GENERAL	BTC	Bit Test and Complement	BTC
BTC	R/M16,R16	8086	BTC R/M16,R16	Store selected bit in CF flag and complement.
BTC	R/M32,R32	386	BTC R/M32,R32	Store selected bit in CF flag and complement.
BTC	R/M64,R64	X64	BTC R/M64,R64	Store selected bit in CF flag and complement.
BTC	R/M16,IMM8	8086	BTC R/M16,IMM8	Store selected bit in CF flag and complement.
BTC	R/M32,IMM8	386	BTC R/M32,IMM8	Store selected bit in CF flag and complement.
BTC	R/M64,IMM8	X64	BTC R/M64,IMM8	Store selected bit in CF flag and complement.
;--------------------------------------------------------
GENERAL	BTR	Bit Test and Reset	BTR
BTR	R/M16,R16	8086	BTR R/M16,R16	Store selected bit in CF flag and clear.
BTR	R/M32,R32	386	BTR R/M32,R32	Store selected bit in CF flag and clear.
BTR	R/M64,R64	X64	BTR R/M64,R64	Store selected bit in CF flag and clear.
BTR	R/M16,IMM8	8086	BTR R/M16,IMM8	Store selected bit in CF flag and clear.
BTR	R/M32,IMM8	386	BTR R/M32,IMM8	Store selected bit in CF flag and clear.
BTR	R/M64,IMM8	X64	BTR R/M64,IMM8	Store selected bit in CF flag and clear.
;--------------------------------------------------------
GENERAL	BTS	Bit Test and Set	BTS
BTS	R/M16,R16	8086	BTS R/M16,R16	Store selected bit in CF flag and set.
BTS	R/M32,R32	386	BTS R/M32,R32	Store selected bit in CF flag and set.
BTS	R/M64,R64	X64	BTS R/M64,R64	Store selected bit in CF flag and set.
BTS	R/M16,IMM8	8086	BTS R/M16,IMM8	Store selected bit in CF flag and set.
BTS	R/M32,IMM8	386	BTS R/M32,IMM8	Store selected bit in CF flag and set.
BTS	R/M64,IMM8	X64	BTS R/M64,IMM8	Store selected bit in CF flag and set.
;--------------------------------------------------------
GENERAL	BZHI	Zero High Bits Starting with Specified Bit Position	BZHI
BZHI	R32,R/M32,R32	BMI2	BZHI R32A,R/M32,R32B	Zero bits in r/m32 starting with the position in r32b, write result to r32a.
BZHI	R64,R/M64,R64	BMI2	BZHI R64A,R/M64,R64B	Zero bits in r/m64 starting with the position in r64b, write result to r64a.
;--------------------------------------------------------
GENERAL	CALL	Call Procedure	CALL
CALL	REL16	386	CALL REL16	Call near, relative, displacement relative to next instruction.
CALL	REL32	386	CALL REL32	Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode.
CALL	R/M16	8086	CALL R/M16	Call near, absolute indirect, address given in r/m16.
CALL	R/M32	386	CALL R/M32	Call near, absolute indirect, address given in r/m32.
CALL	R/M64	X64	CALL R/M64	Call near, absolute indirect, address given in r/m64.
CALL	PTR16:16	8086	CALL PTR16:16	Call far, absolute, address given in operand.
CALL	PTR16:32	8086	CALL PTR16:32	Call far, absolute, address given in operand.
CALL	M16:16	8086	CALL M16:16	Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16-bit offset from far pointer referenced in the instruction.
CALL	M16:32	8086	CALL M16:32	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction.
CALL	M16:64	8086	CALL M16:64	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction.
;--------------------------------------------------------
GENERAL	CBW	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE
CBW		8086	CBW	AX ← sign-extend of AL.
GENERAL	CWDE	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE
CWDE		8086	CWDE	EAX ← sign-extend of AX.
GENERAL	CDQE	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE
CDQE		8086	CDQE	RAX ← sign-extend of EAX.
;--------------------------------------------------------
GENERAL	CLAC	Clear AC Flag in EFLAGS Register	CLAC
CLAC			CLAC	Clear the AC flag in the EFLAGS register.
;--------------------------------------------------------
GENERAL	CLC	Clear Carry Flag	CLC
CLC		8086	CLC	Clear CF flag.
;--------------------------------------------------------
GENERAL	CLD	Clear Direction Flag	CLD
CLD		8086	CLD	Clear DF flag.
;--------------------------------------------------------
GENERAL	CLDEMOTE	Cache Line Demote	CLDEMOTE
CLDEMOTE	M8	CLDEMOTE	CLDEMOTE M8	Hint to hardware to move the cache line containing m8 to a more distant level of the cache without writing back to memory.
;--------------------------------------------------------
GENERAL	CLFLUSH	Flush Cache Line	CLFLUSH
CLFLUSH	M8	8086	CLFLUSH M8	Flushes cache line containing m8.
;--------------------------------------------------------
GENERAL	CLFLUSHOPT	Flush Cache Line Optimized	CLFLUSHOPT
CLFLUSHOPT	M8	8086	CLFLUSHOPT M8	Flushes cache line containing m8.
;--------------------------------------------------------
GENERAL	CLI	Clear Interrupt Flag	CLI
CLI		8086	CLI	Clear interrupt flag; interrupts disabled when interrupt flag cleared.
;--------------------------------------------------------
GENERAL	CLTS	Clear Task-Switched Flag in CR0	CLTS
CLTS		8086	CLTS	Clears TS flag in CR0.
;--------------------------------------------------------
GENERAL	CLWB	Cache Line Write Back	CLWB
CLWB	M8		CLWB M8	Writes back modified cache line containing m8, and may retain the line in cache hierarchy in non-modified state.
;--------------------------------------------------------
GENERAL	CMC	Complement Carry Flag	CMC
CMC		8086	CMC	Complement CF flag.
;--------------------------------------------------------
GENERAL	CMOVA	Conditional Move	CMOVcc
CMOVA	R16,R/M16	P6	CMOVA R16,R/M16	Move if above (CF=0 and ZF=0).
CMOVA	R32,R/M32	P6	CMOVA R32,R/M32	Move if above (CF=0 and ZF=0).
CMOVA	R64,R/M64	X64	CMOVA R64,R/M64	Move if above (CF=0 and ZF=0).
GENERAL	CMOVAE	Conditional Move	CMOVcc
CMOVAE	R16,R/M16	P6	CMOVAE R16,R/M16	Move if above or equal (CF=0).
CMOVAE	R32,R/M32	P6	CMOVAE R32,R/M32	Move if above or equal (CF=0).
CMOVAE	R64,R/M64	X64	CMOVAE R64,R/M64	Move if above or equal (CF=0).
GENERAL	CMOVB	Conditional Move	CMOVcc
CMOVB	R16,R/M16	P6	CMOVB R16,R/M16	Move if below (CF=1).
CMOVB	R32,R/M32	P6	CMOVB R32,R/M32	Move if below (CF=1).
CMOVB	R64,R/M64	X64	CMOVB R64,R/M64	Move if below (CF=1).
GENERAL	CMOVBE	Conditional Move	CMOVcc
CMOVBE	R16,R/M16	P6	CMOVBE R16,R/M16	Move if below or equal (CF=1 or ZF=1).
CMOVBE	R32,R/M32	P6	CMOVBE R32,R/M32	Move if below or equal (CF=1 or ZF=1).
CMOVBE	R64,R/M64	X64	CMOVBE R64,R/M64	Move if below or equal (CF=1 or ZF=1).
GENERAL	CMOVC	Conditional Move	CMOVcc
CMOVC	R16,R/M16	P6	CMOVC R16,R/M16	Move if carry (CF=1).
CMOVC	R32,R/M32	P6	CMOVC R32,R/M32	Move if carry (CF=1).
CMOVC	R64,R/M64	X64	CMOVC R64,R/M64	Move if carry (CF=1).
GENERAL	CMOVE	Conditional Move	CMOVcc
CMOVE	R16,R/M16	P6	CMOVE R16,R/M16	Move if equal (ZF=1).
CMOVE	R32,R/M32	P6	CMOVE R32,R/M32	Move if equal (ZF=1).
CMOVE	R64,R/M64	X64	CMOVE R64,R/M64	Move if equal (ZF=1).
GENERAL	CMOVG	Conditional Move	CMOVcc
CMOVG	R16,R/M16	P6	CMOVG R16,R/M16	Move if greater (ZF=0 and SF=OF).
CMOVG	R32,R/M32	P6	CMOVG R32,R/M32	Move if greater (ZF=0 and SF=OF).
CMOVG	R64,R/M64	X64	CMOVG R64,R/M64	Move if greater (ZF=0 and SF=OF).
GENERAL	CMOVGE	Conditional Move	CMOVcc
CMOVGE	R16,R/M16	P6	CMOVGE R16,R/M16	Move if greater or equal (SF=OF).
CMOVGE	R32,R/M32	P6	CMOVGE R32,R/M32	Move if greater or equal (SF=OF).
CMOVGE	R64,R/M64	X64	CMOVGE R64,R/M64	Move if greater or equal (SF=OF).
GENERAL	CMOVL	Conditional Move	CMOVcc
CMOVL	R16,R/M16	P6	CMOVL R16,R/M16	Move if less (SF≠ OF).
CMOVL	R32,R/M32	P6	CMOVL R32,R/M32	Move if less (SF≠ OF).
CMOVL	R64,R/M64	X64	CMOVL R64,R/M64	Move if less (SF≠ OF).
GENERAL	CMOVLE	Conditional Move	CMOVcc
CMOVLE	R16,R/M16	P6	CMOVLE R16,R/M16	Move if less or equal (ZF=1 or SF≠ OF).
CMOVLE	R32,R/M32	P6	CMOVLE R32,R/M32	Move if less or equal (ZF=1 or SF≠ OF).
CMOVLE	R64,R/M64	X64	CMOVLE R64,R/M64	Move if less or equal (ZF=1 or SF≠ OF).
GENERAL	CMOVNA	Conditional Move	CMOVcc
CMOVNA	R16,R/M16	P6	CMOVNA R16,R/M16	Move if not above (CF=1 or ZF=1).
CMOVNA	R32,R/M32	P6	CMOVNA R32,R/M32	Move if not above (CF=1 or ZF=1).
CMOVNA	R64,R/M64	X64	CMOVNA R64,R/M64	Move if not above (CF=1 or ZF=1).
GENERAL	CMOVNAE	Conditional Move	CMOVcc
CMOVNAE	R16,R/M16	P6	CMOVNAE R16,R/M16	Move if not above or equal (CF=1).
CMOVNAE	R32,R/M32	P6	CMOVNAE R32,R/M32	Move if not above or equal (CF=1).
CMOVNAE	R64,R/M64	X64	CMOVNAE R64,R/M64	Move if not above or equal (CF=1).
GENERAL	CMOVNB	Conditional Move	CMOVcc
CMOVNB	R16,R/M16	P6	CMOVNB R16,R/M16	Move if not below (CF=0).
CMOVNB	R32,R/M32	P6	CMOVNB R32,R/M32	Move if not below (CF=0).
CMOVNB	R64,R/M64	X64	CMOVNB R64,R/M64	Move if not below (CF=0).
GENERAL	CMOVNBE	Conditional Move	CMOVcc
CMOVNBE	R16,R/M16	P6	CMOVNBE R16,R/M16	Move if not below or equal (CF=0 and ZF=0).
CMOVNBE	R32,R/M32	P6	CMOVNBE R32,R/M32	Move if not below or equal (CF=0 and ZF=0).
CMOVNBE	R64,R/M64	X64	CMOVNBE R64,R/M64	Move if not below or equal (CF=0 and ZF=0).
GENERAL	CMOVNC	Conditional Move	CMOVcc
CMOVNC	R16,R/M16	P6	CMOVNC R16,R/M16	Move if not carry (CF=0).
CMOVNC	R32,R/M32	P6	CMOVNC R32,R/M32	Move if not carry (CF=0).
CMOVNC	R64,R/M64	X64	CMOVNC R64,R/M64	Move if not carry (CF=0).
GENERAL	CMOVNE	Conditional Move	CMOVcc
CMOVNE	R16,R/M16	P6	CMOVNE R16,R/M16	Move if not equal (ZF=0).
CMOVNE	R32,R/M32	P6	CMOVNE R32,R/M32	Move if not equal (ZF=0).
CMOVNE	R64,R/M64	X64	CMOVNE R64,R/M64	Move if not equal (ZF=0).
GENERAL	CMOVNG	Conditional Move	CMOVcc
CMOVNG	R16,R/M16	P6	CMOVNG R16,R/M16	Move if not greater (ZF=1 or SF≠ OF).
CMOVNG	R32,R/M32	P6	CMOVNG R32,R/M32	Move if not greater (ZF=1 or SF≠ OF).
CMOVNG	R64,R/M64	X64	CMOVNG R64,R/M64	Move if not greater (ZF=1 or SF≠ OF).
GENERAL	CMOVNGE	Conditional Move	CMOVcc
CMOVNGE	R16,R/M16	P6	CMOVNGE R16,R/M16	Move if not greater or equal (SF≠ OF).
CMOVNGE	R32,R/M32	P6	CMOVNGE R32,R/M32	Move if not greater or equal (SF≠ OF).
CMOVNGE	R64,R/M64	X64	CMOVNGE R64,R/M64	Move if not greater or equal (SF≠ OF).
GENERAL	CMOVNL	Conditional Move	CMOVcc
CMOVNL	R16,R/M16	P6	CMOVNL R16,R/M16	Move if not less (SF=OF).
CMOVNL	R32,R/M32	P6	CMOVNL R32,R/M32	Move if not less (SF=OF).
CMOVNL	R64,R/M64	X64	CMOVNL R64,R/M64	Move if not less (SF=OF).
GENERAL	CMOVNLE	Conditional Move	CMOVcc
CMOVNLE	R16,R/M16	P6	CMOVNLE R16,R/M16	Move if not less or equal (ZF=0 and SF=OF).
CMOVNLE	R32,R/M32	P6	CMOVNLE R32,R/M32	Move if not less or equal (ZF=0 and SF=OF).
CMOVNLE	R64,R/M64	X64	CMOVNLE R64,R/M64	Move if not less or equal (ZF=0 and SF=OF).
GENERAL	CMOVNO	Conditional Move	CMOVcc
CMOVNO	R16,R/M16	P6	CMOVNO R16,R/M16	Move if not overflow (OF=0).
CMOVNO	R32,R/M32	P6	CMOVNO R32,R/M32	Move if not overflow (OF=0).
CMOVNO	R64,R/M64	X64	CMOVNO R64,R/M64	Move if not overflow (OF=0).
GENERAL	CMOVNP	Conditional Move	CMOVcc
CMOVNP	R16,R/M16	P6	CMOVNP R16,R/M16	Move if not parity (PF=0).
CMOVNP	R32,R/M32	P6	CMOVNP R32,R/M32	Move if not parity (PF=0).
CMOVNP	R64,R/M64	X64	CMOVNP R64,R/M64	Move if not parity (PF=0).
GENERAL	CMOVNS	Conditional Move	CMOVcc
CMOVNS	R16,R/M16	P6	CMOVNS R16,R/M16	Move if not sign (SF=0).
CMOVNS	R32,R/M32	P6	CMOVNS R32,R/M32	Move if not sign (SF=0).
CMOVNS	R64,R/M64	X64	CMOVNS R64,R/M64	Move if not sign (SF=0).
GENERAL	CMOVNZ	Conditional Move	CMOVcc
CMOVNZ	R16,R/M16	P6	CMOVNZ R16,R/M16	Move if not zero (ZF=0).
CMOVNZ	R32,R/M32	P6	CMOVNZ R32,R/M32	Move if not zero (ZF=0).
CMOVNZ	R64,R/M64	X64	CMOVNZ R64,R/M64	Move if not zero (ZF=0).
GENERAL	CMOVO	Conditional Move	CMOVcc
CMOVO	R16,R/M16	P6	CMOVO R16,R/M16	Move if overflow (OF=1).
CMOVO	R32,R/M32	P6	CMOVO R32,R/M32	Move if overflow (OF=1).
CMOVO	R64,R/M64	X64	CMOVO R64,R/M64	Move if overflow (OF=1).
GENERAL	CMOVP	Conditional Move	CMOVcc
CMOVP	R16,R/M16	P6	CMOVP R16,R/M16	Move if parity (PF=1).
CMOVP	R32,R/M32	P6	CMOVP R32,R/M32	Move if parity (PF=1).
CMOVP	R64,R/M64	X64	CMOVP R64,R/M64	Move if parity (PF=1).
GENERAL	CMOVPE	Conditional Move	CMOVcc
CMOVPE	R16,R/M16	P6	CMOVPE R16,R/M16	Move if parity even (PF=1).
CMOVPE	R32,R/M32	P6	CMOVPE R32,R/M32	Move if parity even (PF=1).
CMOVPE	R64,R/M64	X64	CMOVPE R64,R/M64	Move if parity even (PF=1).
GENERAL	CMOVPO	Conditional Move	CMOVcc
CMOVPO	R16,R/M16	P6	CMOVPO R16,R/M16	Move if parity odd (PF=0).
CMOVPO	R32,R/M32	P6	CMOVPO R32,R/M32	Move if parity odd (PF=0).
CMOVPO	R64,R/M64	X64	CMOVPO R64,R/M64	Move if parity odd (PF=0).
GENERAL	CMOVS	Conditional Move	CMOVcc
CMOVS	R16,R/M16	P6	CMOVS R16,R/M16	Move if sign (SF=1).
CMOVS	R32,R/M32	P6	CMOVS R32,R/M32	Move if sign (SF=1).
CMOVS	R64,R/M64	X64	CMOVS R64,R/M64	Move if sign (SF=1).
GENERAL	CMOVZ	Conditional Move	CMOVcc
CMOVZ	R16,R/M16	P6	CMOVZ R16,R/M16	Move if zero (ZF=1).
CMOVZ	R32,R/M32	P6	CMOVZ R32,R/M32	Move if zero (ZF=1).
CMOVZ	R64,R/M64	X64	CMOVZ R64,R/M64	Move if zero (ZF=1).
;--------------------------------------------------------
GENERAL	CMP	Compare Two Operands	CMP
CMP	AL,IMM8	8086	CMP AL,IMM8	Compare imm8 with AL.
CMP	AX,IMM16	8086	CMP AX,IMM16	Compare imm16 with AX.
CMP	EAX,IMM32	386	CMP EAX,IMM32	Compare imm32 with EAX.
CMP	RAX,IMM32	386	CMP RAX,IMM32	Compare imm32 sign-extended to 64-bits with RAX.
CMP	R/M8,IMM8	8086	CMP R/M8,IMM8	Compare imm8 with r/m8.
CMP	R/M8,IMM8	8086	CMP R/M8,IMM8	Compare imm8 with r/m8.
CMP	R/M16,IMM16	8086	CMP R/M16,IMM16	Compare imm16 with r/m16.
CMP	R/M32,IMM32	386	CMP R/M32,IMM32	Compare imm32 with r/m32.
CMP	R/M64,IMM32	X64	CMP R/M64,IMM32	Compare imm32 sign-extended to 64-bits with r/m64.
CMP	R/M16,IMM8	8086	CMP R/M16,IMM8	Compare imm8 with r/m16.
CMP	R/M32,IMM8	386	CMP R/M32,IMM8	Compare imm8 with r/m32.
CMP	R/M64,IMM8	X64	CMP R/M64,IMM8	Compare imm8 with r/m64.
CMP	R/M8,R8	8086	CMP R/M8,R8	Compare r8 with r/m8.
CMP	R/M8,R8	8086	CMP R/M8,R8	Compare r8 with r/m8.
CMP	R/M16,R16	8086	CMP R/M16,R16	Compare r16 with r/m16.
CMP	R/M32,R32	386	CMP R/M32,R32	Compare r32 with r/m32.
CMP	R/M64,R64	X64	CMP R/M64,R64	Compare r64 with r/m64.
CMP	R8,R/M8	8086	CMP R8,R/M8	Compare r/m8 with r8.
CMP	R8,R/M8	8086	CMP R8,R/M8	Compare r/m8 with r8.
CMP	R16,R/M16	8086	CMP R16,R/M16	Compare r/m16 with r16.
CMP	R32,R/M32	386	CMP R32,R/M32	Compare r/m32 with r32.
CMP	R64,R/M64	X64	CMP R64,R/M64	Compare r/m64 with r64.
;--------------------------------------------------------
GENERAL	CMPPD	Compare Packed Double-Precision Floating-Point Values	CMPPD
CMPPD	XMM,XMM/M128,IMM8	SSE2	CMPPD XMM1,XMM2/M128,IMM8	Compare packed DP FP values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate.
GENERAL	VCMPPD	Compare Packed Double-Precision Floating-Point Values	CMPPD
VCMPPD	XMM,XMM,XMM/M128,IMM8	AVX	VCMPPD XMM1,XMM2,XMM3/M128,IMM8	Compare packed DP FP values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPD	YMM,YMM,YMM/M256,IMM8	AVX	VCMPPD YMM1,YMM2,YMM3/M256,IMM8	Compare packed DP FP values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPD	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VCMPPD K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed DP FP values in xmm3/m128/m64bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPD	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VCMPPD K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed DP FP values in ymm3/m256/m64bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPD	K{K},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512_F	VCMPPD K1{K2},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Compare packed DP FP values in zmm3/m512/m64bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	CMPPS	Compare Packed Single-Precision Floating-Point Values	CMPPS
CMPPS	XMM,XMM/M128,IMM8	SSE	CMPPS XMM1,XMM2/M128,IMM8	Compare packed SP FP values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate.
GENERAL	VCMPPS	Compare Packed Single-Precision Floating-Point Values	CMPPS
VCMPPS	XMM,XMM,XMM/M128,IMM8	AVX	VCMPPS XMM1,XMM2,XMM3/M128,IMM8	Compare packed SP FP values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPS	YMM,YMM,YMM/M256,IMM8	AVX	VCMPPS YMM1,YMM2,YMM3/M256,IMM8	Compare packed SP FP values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPS	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VCMPPS K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed SP FP values in xmm3/m128/m32bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPS	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VCMPPS K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed SP FP values in ymm3/m256/m32bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPS	K{K},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512_F	VCMPPS K1{K2},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Compare packed SP FP values in zmm3/m512/m32bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	CMPS	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ
CMPS	M8,M8	8086	CMPS M8,M8	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI to byte at address (R|E)DI. The status flags are set accordingly.
CMPS	M16,M16	8086	CMPS M16,M16	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
CMPS	M32,M32	386	CMPS M32,M32	For legacy mode, compare dword at address DS:(E)SI at dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI at dword at address (R|E)DI. The status flags are set accordingly.
CMPS	M64,M64	X64	CMPS M64,M64	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
GENERAL	CMPSB	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ
CMPSB		8086	CMPSB	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly.
GENERAL	CMPSW	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ
CMPSW		8086	CMPSW	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
GENERAL	CMPSD	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ
CMPSD		8086	CMPSD	For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly.
GENERAL	CMPSQ	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ
CMPSQ		8086	CMPSQ	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
;--------------------------------------------------------
GENERAL	CMPSD	Compare Scalar Double-Precision Floating-Point Value	CMPSD
CMPSD	XMM,XMM/M64,IMM8	SSE2	CMPSD XMM1,XMM2/M64,IMM8	Compare low DP FP value in xmm2/m64 and xmm1 using bits 2:0 of imm8 as comparison predicate.
GENERAL	VCMPSD	Compare Scalar Double-Precision Floating-Point Value	CMPSD
VCMPSD	XMM,XMM,XMM/M64,IMM8	AVX	VCMPSD XMM1,XMM2,XMM3/M64,IMM8	Compare low DP FP value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate.
VCMPSD	K{K},XMM,XMM/M64{SAE},IMM8	AVX512_F	VCMPSD K1{K2},XMM2,XMM3/M64{SAE},IMM8	Compare low DP FP value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	CMPSS	Compare Scalar Single-Precision Floating-Point Value	CMPSS
CMPSS	XMM,XMM/M32,IMM8	SSE	CMPSS XMM1,XMM2/M32,IMM8	Compare low SP FP value in xmm2/m32 and xmm1 using bits 2:0 of imm8 as comparison predicate.
GENERAL	VCMPSS	Compare Scalar Single-Precision Floating-Point Value	CMPSS
VCMPSS	XMM,XMM,XMM/M32,IMM8	AVX	VCMPSS XMM1,XMM2,XMM3/M32,IMM8	Compare low SP FP value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate.
VCMPSS	K{K},XMM,XMM/M32{SAE},IMM8	AVX512_F	VCMPSS K1{K2},XMM2,XMM3/M32{SAE},IMM8	Compare low SP FP value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	CMPXCHG	Compare and Exchange	CMPXCHG
CMPXCHG	R/M8,R8		CMPXCHG R/M8,R8	Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
CMPXCHG	R/M8,R8		CMPXCHG R/M8,R8	Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
CMPXCHG	R/M16,R16		CMPXCHG R/M16,R16	Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX.
CMPXCHG	R/M32,R32		CMPXCHG R/M32,R32	Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX.
CMPXCHG	R/M64,R64		CMPXCHG R/M64,R64	Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX.
;--------------------------------------------------------
GENERAL	CMPXCHG8B	Compare and Exchange Bytes	CMPXCHG8B_CMPXCHG16B
CMPXCHG8B	M64		CMPXCHG8B M64	Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX.
GENERAL	CMPXCHG16B	Compare and Exchange Bytes	CMPXCHG8B_CMPXCHG16B
CMPXCHG16B	M128		CMPXCHG16B M128	Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX.
;--------------------------------------------------------
GENERAL	COMISD	Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS	COMISD
COMISD	XMM,XMM/M64	SSE2	COMISD XMM1,XMM2/M64	Compare low DP FP values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
GENERAL	VCOMISD	Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS	COMISD
VCOMISD	XMM,XMM/M64	AVX	VCOMISD XMM1,XMM2/M64	Compare low DP FP values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VCOMISD	XMM,XMM/M64{SAE}	AVX512_F	VCOMISD XMM1,XMM2/M64{SAE}	Compare low DP FP values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
;--------------------------------------------------------
GENERAL	COMISS	Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS	COMISS
COMISS	XMM,XMM/M32	SSE	COMISS XMM1,XMM2/M32	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
GENERAL	VCOMISS	Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS	COMISS
VCOMISS	XMM,XMM/M32	AVX	VCOMISS XMM1,XMM2/M32	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VCOMISS	XMM,XMM/M32{SAE}	AVX512_F	VCOMISS XMM1,XMM2/M32{SAE}	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
;--------------------------------------------------------
GENERAL	CPUID	CPU Identification	CPUID
CPUID		8086	CPUID	Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well).
;--------------------------------------------------------
GENERAL	CRC32	Accumulate CRC32 Value	CRC32
CRC32	R32,R/M8		CRC32 R32,R/M8	Accumulate CRC32 on r/m8.
CRC32	R32,R/M8		CRC32 R32,R/M8	Accumulate CRC32 on r/m8.
CRC32	R32,R/M16		CRC32 R32,R/M16	Accumulate CRC32 on r/m16.
CRC32	R32,R/M32		CRC32 R32,R/M32	Accumulate CRC32 on r/m32.
CRC32	R64,R/M8		CRC32 R64,R/M8	Accumulate CRC32 on r/m8.
CRC32	R64,R/M64		CRC32 R64,R/M64	Accumulate CRC32 on r/m64.
;--------------------------------------------------------
GENERAL	CVTDQ2PD	Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values	CVTDQ2PD
CVTDQ2PD	XMM,XMM/M64	SSE2	CVTDQ2PD XMM1,XMM2/M64	Convert two packed signed doubleword integers from xmm2/mem to two packed DP FP values in xmm1.
GENERAL	VCVTDQ2PD	Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values	CVTDQ2PD
VCVTDQ2PD	XMM,XMM/M64	AVX	VCVTDQ2PD XMM1,XMM2/M64	Convert two packed signed doubleword integers from xmm2/mem to two packed DP FP values in xmm1.
VCVTDQ2PD	YMM,XMM/M128	AVX	VCVTDQ2PD YMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed DP FP values in ymm1.
VCVTDQ2PD	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTDQ2PD XMM1{K1}{Z},XMM2/M128/M32BCST	Convert 2 packed signed doubleword integers from xmm2/m128/m32bcst to eight packed DP FP values in xmm1 with writemask k1.
VCVTDQ2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTDQ2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert 4 packed signed doubleword integers from xmm2/m128/m32bcst to 4 packed DP FP values in ymm1 with writemask k1.
VCVTDQ2PD	ZMM{K}{Z},YMM/M256/M32BCST	AVX512_F	VCVTDQ2PD ZMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed DP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	CVTDQ2PS	Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values	CVTDQ2PS
CVTDQ2PS	XMM,XMM/M128	SSE2	CVTDQ2PS XMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed SP FP values in xmm1.
GENERAL	VCVTDQ2PS	Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values	CVTDQ2PS
VCVTDQ2PS	XMM,XMM/M128	AVX	VCVTDQ2PS XMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed SP FP values in xmm1.
VCVTDQ2PS	YMM,YMM/M256	AVX	VCVTDQ2PS YMM1,YMM2/M256	Convert eight packed signed doubleword integers from ymm2/mem to eight packed SP FP values in ymm1.
VCVTDQ2PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTDQ2PS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed signed doubleword integers from xmm2/m128/m32bcst to four packed SP FP values in xmm1with writemask k1.
VCVTDQ2PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTDQ2PS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed SP FP values in ymm1with writemask k1.
VCVTDQ2PS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512_F	VCVTDQ2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed signed doubleword integers from zmm2/m512/m32bcst to sixteen packed SP FP values in zmm1with writemask k1.
;--------------------------------------------------------
GENERAL	CVTPD2DQ	Convert Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTPD2DQ
CVTPD2DQ	XMM,XMM/M128	SSE2	CVTPD2DQ XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two signed doubleword integers in xmm1.
GENERAL	VCVTPD2DQ	Convert Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTPD2DQ
VCVTPD2DQ	XMM,XMM/M128	AVX	VCVTPD2DQ XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two signed doubleword integers in xmm1.
VCVTPD2DQ	XMM,YMM/M256	AVX	VCVTPD2DQ XMM1,YMM2/M256	Convert four packed DP FP values in ymm2/mem to four signed doubleword integers in xmm1.
VCVTPD2DQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VCVTPD2DQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 subject to writemask k1.
VCVTPD2DQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VCVTPD2DQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 subject to writemask k1.
VCVTPD2DQ	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_F	VCVTPD2DQ YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed DP FP values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	CVTPD2PI	Convert Packed Double-Precision FP Values to Packed Dword Integers	CVTPD2PI
CVTPD2PI	MM,XMM/M128		CVTPD2PI MM,XMM/M128	Convert two packed DP FP values from xmm/m128 to two packed signed doubleword integers in mm.
;--------------------------------------------------------
GENERAL	CVTPD2PS	Convert Packed Double-Precision Floating-Point Values to Packed Single-Precision Floating-Point Values	CVTPD2PS
CVTPD2PS	XMM,XMM/M128	SSE2	CVTPD2PS XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two SP FP values in xmm1.
GENERAL	VCVTPD2PS	Convert Packed Double-Precision Floating-Point Values to Packed Single-Precision Floating-Point Values	CVTPD2PS
VCVTPD2PS	XMM,XMM/M128	AVX	VCVTPD2PS XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two SP FP values in xmm1.
VCVTPD2PS	XMM,YMM/M256	AVX	VCVTPD2PS XMM1,YMM2/M256	Convert four packed DP FP values in ymm2/mem to four SP FP values in xmm1.
VCVTPD2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VCVTPD2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values in xmm2/m128/m64bcst to two SP FP values in xmm1with writemask k1.
VCVTPD2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VCVTPD2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values in ymm2/m256/m64bcst to four SP FP values in xmm1with writemask k1.
VCVTPD2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_F	VCVTPD2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed DP FP values in zmm2/m512/m64bcst to eight SP FP values in ymm1with writemask k1.
;--------------------------------------------------------
GENERAL	CVTPI2PD	Convert Packed Dword Integers to Packed Double-Precision FP Values	CVTPI2PD
CVTPI2PD	XMM,MM/M64		CVTPI2PD XMM,MM/M64	Convert two packed signed doubleword integers from mm/mem64 to two packed DP FP values in xmm.
;--------------------------------------------------------
GENERAL	CVTPI2PS	Convert Packed Dword Integers to Packed Single-Precision FP Values	CVTPI2PS
CVTPI2PS	XMM,MM/M64		CVTPI2PS XMM,MM/M64	Convert two signed doubleword integers from mm/m64 to two SP FP values in xmm.
;--------------------------------------------------------
GENERAL	CVTPS2DQ	Convert Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTPS2DQ
CVTPS2DQ	XMM,XMM/M128	SSE2	CVTPS2DQ XMM1,XMM2/M128	Convert four packed SP FP values from xmm2/mem to four packed signed doubleword values in xmm1.
GENERAL	VCVTPS2DQ	Convert Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTPS2DQ
VCVTPS2DQ	XMM,XMM/M128	AVX	VCVTPS2DQ XMM1,XMM2/M128	Convert four packed SP FP values from xmm2/mem to four packed signed doubleword values in xmm1.
VCVTPS2DQ	YMM,YMM/M256	AVX	VCVTPS2DQ YMM1,YMM2/M256	Convert eight packed SP FP values from ymm2/mem to eight packed signed doubleword values in ymm1.
VCVTPS2DQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTPS2DQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 subject to writemask k1.
VCVTPS2DQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTPS2DQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 subject to writemask k1.
VCVTPS2DQ	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512_F	VCVTPS2DQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed SP FP values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	CVTPS2PD	Convert Packed Single-Precision Floating-Point Values to Packed Double-Precision Floating-Point Values	CVTPS2PD
CVTPS2PD	XMM,XMM/M64	SSE2	CVTPS2PD XMM1,XMM2/M64	Convert two packed SP FP values in xmm2/m64 to two packed DP FP values in xmm1.
GENERAL	VCVTPS2PD	Convert Packed Single-Precision Floating-Point Values to Packed Double-Precision Floating-Point Values	CVTPS2PD
VCVTPS2PD	XMM,XMM/M64	AVX	VCVTPS2PD XMM1,XMM2/M64	Convert two packed SP FP values in xmm2/m64 to two packed DP FP values in xmm1.
VCVTPS2PD	YMM,XMM/M128	AVX	VCVTPS2PD YMM1,XMM2/M128	Convert four packed SP FP values in xmm2/m128 to four packed DP FP values in ymm1.
VCVTPS2PD	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_F	VCVTPS2PD XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed SP FP values in xmm2/m64/m32bcst to packed DP FP values in xmm1 with writemask k1.
VCVTPS2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL	VCVTPS2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed SP FP values in xmm2/m128/m32bcst to packed DP FP values in ymm1 with writemask k1.
VCVTPS2PD	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512_F	VCVTPS2PD ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed SP FP values in ymm2/m256/b32bcst to eight packed DP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	CVTPS2PI	Convert Packed Single-Precision FP Values to Packed Dword Integers	CVTPS2PI
CVTPS2PI	MM,XMM/M64		CVTPS2PI MM,XMM/M64	Convert two packed SP FP values from xmm/m64 to two packed signed doubleword integers in mm.
;--------------------------------------------------------
GENERAL	CVTSD2SI	Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer	CVTSD2SI
CVTSD2SI	R32,XMM/M64	SSE2	CVTSD2SI R32,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed doubleword integer r32.
CVTSD2SI	R64,XMM/M64	SSE2	CVTSD2SI R64,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed quadword integer sign- extended into r64.
GENERAL	VCVTSD2SI	Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer	CVTSD2SI
VCVTSD2SI	R32,XMM/M64	AVX	VCVTSD2SI R32,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed doubleword integer r32.
VCVTSD2SI	R64,XMM/M64	AVX	VCVTSD2SI R64,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed quadword integer sign- extended into r64.
VCVTSD2SI	R32,XMM/M64{ER}	AVX512_F	VCVTSD2SI R32,XMM1/M64{ER}	Convert one DP FP value from xmm1/m64 to one signed doubleword integer r32.
VCVTSD2SI	R64,XMM/M64{ER}	AVX512_F	VCVTSD2SI R64,XMM1/M64{ER}	Convert one DP FP value from xmm1/m64 to one signed quadword integer sign- extended into r64.
;--------------------------------------------------------
GENERAL	CVTSD2SS	Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value	CVTSD2SS
CVTSD2SS	XMM,XMM/M64	SSE2	CVTSD2SS XMM1,XMM2/M64	Convert one DP FP value in xmm2/m64 to one SP FP value in xmm1.
GENERAL	VCVTSD2SS	Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value	CVTSD2SS
VCVTSD2SS	XMM,XMM,XMM/M64	AVX	VCVTSD2SS XMM1,XMM2,XMM3/M64	Convert one DP FP value in xmm3/m64 to one SP FP value and merge with high bits in xmm2.
VCVTSD2SS	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VCVTSD2SS XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Convert one DP FP value in xmm3/m64 to one SP FP value and merge with high bits in xmm2 under writemask k1.
;--------------------------------------------------------
GENERAL	CVTSI2SD	Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value	CVTSI2SD
CVTSI2SD	XMM,R32/M32	SSE2	CVTSI2SD XMM1,R32/M32	Convert one signed doubleword integer from r32/m32 to one DP FP value in xmm1.
CVTSI2SD	XMM,R/M64	SSE2	CVTSI2SD XMM1,R/M64	Convert one signed quadword integer from r/m64 to one DP FP value in xmm1.
GENERAL	VCVTSI2SD	Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value	CVTSI2SD
VCVTSI2SD	XMM,XMM,R/M32	AVX	VCVTSI2SD XMM1,XMM2,R/M32	Convert one signed doubleword integer from r/m32 to one DP FP value in xmm1.
VCVTSI2SD	XMM,XMM,R/M64	AVX	VCVTSI2SD XMM1,XMM2,R/M64	Convert one signed quadword integer from r/m64 to one DP FP value in xmm1.
VCVTSI2SD	XMM,XMM,R/M32	AVX512_F	VCVTSI2SD XMM1,XMM2,R/M32	Convert one signed doubleword integer from r/m32 to one DP FP value in xmm1.
VCVTSI2SD	XMM,XMM,R/M64{ER}	AVX512_F	VCVTSI2SD XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one DP FP value in xmm1.
;--------------------------------------------------------
GENERAL	CVTSI2SS	Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value	CVTSI2SS
CVTSI2SS	XMM,R/M32	SSE	CVTSI2SS XMM1,R/M32	Convert one signed doubleword integer from r/m32 to one SP FP value in xmm1.
CVTSI2SS	XMM,R/M64	SSE	CVTSI2SS XMM1,R/M64	Convert one signed quadword integer from r/m64 to one SP FP value in xmm1.
GENERAL	VCVTSI2SS	Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value	CVTSI2SS
VCVTSI2SS	XMM,XMM,R/M32	AVX	VCVTSI2SS XMM1,XMM2,R/M32	Convert one signed doubleword integer from r/m32 to one SP FP value in xmm1.
VCVTSI2SS	XMM,XMM,R/M64	AVX	VCVTSI2SS XMM1,XMM2,R/M64	Convert one signed quadword integer from r/m64 to one SP FP value in xmm1.
VCVTSI2SS	XMM,XMM,R/M32{ER}	AVX512_F	VCVTSI2SS XMM1,XMM2,R/M32{ER}	Convert one signed doubleword integer from r/m32 to one SP FP value in xmm1.
VCVTSI2SS	XMM,XMM,R/M64{ER}	AVX512_F	VCVTSI2SS XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one SP FP value in xmm1.
;--------------------------------------------------------
GENERAL	CVTSS2SD	Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value	CVTSS2SD
CVTSS2SD	XMM,XMM/M32	SSE2	CVTSS2SD XMM1,XMM2/M32	Convert one SP FP value in xmm2/m32 to one DP FP value in xmm1.
GENERAL	VCVTSS2SD	Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value	CVTSS2SD
VCVTSS2SD	XMM,XMM,XMM/M32	AVX	VCVTSS2SD XMM1,XMM2,XMM3/M32	Convert one SP FP value in xmm3/m32 to one DP FP value and merge with high bits of xmm2.
VCVTSS2SD	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_F	VCVTSS2SD XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Convert one SP FP value in xmm3/m32 to one DP FP value and merge with high bits of xmm2 under writemask k1.
;--------------------------------------------------------
GENERAL	CVTSS2SI	Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer	CVTSS2SI
CVTSS2SI	R32,XMM/M32	SSE	CVTSS2SI R32,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32.
CVTSS2SI	R64,XMM/M32	SSE	CVTSS2SI R64,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64.
GENERAL	VCVTSS2SI	Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer	CVTSS2SI
VCVTSS2SI	R32,XMM/M32	AVX	VCVTSS2SI R32,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32.
VCVTSS2SI	R64,XMM/M32	AVX	VCVTSS2SI R64,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64.
VCVTSS2SI	R32,XMM/M32{ER}	AVX512_F	VCVTSS2SI R32,XMM1/M32{ER}	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32.
VCVTSS2SI	R64,XMM/M32{ER}	AVX512_F	VCVTSS2SI R64,XMM1/M32{ER}	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64.
;--------------------------------------------------------
GENERAL	CVTTPD2DQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTTPD2DQ
CVTTPD2DQ	XMM,XMM/M128	SSE2	CVTTPD2DQ XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
GENERAL	VCVTTPD2DQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTTPD2DQ
VCVTTPD2DQ	XMM,XMM/M128	AVX	VCVTTPD2DQ XMM1,XMM2/M128	Convert two packed DP FP values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
VCVTTPD2DQ	XMM,YMM/M256	AVX	VCVTTPD2DQ XMM1,YMM2/M256	Convert four packed DP FP values in ymm2/mem to four signed doubleword integers in xmm1 using truncation.
VCVTTPD2DQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VCVTTPD2DQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2DQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VCVTTPD2DQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2DQ	YMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_F	VCVTTPD2DQ YMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed DP FP values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	CVTTPD2PI	Convert with Truncation Packed Double-Precision FP Values to Packed Dword Integers	CVTTPD2PI
CVTTPD2PI	MM,XMM/M128		CVTTPD2PI MM,XMM/M128	Convert two packer DP FP values from xmm/m128 to two packed signed doubleword integers in mm using truncation.
;--------------------------------------------------------
GENERAL	CVTTPS2DQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTTPS2DQ
CVTTPS2DQ	XMM,XMM/M128	SSE2	CVTTPS2DQ XMM1,XMM2/M128	Convert four packed SP FP values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
GENERAL	VCVTTPS2DQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTTPS2DQ
VCVTTPS2DQ	XMM,XMM/M128	AVX	VCVTTPS2DQ XMM1,XMM2/M128	Convert four packed SP FP values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
VCVTTPS2DQ	YMM,YMM/M256	AVX	VCVTTPS2DQ YMM1,YMM2/M256	Convert eight packed SP FP values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation.
VCVTTPS2DQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTTPS2DQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2DQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTTPS2DQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2DQ	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_F	VCVTTPS2DQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert sixteen packed SP FP values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	CVTTPS2PI	Convert with Truncation Packed Single-Precision FP Values to Packed Dword Integers	CVTTPS2PI
CVTTPS2PI	MM,XMM/M64		CVTTPS2PI MM,XMM/M64	Convert two SP FP values from xmm/m64 to two signed doubleword signed integers in mm using truncation.
;--------------------------------------------------------
GENERAL	CVTTSD2SI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Integer	CVTTSD2SI
CVTTSD2SI	R32,XMM/M64	SSE2	CVTTSD2SI R32,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
CVTTSD2SI	R64,XMM/M64	SSE2	CVTTSD2SI R64,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed quadword integer in r64 using truncation.
GENERAL	VCVTTSD2SI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Integer	CVTTSD2SI
VCVTTSD2SI	R32,XMM/M64	AVX	VCVTTSD2SI R32,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VCVTTSD2SI	R64,XMM/M64	AVX	VCVTTSD2SI R64,XMM1/M64	Convert one DP FP value from xmm1/m64 to one signed quadword integer in r64 using truncation.
VCVTTSD2SI	R32,XMM/M64{SAE}	AVX512_F	VCVTTSD2SI R32,XMM1/M64{SAE}	Convert one DP FP value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VCVTTSD2SI	R64,XMM/M64{SAE}	AVX512_F	VCVTTSD2SI R64,XMM1/M64{SAE}	Convert one DP FP value from xmm1/m64 to one signed quadword integer in r64 using truncation.
;--------------------------------------------------------
GENERAL	CVTTSS2SI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Integer	CVTTSS2SI
CVTTSS2SI	R32,XMM/M32	SSE	CVTTSS2SI R32,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
CVTTSS2SI	R64,XMM/M32	SSE	CVTTSS2SI R64,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64 using truncation.
GENERAL	VCVTTSS2SI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Integer	CVTTSS2SI
VCVTTSS2SI	R32,XMM/M32	AVX	VCVTTSS2SI R32,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VCVTTSS2SI	R64,XMM/M32	AVX	VCVTTSS2SI R64,XMM1/M32	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64 using truncation.
VCVTTSS2SI	R32,XMM/M32{SAE}	AVX512_F	VCVTTSS2SI R32,XMM1/M32{SAE}	Convert one SP FP value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VCVTTSS2SI	R64,XMM/M32{SAE}	AVX512_F	VCVTTSS2SI R64,XMM1/M32{SAE}	Convert one SP FP value from xmm1/m32 to one signed quadword integer in r64 using truncation.
;--------------------------------------------------------
GENERAL	CWD	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO
CWD		8086	CWD	DX:AX ← sign-extend of AX.
GENERAL	CDQ	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO
CDQ		8086	CDQ	EDX:EAX ← sign-extend of EAX.
GENERAL	CQO	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO
CQO		8086	CQO	RDX:RAX← sign-extend of RAX.
;--------------------------------------------------------
GENERAL	DAA	Decimal Adjust AL after Addition	DAA
DAA		8086	DAA	Decimal adjust AL after addition.
;--------------------------------------------------------
GENERAL	DAS	Decimal Adjust AL after Subtraction	DAS
DAS		8086	DAS	Decimal adjust AL after subtraction.
;--------------------------------------------------------
GENERAL	DEC	Decrement by 1	DEC
DEC	R/M8	8086	DEC R/M8	Decrement r/m8 by 1.
DEC	R/M8	8086	DEC R/M8	Decrement r/m8 by 1.
DEC	R/M16	8086	DEC R/M16	Decrement r/m16 by 1.
DEC	R/M32	386	DEC R/M32	Decrement r/m32 by 1.
DEC	R/M64	X64	DEC R/M64	Decrement r/m64 by 1.
DEC	R16	8086	DEC R16	Decrement r16 by 1.
DEC	R32	386	DEC R32	Decrement r32 by 1.
;--------------------------------------------------------
GENERAL	DIV	Unsigned Divide	DIV
DIV	R/M8	8086	DIV R/M8	Unsigned divide AX by r/m8, with result stored in AL ← Quotient, AH ← Remainder.
DIV	R/M8	8086	DIV R/M8	Unsigned divide AX by r/m8, with result stored in AL ← Quotient, AH ← Remainder.
DIV	R/M16	8086	DIV R/M16	Unsigned divide DX:AX by r/m16, with result stored in AX ← Quotient, DX ← Remainder.
DIV	R/M32	386	DIV R/M32	Unsigned divide EDX:EAX by r/m32, with result stored in EAX ← Quotient, EDX ← Remainder.
DIV	R/M64	X64	DIV R/M64	Unsigned divide RDX:RAX by r/m64, with result stored in RAX ← Quotient, RDX ← Remainder.
;--------------------------------------------------------
GENERAL	DIVPD	Divide Packed Double-Precision Floating-Point Values	DIVPD
DIVPD	XMM,XMM/M128	SSE2	DIVPD XMM1,XMM2/M128	Divide packed DP FP values in xmm1 by packed DP FP values in xmm2/mem.
GENERAL	VDIVPD	Divide Packed Double-Precision Floating-Point Values	DIVPD
VDIVPD	XMM,XMM,XMM/M128	AVX	VDIVPD XMM1,XMM2,XMM3/M128	Divide packed DP FP values in xmm2 by packed DP FP values in xmm3/mem.
VDIVPD	YMM,YMM,YMM/M256	AVX	VDIVPD YMM1,YMM2,YMM3/M256	Divide packed DP FP values in ymm2 by packed DP FP values in ymm3/mem.
VDIVPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VDIVPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Divide packed DP FP values in xmm2 by packed DP FP values in xmm3/m128/m64bcst and write results to xmm1 subject to writemask k1.
VDIVPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VDIVPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Divide packed DP FP values in ymm2 by packed DP FP values in ymm3/m256/m64bcst and write results to ymm1 subject to writemask k1.
VDIVPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VDIVPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Divide packed DP FP values in zmm2 by packed DP FP values in zmm3/m512/m64bcst and write results to zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	DIVPS	Divide Packed Single-Precision Floating-Point Values	DIVPS
DIVPS	XMM,XMM/M128	SSE	DIVPS XMM1,XMM2/M128	Divide packed SP FP values in xmm1 by packed SP FP values in xmm2/mem.
GENERAL	VDIVPS	Divide Packed Single-Precision Floating-Point Values	DIVPS
VDIVPS	XMM,XMM,XMM/M128	AVX	VDIVPS XMM1,XMM2,XMM3/M128	Divide packed SP FP values in xmm2 by packed SP FP values in xmm3/mem.
VDIVPS	YMM,YMM,YMM/M256	AVX	VDIVPS YMM1,YMM2,YMM3/M256	Divide packed SP FP values in ymm2 by packed SP FP values in ymm3/mem.
VDIVPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VDIVPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Divide packed SP FP values in xmm2 by packed SP FP values in xmm3/m128/m32bcst and write results to xmm1 subject to writemask k1.
VDIVPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VDIVPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Divide packed SP FP values in ymm2 by packed SP FP values in ymm3/m256/m32bcst and write results to ymm1 subject to writemask k1.
VDIVPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VDIVPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Divide packed SP FP values in zmm2 by packed SP FP values in zmm3/m512/m32bcst and write results to zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	DIVSD	Divide Scalar Double-Precision Floating-Point Value	DIVSD
DIVSD	XMM,XMM/M64	SSE2	DIVSD XMM1,XMM2/M64	Divide low DP FP value in xmm1 by low DP FP value in xmm2/m64.
GENERAL	VDIVSD	Divide Scalar Double-Precision Floating-Point Value	DIVSD
VDIVSD	XMM,XMM,XMM/M64	AVX	VDIVSD XMM1,XMM2,XMM3/M64	Divide low DP FP value in xmm2 by low DP FP value in xmm3/m64.
VDIVSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VDIVSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Divide low DP FP value in xmm2 by low DP FP value in xmm3/m64.
;--------------------------------------------------------
GENERAL	DIVSS	Divide Scalar Single-Precision Floating-Point Values	DIVSS
DIVSS	XMM,XMM/M32	SSE	DIVSS XMM1,XMM2/M32	Divide low SP FP value in xmm1 by low SP FP value in xmm2/m32.
GENERAL	VDIVSS	Divide Scalar Single-Precision Floating-Point Values	DIVSS
VDIVSS	XMM,XMM,XMM/M32	AVX	VDIVSS XMM1,XMM2,XMM3/M32	Divide low SP FP value in xmm2 by low SP FP value in xmm3/m32.
VDIVSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VDIVSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Divide low SP FP value in xmm2 by low SP FP value in xmm3/m32.
;--------------------------------------------------------
GENERAL	DPPD	Dot Product of Packed Double Precision Floating-Point Values	DPPD
DPPD	XMM,XMM/M128,IMM8	SSE4_1	DPPD XMM1,XMM2/M128,IMM8	Selectively multiply packed DP FP values from xmm1 with packed DP FP values from xmm2, add and selectively store the packed DP FP values to xmm1.
GENERAL	VDPPD	Dot Product of Packed Double Precision Floating-Point Values	DPPD
VDPPD	XMM,XMM,XMM/M128,IMM8	AVX	VDPPD XMM1,XMM2,XMM3/M128,IMM8	Selectively multiply packed DP FP values from xmm2 with packed DP FP values from xmm3, add and selectively store the packed DP FP values to xmm1.
;--------------------------------------------------------
GENERAL	DPPS	Dot Product of Packed Single Precision Floating-Point Values	DPPS
DPPS	XMM,XMM/M128,IMM8	SSE4_1	DPPS XMM1,XMM2/M128,IMM8	Selectively multiply packed SP FP values from xmm1 with packed SP FP values from xmm2, add and selectively store the packed SP FP values or zero values to xmm1.
GENERAL	VDPPS	Dot Product of Packed Single Precision Floating-Point Values	DPPS
VDPPS	XMM,XMM,XMM/M128,IMM8	AVX	VDPPS XMM1,XMM2,XMM3/M128,IMM8	Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1.
VDPPS	YMM,YMM,YMM/M256,IMM8	AVX	VDPPS YMM1,YMM2,YMM3/M256,IMM8	Multiply packed SP FP values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1.
;--------------------------------------------------------
GENERAL	EACCEPT	Accept Changes to an EPC Page	EACCEPT
EACCEPT		SGX2	EACCEPT	This leaf function accepts changes made by system software to an EPC page in the running enclave.
;--------------------------------------------------------
GENERAL	EACCEPTCOPY	Initialize a Pending Page	EACCEPTCOPY
EACCEPTCOPY		SGX2	EACCEPTCOPY	This leaf function initializes a dynamically allocated EPC page from another page in the EPC.
;--------------------------------------------------------
GENERAL	EADD	Add a Page to an Uninitialized Enclave	EADD
EADD		SGX1	EADD	This leaf function adds a page to an uninitialized enclave.
;--------------------------------------------------------
GENERAL	EAUG	Add a Page to an Initialized Enclave	EAUG
EAUG		SGX2	EAUG	This leaf function adds a page to an initialized enclave.
;--------------------------------------------------------
GENERAL	EBLOCK	Mark a page in EPC as Blocked	EBLOCK
EBLOCK		SGX1	EBLOCK	This leaf function marks a page in the EPC as blocked.
;--------------------------------------------------------
GENERAL	ECREATE	Create an SECS page in the Enclave Page Cache	ECREATE
ECREATE		SGX1	ECREATE	This leaf function begins an enclave build by creating an SECS page in EPC.
;--------------------------------------------------------
GENERAL	EDBGRD	Read From a Debug Enclave	EDBGRD
EDBGRD		SGX1	EDBGRD	This leaf function reads a dword/quadword from a debug enclave.
;--------------------------------------------------------
GENERAL	EDBGWR	Write to a Debug Enclave	EDBGWR
EDBGWR		SGX1	EDBGWR	This leaf function writes a dword/quadword to a debug enclave.
;--------------------------------------------------------
GENERAL	EDECVIRTCHILD	Decrement VIRTCHILDCNT in SECS	EDECVIRTCHILD
EDECVIRTCHILD			EDECVIRTCHILD	This leaf function decrements the SECS VIRTCHILDCNT field.
;--------------------------------------------------------
GENERAL	EENTER	Enters an Enclave	EENTER
EENTER		SGX1	EENTER	This leaf function is used to enter an enclave.
;--------------------------------------------------------
GENERAL	EEXIT	Exits an Enclave	EEXIT
EEXIT		SGX1	EEXIT	This leaf function is used to exit an enclave.
;--------------------------------------------------------
GENERAL	EEXTEND	Extend Uninitialized Enclave Measurement by 256 Bytes	EEXTEND
EEXTEND		SGX1	EEXTEND	This leaf function measures 256 bytes of an uninitialized enclave page.
;--------------------------------------------------------
GENERAL	EGETKEY	Retrieves a Cryptographic Key	EGETKEY
EGETKEY		SGX1	EGETKEY	This leaf function retrieves a cryptographic key.
;--------------------------------------------------------
GENERAL	EINCVIRTCHILD	Increment VIRTCHILDCNT in SECS	EINCVIRTCHILD
EINCVIRTCHILD			EINCVIRTCHILD	This leaf function increments the SECS VIRTCHILDCNT field.
;--------------------------------------------------------
GENERAL	EINIT	Initialize an Enclave for Execution	EINIT
EINIT		SGX1	EINIT	This leaf function initializes the enclave and makes it ready to execute enclave code.
;--------------------------------------------------------
GENERAL	ELDB	Load an EPC Page and Mark its State	ELDB_ELDU_ELDBC_ELBUC
ELDB		SGX1	ELDB	This leaf function loads, verifies an EPC page and marks the page as blocked.
GENERAL	ELDU	Load an EPC Page and Mark its State	ELDB_ELDU_ELDBC_ELBUC
ELDU		SGX1	ELDU	This leaf function loads, verifies an EPC page and marks the page as unblocked.
GENERAL	ELDBC	Load an EPC Page and Mark its State	ELDB_ELDU_ELDBC_ELBUC
ELDBC			ELDBC	This leaf function behaves lie ELDB but with improved conflict handling for oversubscription.
ELDBC			ELDBC	This leaf function behaves like ELDU but with improved conflict handling for oversubscription.
;--------------------------------------------------------
GENERAL	EMMS	Empty MMX Technology State	EMMS
EMMS		8086	EMMS	Set the x87 FPU tag word to empty.
;--------------------------------------------------------
GENERAL	EMODPE	Extend an EPC Page Permissions	EMODPE
EMODPE		SGX2	EMODPE	This leaf function extends the access rights of an existing EPC page.
;--------------------------------------------------------
GENERAL	EMODPR	Restrict the Permissions of an EPC Page	EMODPR
EMODPR		SGX2	EMODPR	This leaf function restricts the access rights associated with a EPC page in an initialized enclave.
;--------------------------------------------------------
GENERAL	EMODT	Change the Type of an EPC Page	EMODT
EMODT		SGX2	EMODT	This leaf function changes the type of an existing EPC page.
;--------------------------------------------------------
GENERAL	ENCLS	Execute an Enclave System Function of Specified Leaf Number	ENCLS
ENCLS			ENCLS	This instruction is used to execute privileged Intel SGX leaf functions that are used for managing and debugging the enclaves.
;--------------------------------------------------------
GENERAL	ENCLU	Execute an Enclave User Function of Specified Leaf Number	ENCLU
ENCLU			ENCLU	This instruction is used to execute non-privileged Intel SGX leaf functions.
;--------------------------------------------------------
GENERAL	ENCLV	Execute an Enclave VMM Function of Specified Leaf Number	ENCLV
ENCLV			ENCLV	This instruction is used to execute privileged SGX leaf functions that are reserved for VMM use. They are used for managing the enclaves.
;--------------------------------------------------------
GENERAL	ENTER	Make Stack Frame for Procedure Parameters	ENTER
ENTER	IMM16,0	8086	ENTER IMM16,0	Create a stack frame for a procedure.
ENTER	IMM16,1	8086	ENTER IMM16,1	Create a stack frame with a nested pointer for a procedure.
ENTER	IMM16,IMM8	8086	ENTER IMM16,IMM8	Create a stack frame with nested pointers for a procedure.
;--------------------------------------------------------
GENERAL	EPA	Add Version Array	EPA
EPA		SGX1	EPA	This leaf function adds a Version Array to the EPC.
;--------------------------------------------------------
GENERAL	ERDINFO	Read Type and Status Information About an EPC Page	ERDINFO
ERDINFO			ERDINFO	This leaf function returns type and status information about an EPC page.
;--------------------------------------------------------
GENERAL	EREMOVE	Remove a page from the EPC	EREMOVE
EREMOVE		SGX1	EREMOVE	This leaf function removes a page from the EPC.
;--------------------------------------------------------
GENERAL	EREPORT	Create a Cryptographic Report of the Enclave	EREPORT
EREPORT		SGX1	EREPORT	This leaf function creates a cryptographic report of the enclave.
;--------------------------------------------------------
GENERAL	ERESUME	Re-Enters an Enclave	ERESUME
ERESUME		SGX1	ERESUME	This leaf function is used to re-enter an enclave after an inter- rupt.
;--------------------------------------------------------
GENERAL	ESETCONTEXT	Set the ENCLAVECONTEXT Field in SECS	ESETCONTEXT
ESETCONTEXT			ESETCONTEXT	This leaf function sets the ENCLAVECONTEXT field in SECS.
;--------------------------------------------------------
GENERAL	ETRACK	Activates EBLOCK Checks	ETRACK
ETRACK		SGX1	ETRACK	This leaf function activates EBLOCK checks.
;--------------------------------------------------------
GENERAL	ETRACKC	Activates EBLOCK Checks	ETRACKC
ETRACKC			ETRACKC	This leaf function activates EBLOCK checks.
;--------------------------------------------------------
GENERAL	EWB	Invalidate an EPC Page and Write out to Main Memory	EWB
EWB		SGX1	EWB	This leaf function invalidates an EPC page and writes it out to main memory.
;--------------------------------------------------------
GENERAL	EXTRACTPS	Extract Packed Floating-Point Values	EXTRACTPS
EXTRACTPS	REG/M32,XMM,IMM8	SSE4_1	EXTRACTPS REG/M32,XMM1,IMM8	Extract one SP FP value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
GENERAL	VEXTRACTPS	Extract Packed Floating-Point Values	EXTRACTPS
VEXTRACTPS	REG/M32,XMM,IMM8	AVX	VEXTRACTPS REG/M32,XMM1,IMM8	Extract one SP FP value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
VEXTRACTPS	REG/M32,XMM,IMM8	AVX512_F	VEXTRACTPS REG/M32,XMM1,IMM8	Extract one SP FP value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
;--------------------------------------------------------
GENERAL	F2XM1	Compute 2x–1	F2XM1
F2XM1		8086	F2XM1	Replace ST(0) with (2ST(0) – 1).
;--------------------------------------------------------
GENERAL	FABS	Absolute Value	FABS
FABS		8086	FABS	Replace ST with its absolute value.
;--------------------------------------------------------
GENERAL	FADD	Add	FADD_FADDP_FIADD
FADD	M32FP	386	FADD M32FP	Add m32fp to ST(0) and store result in ST(0).
FADD	M64FP	X64	FADD M64FP	Add m64fp to ST(0) and store result in ST(0).
FADD	ST(0),ST(I)	8086	FADD ST(0),ST(I)	Add ST(0) to ST(i) and store result in ST(0).
FADD	ST(I),ST(0)	8086	FADD ST(I),ST(0)	Add ST(i) to ST(0) and store result in ST(i).
GENERAL	FADDP	Add	FADD_FADDP_FIADD
FADDP	ST(I),ST(0)	8086	FADDP ST(I),ST(0)	Add ST(0) to ST(i), store result in ST(i), and pop the register stack.
FADDP		8086	FADDP	Add ST(0) to ST(1), store result in ST(1), and pop the register stack.
GENERAL	FIADD	Add	FADD_FADDP_FIADD
FIADD	M32INT	386	FIADD M32INT	Add m32int to ST(0) and store result in ST(0).
FIADD	M16INT	8086	FIADD M16INT	Add m16int to ST(0) and store result in ST(0).
;--------------------------------------------------------
GENERAL	FBLD	Load Binary Coded Decimal	FBLD
FBLD	M80BCD	8086	FBLD M80BCD	Convert BCD value to FP and push onto the FPU stack.
;--------------------------------------------------------
GENERAL	FBSTP	Store BCD Integer and Pop	FBSTP
FBSTP	M80BCD	8086	FBSTP M80BCD	Store ST(0) in m80bcd and pop ST(0).
;--------------------------------------------------------
GENERAL	FCHS	Change Sign	FCHS
FCHS		8086	FCHS	Complements sign of ST(0).
;--------------------------------------------------------
GENERAL	FCLEX	Clear Exceptions	FCLEX_FNCLEX
FCLEX		8086	FCLEX	Clear FP exception flags after checking for pending unmasked FP exceptions.
GENERAL	FNCLEX	Clear Exceptions	FCLEX_FNCLEX
FNCLEX		8086	FNCLEX	Clear FP exception flags without checking for pending unmasked FP exceptions.
;--------------------------------------------------------
GENERAL	FCMOVB	Floating-Point Conditional Move	FCMOVcc
FCMOVB	ST(0),ST(I)	8086	FCMOVB ST(0),ST(I)	Move if below (CF=1).
GENERAL	FCMOVE	Floating-Point Conditional Move	FCMOVcc
FCMOVE	ST(0),ST(I)	8086	FCMOVE ST(0),ST(I)	Move if equal (ZF=1).
GENERAL	FCMOVBE	Floating-Point Conditional Move	FCMOVcc
FCMOVBE	ST(0),ST(I)	8086	FCMOVBE ST(0),ST(I)	Move if below or equal (CF=1 or ZF=1).
GENERAL	FCMOVU	Floating-Point Conditional Move	FCMOVcc
FCMOVU	ST(0),ST(I)	8086	FCMOVU ST(0),ST(I)	Move if unordered (PF=1).
GENERAL	FCMOVNB	Floating-Point Conditional Move	FCMOVcc
FCMOVNB	ST(0),ST(I)	8086	FCMOVNB ST(0),ST(I)	Move if not below (CF=0).
GENERAL	FCMOVNE	Floating-Point Conditional Move	FCMOVcc
FCMOVNE	ST(0),ST(I)	8086	FCMOVNE ST(0),ST(I)	Move if not equal (ZF=0).
GENERAL	FCMOVNBE	Floating-Point Conditional Move	FCMOVcc
FCMOVNBE	ST(0),ST(I)	8086	FCMOVNBE ST(0),ST(I)	Move if not below or equal (CF=0 and ZF=0).
GENERAL	FCMOVNU	Floating-Point Conditional Move	FCMOVcc
FCMOVNU	ST(0),ST(I)	8086	FCMOVNU ST(0),ST(I)	Move if not unordered (PF=0).
;--------------------------------------------------------
GENERAL	FCOM	Compare Floating Point Values	FCOM_FCOMP_FCOMPP
FCOM	M32FP	386	FCOM M32FP	Compare ST(0) with m32fp.
FCOM	M64FP	X64	FCOM M64FP	Compare ST(0) with m64fp.
FCOM	ST(I)	8086	FCOM ST(I)	Compare ST(0) with ST(i).
FCOM		8086	FCOM	Compare ST(0) with ST(1).
GENERAL	FCOMP	Compare Floating Point Values	FCOM_FCOMP_FCOMPP
FCOMP	M32FP	386	FCOMP M32FP	Compare ST(0) with m32fp and pop register stack.
FCOMP	M64FP	X64	FCOMP M64FP	Compare ST(0) with m64fp and pop register stack.
FCOMP	ST(I)	8086	FCOMP ST(I)	Compare ST(0) with ST(i) and pop register stack.
FCOMP		8086	FCOMP	Compare ST(0) with ST(1) and pop register stack.
GENERAL	FCOMPP	Compare Floating Point Values	FCOM_FCOMP_FCOMPP
FCOMPP		8086	FCOMPP	Compare ST(0) with ST(1) and pop register stack twice.
;--------------------------------------------------------
GENERAL	FCOMI	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP__FUCOMI_FUCOMIP
FCOMI	ST,ST(I)	8086	FCOMI ST,ST(I)	Compare ST(0) with ST(i) and set status flags accordingly.
GENERAL	FCOMIP	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP__FUCOMI_FUCOMIP
FCOMIP	ST,ST(I)	8086	FCOMIP ST,ST(I)	Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
GENERAL	FUCOMI	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP__FUCOMI_FUCOMIP
FUCOMI	ST,ST(I)	8086	FUCOMI ST,ST(I)	Compare ST(0) with ST(i), check for ordered values, and set status flags accordingly.
GENERAL	FUCOMIP	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP__FUCOMI_FUCOMIP
FUCOMIP	ST,ST(I)	8086	FUCOMIP ST,ST(I)	Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack.
;--------------------------------------------------------
GENERAL	FCOS	Cosine	FCOS
FCOS		8086	FCOS	Replace ST(0) with its approximate cosine.
;--------------------------------------------------------
GENERAL	FDECSTP	Decrement Stack-Top Pointer	FDECSTP
FDECSTP		8086	FDECSTP	Decrement TOP field in FPU status word.
;--------------------------------------------------------
GENERAL	FDIV	Divide	FDIV_FDIVP_FIDIV
FDIV	M32FP	386	FDIV M32FP	Divide ST(0) by m32fp and store result in ST(0).
FDIV	M64FP	X64	FDIV M64FP	Divide ST(0) by m64fp and store result in ST(0).
FDIV	ST(0),ST(I)	8086	FDIV ST(0),ST(I)	Divide ST(0) by ST(i) and store result in ST(0).
FDIV	ST(I),ST(0)	8086	FDIV ST(I),ST(0)	Divide ST(i) by ST(0) and store result in ST(i).
GENERAL	FDIVP	Divide	FDIV_FDIVP_FIDIV
FDIVP	ST(I),ST(0)	8086	FDIVP ST(I),ST(0)	Divide ST(i) by ST(0), store result in ST(i), and pop the register stack.
FDIVP		8086	FDIVP	Divide ST(1) by ST(0), store result in ST(1), and pop the register stack.
GENERAL	FIDIV	Divide	FDIV_FDIVP_FIDIV
FIDIV	M32INT	386	FIDIV M32INT	Divide ST(0) by m32int and store result in ST(0).
FIDIV	M16INT	8086	FIDIV M16INT	Divide ST(0) by m16int and store result in ST(0).
;--------------------------------------------------------
GENERAL	FDIVR	Reverse Divide	FDIVR_FDIVRP_FIDIVR
FDIVR	M32FP	386	FDIVR M32FP	Divide m32fp by ST(0) and store result in ST(0).
FDIVR	M64FP	X64	FDIVR M64FP	Divide m64fp by ST(0) and store result in ST(0).
FDIVR	ST(0),ST(I)	8086	FDIVR ST(0),ST(I)	Divide ST(i) by ST(0) and store result in ST(0).
FDIVR	ST(I),ST(0)	8086	FDIVR ST(I),ST(0)	Divide ST(0) by ST(i) and store result in ST(i).
GENERAL	FDIVRP	Reverse Divide	FDIVR_FDIVRP_FIDIVR
FDIVRP	ST(I),ST(0)	8086	FDIVRP ST(I),ST(0)	Divide ST(0) by ST(i), store result in ST(i), and pop the register stack.
FDIVRP		8086	FDIVRP	Divide ST(0) by ST(1), store result in ST(1), and pop the register stack.
GENERAL	FIDIVR	Reverse Divide	FDIVR_FDIVRP_FIDIVR
FIDIVR	M32INT	386	FIDIVR M32INT	Divide m32int by ST(0) and store result in ST(0).
FIDIVR	M16INT	8086	FIDIVR M16INT	Divide m16int by ST(0) and store result in ST(0).
;--------------------------------------------------------
GENERAL	FFREE	Free Floating-Point Register	FFREE
FFREE	ST(I)	8086	FFREE ST(I)	Sets tag for ST(i) to empty.
;--------------------------------------------------------
GENERAL	FICOM	Compare Integer	FICOM_FICOMP
FICOM	M16INT	8086	FICOM M16INT	Compare ST(0) with m16int.
FICOM	M32INT	386	FICOM M32INT	Compare ST(0) with m32int.
GENERAL	FICOMP	Compare Integer	FICOM_FICOMP
FICOMP	M16INT	8086	FICOMP M16INT	Compare ST(0) with m16int and pop stack register.
FICOMP	M32INT	386	FICOMP M32INT	Compare ST(0) with m32int and pop stack register.
;--------------------------------------------------------
GENERAL	FILD	Load Integer	FILD
FILD	M16INT	8086	FILD M16INT	Push m16int onto the FPU register stack.
FILD	M32INT	386	FILD M32INT	Push m32int onto the FPU register stack.
FILD	M64INT	X64	FILD M64INT	Push m64int onto the FPU register stack.
;--------------------------------------------------------
GENERAL	FINCSTP	Increment Stack-Top Pointer	FINCSTP
FINCSTP		8086	FINCSTP	Increment the TOP field in the FPU status register.
;--------------------------------------------------------
GENERAL	FINIT	Initialize Floating-Point Unit	FINIT_FNINIT
FINIT		8086	FINIT	Initialize FPU after checking for pending unmasked FP exceptions.
GENERAL	FNINIT	Initialize Floating-Point Unit	FINIT_FNINIT
FNINIT		8086	FNINIT	Initialize FPU without checking for pending unmasked FP exceptions.
;--------------------------------------------------------
GENERAL	FIST	Store Integer	FIST_FISTP
FIST	M16INT	8086	FIST M16INT	Store ST(0) in m16int.
FIST	M32INT	386	FIST M32INT	Store ST(0) in m32int.
GENERAL	FISTP	Store Integer	FIST_FISTP
FISTP	M16INT	8086	FISTP M16INT	Store ST(0) in m16int and pop register stack.
FISTP	M32INT	386	FISTP M32INT	Store ST(0) in m32int and pop register stack.
FISTP	M64INT	X64	FISTP M64INT	Store ST(0) in m64int and pop register stack.
;--------------------------------------------------------
GENERAL	FISTTP	Store Integer with Truncation	FISTTP
FISTTP	M16INT	8086	FISTTP M16INT	Store ST(0) in m16int with truncation.
FISTTP	M32INT	386	FISTTP M32INT	Store ST(0) in m32int with truncation.
FISTTP	M64INT	X64	FISTTP M64INT	Store ST(0) in m64int with truncation.
;--------------------------------------------------------
GENERAL	FLD	Load Floating Point Value	FLD
FLD	M32FP	386	FLD M32FP	Push m32fp onto the FPU register stack.
FLD	M64FP	X64	FLD M64FP	Push m64fp onto the FPU register stack.
FLD	M80FP	8086	FLD M80FP	Push m80fp onto the FPU register stack.
FLD	ST(I)	8086	FLD ST(I)	Push ST(i) onto the FPU register stack.
;--------------------------------------------------------
GENERAL	FLD1	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLD1		8086	FLD1	Push +1.0 onto the FPU register stack.
GENERAL	FLDL2T	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDL2T		8086	FLDL2T	Push log210 onto the FPU register stack.
GENERAL	FLDL2E	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDL2E		8086	FLDL2E	Push log2e onto the FPU register stack.
GENERAL	FLDPI	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDPI		8086	FLDPI	Push π onto the FPU register stack.
GENERAL	FLDLG2	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDLG2		8086	FLDLG2	Push log102 onto the FPU register stack.
GENERAL	FLDLN2	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDLN2		8086	FLDLN2	Push loge2 onto the FPU register stack.
GENERAL	FLDZ	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ
FLDZ		8086	FLDZ	Push +0.0 onto the FPU register stack.
;--------------------------------------------------------
GENERAL	FLDCW	Load x87 FPU Control Word	FLDCW
FLDCW	M2BYTE	8086	FLDCW M2BYTE	Load FPU control word from m2byte.
;--------------------------------------------------------
GENERAL	FLDENV	Load x87 FPU Environment	FLDENV
FLDENV	M14/28BYTE	8086	FLDENV M14/28BYTE	Load FPU environment from m14byte or m28byte.
;--------------------------------------------------------
GENERAL	FMUL	Multiply	FMUL_FMULP_FIMUL
FMUL	M32FP	386	FMUL M32FP	Multiply ST(0) by m32fp and store result in ST(0).
FMUL	M64FP	X64	FMUL M64FP	Multiply ST(0) by m64fp and store result in ST(0).
FMUL	ST(0),ST(I)	8086	FMUL ST(0),ST(I)	Multiply ST(0) by ST(i) and store result in ST(0).
FMUL	ST(I),ST(0)	8086	FMUL ST(I),ST(0)	Multiply ST(i) by ST(0) and store result in ST(i).
GENERAL	FMULP	Multiply	FMUL_FMULP_FIMUL
FMULP	ST(I),ST(0)	8086	FMULP ST(I),ST(0)	Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack.
FMULP		8086	FMULP	Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack.
GENERAL	FIMUL	Multiply	FMUL_FMULP_FIMUL
FIMUL	M32INT	386	FIMUL M32INT	Multiply ST(0) by m32int and store result in ST(0).
FIMUL	M16INT	8086	FIMUL M16INT	Multiply ST(0) by m16int and store result in ST(0).
;--------------------------------------------------------
GENERAL	FNOP	No Operation	FNOP
FNOP		8086	FNOP	No operation is performed.
;--------------------------------------------------------
GENERAL	FPATAN	Partial Arctangent	FPATAN
FPATAN		8086	FPATAN	Replace ST(1) with arctan(ST(1)/ST(0)) and pop the register stack.
;--------------------------------------------------------
GENERAL	FPREM	Partial Remainder	FPREM
FPREM		8086	FPREM	Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1).
;--------------------------------------------------------
GENERAL	FPREM1	Partial Remainder	FPREM1
FPREM1		8086	FPREM1	Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1).
;--------------------------------------------------------
GENERAL	FPTAN	Partial Tangent	FPTAN
FPTAN		8086	FPTAN	Replace ST(0) with its approximate tangent and push 1 onto the FPU stack.
;--------------------------------------------------------
GENERAL	FRNDINT	Round to Integer	FRNDINT
FRNDINT		8086	FRNDINT	Round ST(0) to an integer.
;--------------------------------------------------------
GENERAL	FRSTOR	Restore x87 FPU State	FRSTOR
FRSTOR	M94/108BYTE	8086	FRSTOR M94/108BYTE	Load FPU state from m94byte or m108byte.
;--------------------------------------------------------
GENERAL	FSAVE	Store x87 FPU State	FSAVE_FNSAVE
FSAVE	M94/108BYTE	8086	FSAVE M94/108BYTE	Store FPU state to m94byte or m108byte after checking for pending unmasked FP exceptions. Then re-initialize the FPU.
GENERAL	FNSAVE	Store x87 FPU State	FSAVE_FNSAVE
FNSAVE	M94/108BYTE	8086	FNSAVE M94/108BYTE	Store FPU environment to m94byte or m108byte without checking for pending unmasked FP exceptions. Then re-initialize the FPU.
;--------------------------------------------------------
GENERAL	FSCALE	Scale	FSCALE
FSCALE		8086	FSCALE	Scale ST(0) by ST(1).
;--------------------------------------------------------
GENERAL	FSIN	Sine	FSIN
FSIN		8086	FSIN	Replace ST(0) with the approximate of its sine.
;--------------------------------------------------------
GENERAL	FSINCOS	Sine and Cosine	FSINCOS
FSINCOS		8086	FSINCOS	Compute the sine and cosine of ST(0); replace ST(0) with the approximate sine, and push the approximate cosine onto the register stack.
;--------------------------------------------------------
GENERAL	FSQRT	Square Root	FSQRT
FSQRT		8086	FSQRT	Computes square root of ST(0) and stores the result in ST(0).
;--------------------------------------------------------
GENERAL	FST	Store Floating Point Value	FST_FSTP
FST	M32FP	386	FST M32FP	Copy ST(0) to m32fp.
FST	M64FP	X64	FST M64FP	Copy ST(0) to m64fp.
FST	ST(I)	8086	FST ST(I)	Copy ST(0) to ST(i).
GENERAL	FSTP	Store Floating Point Value	FST_FSTP
FSTP	M32FP	386	FSTP M32FP	Copy ST(0) to m32fp and pop register stack.
FSTP	M64FP	X64	FSTP M64FP	Copy ST(0) to m64fp and pop register stack.
FSTP	M80FP	8086	FSTP M80FP	Copy ST(0) to m80fp and pop register stack.
FSTP	ST(I)	8086	FSTP ST(I)	Copy ST(0) to ST(i) and pop register stack.
;--------------------------------------------------------
GENERAL	FSTCW	Store x87 FPU Control Word	FSTCW_FNSTCW
FSTCW	M2BYTE	8086	FSTCW M2BYTE	Store FPU control word to m2byte after checking for pending unmasked FP exceptions.
GENERAL	FNSTCW	Store x87 FPU Control Word	FSTCW_FNSTCW
FNSTCW	M2BYTE	8086	FNSTCW M2BYTE	Store FPU control word to m2byte without checking for pending unmasked FP exceptions.
;--------------------------------------------------------
GENERAL	FSTENV	Store x87 FPU Environment	FSTENV_FNSTENV
FSTENV	M14/28BYTE	8086	FSTENV M14/28BYTE	Store FPU environment to m14byte or m28byte after checking for pending unmasked FP exceptions. Then mask all FP exceptions.
GENERAL	FNSTENV	Store x87 FPU Environment	FSTENV_FNSTENV
FNSTENV	M14/28BYTE	8086	FNSTENV M14/28BYTE	Store FPU environment to m14byte or m28byte without checking for pending unmasked FP exceptions. Then mask all FP exceptions.
;--------------------------------------------------------
GENERAL	FSTSW	Store x87 FPU Status Word	FSTSW_FNSTSW
FSTSW	M2BYTE	8086	FSTSW M2BYTE	Store FPU status word at m2byte after checking for pending unmasked FP exceptions.
FSTSW	AX	8086	FSTSW AX	Store FPU status word in AX register after checking for pending unmasked FP exceptions.
GENERAL	FNSTSW	Store x87 FPU Status Word	FSTSW_FNSTSW
FNSTSW	M2BYTE	8086	FNSTSW M2BYTE	Store FPU status word at m2byte without checking for pending unmasked FP exceptions.
FNSTSW	AX	8086	FNSTSW AX	Store FPU status word in AX register without checking for pending unmasked FP exceptions.
;--------------------------------------------------------
GENERAL	FSUB	Subtract	FSUB_FSUBP_FISUB
FSUB	M32FP	386	FSUB M32FP	Subtract m32fp from ST(0) and store result in ST(0).
FSUB	M64FP	X64	FSUB M64FP	Subtract m64fp from ST(0) and store result in ST(0).
FSUB	ST(0),ST(I)	8086	FSUB ST(0),ST(I)	Subtract ST(i) from ST(0) and store result in ST(0).
FSUB	ST(I),ST(0)	8086	FSUB ST(I),ST(0)	Subtract ST(0) from ST(i) and store result in ST(i).
GENERAL	FSUBP	Subtract	FSUB_FSUBP_FISUB
FSUBP	ST(I),ST(0)	8086	FSUBP ST(I),ST(0)	Subtract ST(0) from ST(i), store result in ST(i), and pop register stack.
FSUBP		8086	FSUBP	Subtract ST(0) from ST(1), store result in ST(1), and pop register stack.
GENERAL	FISUB	Subtract	FSUB_FSUBP_FISUB
FISUB	M32INT	386	FISUB M32INT	Subtract m32int from ST(0) and store result in ST(0).
FISUB	M16INT	8086	FISUB M16INT	Subtract m16int from ST(0) and store result in ST(0).
;--------------------------------------------------------
GENERAL	FSUBR	Reverse Subtract	FSUBR_FSUBRP_FISUBR
FSUBR	M32FP	386	FSUBR M32FP	Subtract ST(0) from m32fp and store result in ST(0).
FSUBR	M64FP	X64	FSUBR M64FP	Subtract ST(0) from m64fp and store result in ST(0).
FSUBR	ST(0),ST(I)	8086	FSUBR ST(0),ST(I)	Subtract ST(0) from ST(i) and store result in ST(0).
FSUBR	ST(I),ST(0)	8086	FSUBR ST(I),ST(0)	Subtract ST(i) from ST(0) and store result in ST(i).
GENERAL	FSUBRP	Reverse Subtract	FSUBR_FSUBRP_FISUBR
FSUBRP	ST(I),ST(0)	8086	FSUBRP ST(I),ST(0)	Subtract ST(i) from ST(0), store result in ST(i), and pop register stack.
FSUBRP		8086	FSUBRP	Subtract ST(1) from ST(0), store result in ST(1), and pop register stack.
GENERAL	FISUBR	Reverse Subtract	FSUBR_FSUBRP_FISUBR
FISUBR	M32INT	386	FISUBR M32INT	Subtract ST(0) from m32int and store result in ST(0).
FISUBR	M16INT	8086	FISUBR M16INT	Subtract ST(0) from m16int and store result in ST(0).
;--------------------------------------------------------
GENERAL	FTST	TEST	FTST
FTST		8086	FTST	Compare ST(0) with 0.0.
;--------------------------------------------------------
GENERAL	FUCOM	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP
FUCOM	ST(I)	8086	FUCOM ST(I)	Compare ST(0) with ST(i).
FUCOM		8086	FUCOM	Compare ST(0) with ST(1).
GENERAL	FUCOMP	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP
FUCOMP	ST(I)	8086	FUCOMP ST(I)	Compare ST(0) with ST(i) and pop register stack.
FUCOMP		8086	FUCOMP	Compare ST(0) with ST(1) and pop register stack.
GENERAL	FUCOMPP	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP
FUCOMPP		8086	FUCOMPP	Compare ST(0) with ST(1) and pop register stack twice.
;--------------------------------------------------------
GENERAL	FXAM	Examine Floating-Point	FXAM
FXAM		8086	FXAM	Classify value or number in ST(0).
;--------------------------------------------------------
GENERAL	FXCH	Exchange Register Contents	FXCH
FXCH	ST(I)	8086	FXCH ST(I)	Exchange the contents of ST(0) and ST(i).
FXCH		8086	FXCH	Exchange the contents of ST(0) and ST(1).
;--------------------------------------------------------
GENERAL	FXRSTOR	Restore x87 FPU, MMX, XMM, and MXCSR State	FXRSTOR
FXRSTOR	M512BYTE		FXRSTOR M512BYTE	Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
GENERAL	FXRSTOR64	Restore x87 FPU, MMX, XMM, and MXCSR State	FXRSTOR
FXRSTOR64	M512BYTE		FXRSTOR64 M512BYTE	Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
;--------------------------------------------------------
GENERAL	FXSAVE	Save x87 FPU, MMX Technology, and SSE State	FXSAVE
FXSAVE	M512BYTE		FXSAVE M512BYTE	Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
GENERAL	FXSAVE64	Save x87 FPU, MMX Technology, and SSE State	FXSAVE
FXSAVE64	M512BYTE		FXSAVE64 M512BYTE	Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
;--------------------------------------------------------
GENERAL	FXTRACT	Extract Exponent and Significand	FXTRACT
FXTRACT		8086	FXTRACT	Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack.
;--------------------------------------------------------
GENERAL	FYL2X	Compute y ∗ log2x	FYL2X
FYL2X		8086	FYL2X	Replace ST(1) with (ST(1) ∗ log2ST(0)) and pop the register stack.
;--------------------------------------------------------
GENERAL	FYL2XP1	Compute y ∗ log2(x +1)	FYL2XP1
FYL2XP1		8086	FYL2XP1	Replace ST(1) with ST(1) ∗ log2(ST(0) + 1.0) and pop the register stack.
;--------------------------------------------------------
GENERAL	EXITAC	Exit Authenticated Code Execution Mode	GETSEC[EXITAC]
EXITAC		SMX	EXITAC	Exit authenticated code execution mode. RBX holds the Near Absolute Indirect jump target and EDX hold the exit parameter flags.
;--------------------------------------------------------
GENERAL	PARAMETERS	Report the SMX Parameters	GETSEC[PARAMETERS]
PARAMETERS		SMX	PARAMETERS	Report the SMX parameters. The parameters index is input in EBX with the result returned in EAX, EBX, and ECX.
;--------------------------------------------------------
GENERAL	SENTER	Enter a Measured Environment	GETSEC[SENTER]
SENTER		SMX	SENTER	Launch a measured environment. EBX holds the SINIT authenticated code module physical base address. ECX holds the SINIT authenticated code module size (bytes). EDX controls the level of functionality supported by the measured environment launch.
;--------------------------------------------------------
GENERAL	SEXIT	Exit Measured Environment	GETSEC[SEXIT]
SEXIT		SMX	SEXIT	Exit measured environment.
;--------------------------------------------------------
GENERAL	SMCTRL	SMX Mode Control	GETSEC[SMCTRL]
SMCTRL		SMX	SMCTRL	Perform specified SMX mode control as selected with the input EBX.
;--------------------------------------------------------
GENERAL	WAKEUP	Wake up sleeping processors in measured environment	GETSEC[WAKEUP]
WAKEUP		SMX	WAKEUP	Wake up the responding logical processors from the SENTER sleep state.
;--------------------------------------------------------
GENERAL	GF2P8AFFINEINVQB	Galois Field Affine Transformation Inverse	GF2P8AFFINEINVQB
GF2P8AFFINEINVQB	XMM,XMM/M128,IMM8	AVX512_GFNI	GF2P8AFFINEINVQB XMM1,XMM2/M128,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
GENERAL	VGF2P8AFFINEINVQB	Galois Field Affine Transformation Inverse	GF2P8AFFINEINVQB
VGF2P8AFFINEINVQB	XMM,XMM,XMM/M128,IMM8	AVX,AVX512_GFNI	VGF2P8AFFINEINVQB XMM1,XMM2,XMM3/M128,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
VGF2P8AFFINEINVQB	YMM,YMM,YMM/M256,IMM8	AVX,AVX512_GFNI	VGF2P8AFFINEINVQB YMM1,YMM2,YMM3/M256,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
VGF2P8AFFINEINVQB	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_GFNI	VGF2P8AFFINEINVQB XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
VGF2P8AFFINEINVQB	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_GFNI	VGF2P8AFFINEINVQB YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
VGF2P8AFFINEINVQB	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F,AVX512_GFNI	VGF2P8AFFINEINVQB ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Computes inverse affine transformation in the finite field GF(2^8).
;--------------------------------------------------------
GENERAL	GF2P8AFFINEQB	Galois Field Affine Transformation	GF2P8AFFINEQB
GF2P8AFFINEQB	XMM,XMM/M128,IMM8	AVX512_GFNI	GF2P8AFFINEQB XMM1,XMM2/M128,IMM8	Computes affine transformation in the finite field GF(2^8).
GENERAL	VGF2P8AFFINEQB	Galois Field Affine Transformation	GF2P8AFFINEQB
VGF2P8AFFINEQB	XMM,XMM,XMM/M128,IMM8	AVX,AVX512_GFNI	VGF2P8AFFINEQB XMM1,XMM2,XMM3/M128,IMM8	Computes affine transformation in the finite field GF(2^8).
VGF2P8AFFINEQB	YMM,YMM,YMM/M256,IMM8	AVX,AVX512_GFNI	VGF2P8AFFINEQB YMM1,YMM2,YMM3/M256,IMM8	Computes affine transformation in the finite field GF(2^8).
VGF2P8AFFINEQB	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_GFNI	VGF2P8AFFINEQB XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Computes affine transformation in the finite field GF(2^8).
VGF2P8AFFINEQB	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_GFNI	VGF2P8AFFINEQB YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Computes affine transformation in the finite field GF(2^8).
VGF2P8AFFINEQB	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F,AVX512_GFNI	VGF2P8AFFINEQB ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Computes affine transformation in the finite field GF(2^8).
;--------------------------------------------------------
GENERAL	GF2P8MULB	Galois Field Multiply Bytes	GF2P8MULB
GF2P8MULB	XMM,XMM/M128	AVX512_GFNI	GF2P8MULB XMM1,XMM2/M128	Multiplies elements in the finite field GF(2^8).
GENERAL	VGF2P8MULB	Galois Field Multiply Bytes	GF2P8MULB
VGF2P8MULB	XMM,XMM,XMM/M128	AVX,AVX512_GFNI	VGF2P8MULB XMM1,XMM2,XMM3/M128	Multiplies elements in the finite field GF(2^8).
VGF2P8MULB	YMM,YMM,YMM/M256	AVX,AVX512_GFNI	VGF2P8MULB YMM1,YMM2,YMM3/M256	Multiplies elements in the finite field GF(2^8).
VGF2P8MULB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_GFNI	VGF2P8MULB XMM1{K1}{Z},XMM2,XMM3/M128	Multiplies elements in the finite field GF(2^8).
VGF2P8MULB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_GFNI	VGF2P8MULB YMM1{K1}{Z},YMM2,YMM3/M256	Multiplies elements in the finite field GF(2^8).
VGF2P8MULB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_F,AVX512_GFNI	VGF2P8MULB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiplies elements in the finite field GF(2^8).
;--------------------------------------------------------
GENERAL	HADDPD	Packed Double-FP Horizontal Add	HADDPD
HADDPD	XMM,XMM/M128	SSE3	HADDPD XMM1,XMM2/M128	Horizontal add packed DP FP values from xmm2/m128 to xmm1.
GENERAL	VHADDPD	Packed Double-FP Horizontal Add	HADDPD
VHADDPD	XMM,XMM,XMM/M128	AVX	VHADDPD XMM1,XMM2,XMM3/M128	Horizontal add packed DP FP values from xmm2 and xmm3/mem.
VHADDPD	YMM,YMM,YMM/M256	AVX	VHADDPD YMM1,YMM2,YMM3/M256	Horizontal add packed DP FP values from ymm2 and ymm3/mem.
;--------------------------------------------------------
GENERAL	HADDPS	Packed Single-FP Horizontal Add	HADDPS
HADDPS	XMM,XMM/M128	SSE3	HADDPS XMM1,XMM2/M128	Horizontal add packed SP FP values from xmm2/m128 to xmm1.
GENERAL	VHADDPS	Packed Single-FP Horizontal Add	HADDPS
VHADDPS	XMM,XMM,XMM/M128	AVX	VHADDPS XMM1,XMM2,XMM3/M128	Horizontal add packed SP FP values from xmm2 and xmm3/mem.
VHADDPS	YMM,YMM,YMM/M256	AVX	VHADDPS YMM1,YMM2,YMM3/M256	Horizontal add packed SP FP values from ymm2 and ymm3/mem.
;--------------------------------------------------------
GENERAL	HLT	Halt	HLT
HLT		8086	HLT	Halt
;--------------------------------------------------------
GENERAL	HSUBPD	Packed Double-FP Horizontal Subtract	HSUBPD
HSUBPD	XMM,XMM/M128	SSE3	HSUBPD XMM1,XMM2/M128	Horizontal subtract packed DP FP values from xmm2/m128 to xmm1.
GENERAL	VHSUBPD	Packed Double-FP Horizontal Subtract	HSUBPD
VHSUBPD	XMM,XMM,XMM/M128	AVX	VHSUBPD XMM1,XMM2,XMM3/M128	Horizontal subtract packed DP FP values from xmm2 and xmm3/mem.
VHSUBPD	YMM,YMM,YMM/M256	AVX	VHSUBPD YMM1,YMM2,YMM3/M256	Horizontal subtract packed DP FP values from ymm2 and ymm3/mem.
;--------------------------------------------------------
GENERAL	HSUBPS	Packed Single-FP Horizontal Subtract	HSUBPS
HSUBPS	XMM,XMM/M128	SSE3	HSUBPS XMM1,XMM2/M128	Horizontal subtract packed SP FP values from xmm2/m128 to xmm1.
GENERAL	VHSUBPS	Packed Single-FP Horizontal Subtract	HSUBPS
VHSUBPS	XMM,XMM,XMM/M128	AVX	VHSUBPS XMM1,XMM2,XMM3/M128	Horizontal subtract packed SP FP values from xmm2 and xmm3/mem.
VHSUBPS	YMM,YMM,YMM/M256	AVX	VHSUBPS YMM1,YMM2,YMM3/M256	Horizontal subtract packed SP FP values from ymm2 and ymm3/mem.
;--------------------------------------------------------
GENERAL	IDIV	Signed Divide	IDIV
IDIV	R/M8	8086	IDIV R/M8	Signed divide AX by r/m8, with result stored in: AL ← Quotient, AH ← Remainder.
IDIV	R/M8	8086	IDIV R/M8	Signed divide AX by r/m8, with result stored in AL ← Quotient, AH ← Remainder.
IDIV	R/M16	8086	IDIV R/M16	Signed divide DX:AX by r/m16, with result stored in AX ← Quotient, DX ← Remainder.
IDIV	R/M32	386	IDIV R/M32	Signed divide EDX:EAX by r/m32, with result stored in EAX ← Quotient, EDX ← Remainder.
IDIV	R/M64	X64	IDIV R/M64	Signed divide RDX:RAX by r/m64, with result stored in RAX ← Quotient, RDX ← Remainder.
;--------------------------------------------------------
GENERAL	IMUL	Signed Multiply	IMUL
IMUL	R/M8	8086	IMUL R/M8	AX← AL ∗ r/m byte.
IMUL	R/M16	8086	IMUL R/M16	DX:AX ← AX ∗ r/m word.
IMUL	R/M32	386	IMUL R/M32	EDX:EAX ← EAX ∗ r/m32.
IMUL	R/M64	X64	IMUL R/M64	RDX:RAX ← RAX ∗ r/m64.
IMUL	R16,R/M16	8086	IMUL R16,R/M16	word register ← word register ∗ r/m16.
IMUL	R32,R/M32	386	IMUL R32,R/M32	doubleword register ← doubleword register ∗ r/m32.
IMUL	R64,R/M64	X64	IMUL R64,R/M64	Quadword register ← Quadword register ∗ r/m64.
IMUL	R16,R/M16,IMM8	8086	IMUL R16,R/M16,IMM8	word register ← r/m16 ∗ sign-extended immediate byte.
IMUL	R32,R/M32,IMM8	386	IMUL R32,R/M32,IMM8	doubleword register ← r/m32 ∗ sign-extended immediate byte.
IMUL	R64,R/M64,IMM8	X64	IMUL R64,R/M64,IMM8	Quadword register ← r/m64 ∗ sign-extended immediate byte.
IMUL	R16,R/M16,IMM16	8086	IMUL R16,R/M16,IMM16	word register ← r/m16 ∗ immediate word.
IMUL	R32,R/M32,IMM32	386	IMUL R32,R/M32,IMM32	doubleword register ← r/m32 ∗ immediate doubleword.
IMUL	R64,R/M64,IMM32	X64	IMUL R64,R/M64,IMM32	Quadword register ← r/m64 ∗ immediate doubleword.
;--------------------------------------------------------
GENERAL	IN	Input from Port	IN
IN	AL,IMM8	8086	IN AL,IMM8	Input byte from imm8 I/O port address into AL.
IN	AX,IMM8	8086	IN AX,IMM8	Input word from imm8 I/O port address into AX.
IN	EAX,IMM8	8086	IN EAX,IMM8	Input dword from imm8 I/O port address into EAX.
IN	AL,DX	8086	IN AL,DX	Input byte from I/O port in DX into AL.
IN	AX,DX	8086	IN AX,DX	Input word from I/O port in DX into AX.
IN	EAX,DX	8086	IN EAX,DX	Input doubleword from I/O port in DX into EAX.
;--------------------------------------------------------
GENERAL	INC	Increment by 1	INC
INC	R/M8	8086	INC R/M8	Increment r/m byte by 1.
INC	R/M8	8086	INC R/M8	Increment r/m byte by 1.
INC	R/M16	8086	INC R/M16	Increment r/m word by 1.
INC	R/M32	386	INC R/M32	Increment r/m doubleword by 1.
INC	R/M64	X64	INC R/M64	Increment r/m quadword by 1.
INC	R16	8086	INC R16	Increment word register by 1.
INC	R32	386	INC R32	Increment doubleword register by 1.
;--------------------------------------------------------
GENERAL	INS	Input from Port to String	INS_INSB_INSW_INSD
INS	M8,DX	8086	INS M8,DX	Input byte from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.*
INS	M16,DX	8086	INS M16,DX	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
INS	M32,DX	386	INS M32,DX	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
GENERAL	INSB	Input from Port to String	INS_INSB_INSW_INSD
INSB		8086	INSB	Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI.1
GENERAL	INSW	Input from Port to String	INS_INSB_INSW_INSD
INSW		8086	INSW	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
GENERAL	INSD	Input from Port to String	INS_INSB_INSW_INSD
INSD		8086	INSD	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
;--------------------------------------------------------
GENERAL	INSERTPS	Insert Scalar Single-Precision Floating-Point Value	INSERTPS
INSERTPS	XMM,XMM/M32,IMM8	SSE4_1	INSERTPS XMM1,XMM2/M32,IMM8	Insert a SP FP value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
GENERAL	VINSERTPS	Insert Scalar Single-Precision Floating-Point Value	INSERTPS
VINSERTPS	XMM,XMM,XMM/M32,IMM8	AVX	VINSERTPS XMM1,XMM2,XMM3/M32,IMM8	Insert a SP FP value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8.
VINSERTPS	XMM,XMM,XMM/M32,IMM8	AVX512_F	VINSERTPS XMM1,XMM2,XMM3/M32,IMM8	Insert a SP FP value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8.
;--------------------------------------------------------
GENERAL	INT3	Call to Interrupt Procedure	INT_n_INTO_INT3_INT1
INT3		8086	INT3	Generate breakpoint trap.
GENERAL	INT	Call to Interrupt Procedure	INT_n_INTO_INT3_INT1
INT	IMM8	8086	INT IMM8	Generate software interrupt with vector specified by immediate byte.
GENERAL	INTO	Call to Interrupt Procedure	INT_n_INTO_INT3_INT1
INTO		8086	INTO	Generate overflow trap if overflow flag is 1.
GENERAL	INT1	Call to Interrupt Procedure	INT_n_INTO_INT3_INT1
INT1		8086	INT1	Generate debug trap.
;--------------------------------------------------------
GENERAL	INVD	Invalidate Internal Caches	INVD
INVD		8086	INVD	Flush internal caches; initiate flushing of external caches.
;--------------------------------------------------------
GENERAL	INVLPG	Invalidate TLB Entries	INVLPG
INVLPG	M	8086	INVLPG M	Invalidate TLB entries for page containing m.
;--------------------------------------------------------
GENERAL	INVPCID	Invalidate Process-Context Identifier	INVPCID
INVPCID	R32,M128	INVPCID	INVPCID R32,M128	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descriptor in m128.
INVPCID	R64,M128	INVPCID	INVPCID R64,M128	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descriptor in m128.
;--------------------------------------------------------
GENERAL	IRET	Interrupt Return	IRET_IRETD
IRET		8086	IRET	Interrupt return (16-bit operand size).
GENERAL	IRETD	Interrupt Return	IRET_IRETD
IRETD		8086	IRETD	Interrupt return (32-bit operand size).
GENERAL	IRETQ	Interrupt Return	IRET_IRETD
IRETQ		8086	IRETQ	Interrupt return (64-bit operand size).
;--------------------------------------------------------
GENERAL	JA	Jump if Condition Is Met	Jcc
JA	REL8	8086	JA REL8	Jump short if above (CF=0 and ZF=0).
JA	REL16	386	JA REL16	Jump near if above (CF=0 and ZF=0). Not supported in 64-bit mode.
JA	REL32	386	JA REL32	Jump near if above (CF=0 and ZF=0).
GENERAL	JAE	Jump if Condition Is Met	Jcc
JAE	REL8	8086	JAE REL8	Jump short if above or equal (CF=0).
JAE	REL16	386	JAE REL16	Jump near if above or equal (CF=0). Not supported in 64-bit mode.
JAE	REL32	386	JAE REL32	Jump near if above or equal (CF=0).
GENERAL	JB	Jump if Condition Is Met	Jcc
JB	REL8	8086	JB REL8	Jump short if below (CF=1).
JB	REL16	386	JB REL16	Jump near if below (CF=1). Not supported in 64-bit mode.
JB	REL32	386	JB REL32	Jump near if below (CF=1).
GENERAL	JBE	Jump if Condition Is Met	Jcc
JBE	REL8	8086	JBE REL8	Jump short if below or equal (CF=1 or ZF=1).
JBE	REL16	386	JBE REL16	Jump near if below or equal (CF=1 or ZF=1). Not supported in 64-bit mode.
JBE	REL32	386	JBE REL32	Jump near if below or equal (CF=1 or ZF=1).
GENERAL	JC	Jump if Condition Is Met	Jcc
JC	REL8	8086	JC REL8	Jump short if carry (CF=1).
JC	REL16	386	JC REL16	Jump near if carry (CF=1). Not supported in 64-bit mode.
JC	REL32	386	JC REL32	Jump near if carry (CF=1).
GENERAL	JCXZ	Jump if Condition Is Met	Jcc
JCXZ	REL8	8086	JCXZ REL8	Jump short if CX register is 0.
GENERAL	JECXZ	Jump if Condition Is Met	Jcc
JECXZ	REL8	386	JECXZ REL8	Jump short if ECX register is 0.
GENERAL	JRCXZ	Jump if Condition Is Met	Jcc
JRCXZ	REL8	X64	JRCXZ REL8	Jump short if RCX register is 0.
GENERAL	JE	Jump if Condition Is Met	Jcc
JE	REL8	8086	JE REL8	Jump short if equal (ZF=1).
JE	REL16	386	JE REL16	Jump near if equal (ZF=1). Not supported in 64-bit mode.
JE	REL32	386	JE REL32	Jump near if equal (ZF=1).
GENERAL	JG	Jump if Condition Is Met	Jcc
JG	REL8	8086	JG REL8	Jump short if greater (ZF=0 and SF=OF).
JG	REL16	386	JG REL16	Jump near if greater (ZF=0 and SF=OF). Not supported in 64-bit mode.
JG	REL32	386	JG REL32	Jump near if greater (ZF=0 and SF=OF).
GENERAL	JGE	Jump if Condition Is Met	Jcc
JGE	REL8	8086	JGE REL8	Jump short if greater or equal (SF=OF).
JGE	REL16	386	JGE REL16	Jump near if greater or equal (SF=OF). Not supported in 64-bit mode.
JGE	REL32	386	JGE REL32	Jump near if greater or equal (SF=OF).
GENERAL	JL	Jump if Condition Is Met	Jcc
JL	REL8	8086	JL REL8	Jump short if less (SF≠ OF).
JL	REL16	386	JL REL16	Jump near if less (SF≠ OF). Not supported in 64-bit mode.
JL	REL32	386	JL REL32	Jump near if less (SF≠ OF).
GENERAL	JLE	Jump if Condition Is Met	Jcc
JLE	REL8	8086	JLE REL8	Jump short if less or equal (ZF=1 or SF≠ OF).
JLE	REL16	386	JLE REL16	Jump near if less or equal (ZF=1 or SF≠ OF). Not supported in 64-bit mode.
JLE	REL32	386	JLE REL32	Jump near if less or equal (ZF=1 or SF≠ OF).
GENERAL	JNA	Jump if Condition Is Met	Jcc
JNA	REL8	8086	JNA REL8	Jump short if not above (CF=1 or ZF=1).
JNA	REL16	386	JNA REL16	Jump near if not above (CF=1 or ZF=1). Not supported in 64-bit mode.
JNA	REL32	386	JNA REL32	Jump near if not above (CF=1 or ZF=1).
GENERAL	JNAE	Jump if Condition Is Met	Jcc
JNAE	REL8	8086	JNAE REL8	Jump short if not above or equal (CF=1).
JNAE	REL16	386	JNAE REL16	Jump near if not above or equal (CF=1). Not supported in 64-bit mode.
JNAE	REL32	386	JNAE REL32	Jump near if not above or equal (CF=1).
GENERAL	JNB	Jump if Condition Is Met	Jcc
JNB	REL8	8086	JNB REL8	Jump short if not below (CF=0).
JNB	REL16	386	JNB REL16	Jump near if not below (CF=0). Not supported in 64-bit mode.
JNB	REL32	386	JNB REL32	Jump near if not below (CF=0).
GENERAL	JNBE	Jump if Condition Is Met	Jcc
JNBE	REL8	8086	JNBE REL8	Jump short if not below or equal (CF=0 and ZF=0).
JNBE	REL16	386	JNBE REL16	Jump near if not below or equal (CF=0 and ZF=0). Not supported in 64-bit mode.
JNBE	REL32	386	JNBE REL32	Jump near if not below or equal (CF=0 and ZF=0).
GENERAL	JNC	Jump if Condition Is Met	Jcc
JNC	REL8	8086	JNC REL8	Jump short if not carry (CF=0).
JNC	REL16	386	JNC REL16	Jump near if not carry (CF=0). Not supported in 64-bit mode.
JNC	REL32	386	JNC REL32	Jump near if not carry (CF=0).
GENERAL	JNE	Jump if Condition Is Met	Jcc
JNE	REL8	8086	JNE REL8	Jump short if not equal (ZF=0).
JNE	REL16	386	JNE REL16	Jump near if not equal (ZF=0). Not supported in 64-bit mode.
JNE	REL32	386	JNE REL32	Jump near if not equal (ZF=0).
GENERAL	JNG	Jump if Condition Is Met	Jcc
JNG	REL8	8086	JNG REL8	Jump short if not greater (ZF=1 or SF≠ OF).
JNG	REL16	386	JNG REL16	Jump near if not greater (ZF=1 or SF≠ OF). Not supported in 64-bit mode.
JNG	REL32	386	JNG REL32	Jump near if not greater (ZF=1 or SF≠ OF).
GENERAL	JNGE	Jump if Condition Is Met	Jcc
JNGE	REL8	8086	JNGE REL8	Jump short if not greater or equal (SF≠ OF).
JNGE	REL16	386	JNGE REL16	Jump near if not greater or equal (SF≠ OF). Not supported in 64-bit mode.
JNGE	REL32	386	JNGE REL32	Jump near if not greater or equal (SF≠ OF).
GENERAL	JNL	Jump if Condition Is Met	Jcc
JNL	REL8	8086	JNL REL8	Jump short if not less (SF=OF).
JNL	REL16	386	JNL REL16	Jump near if not less (SF=OF). Not supported in 64-bit mode.
JNL	REL32	386	JNL REL32	Jump near if not less (SF=OF).
GENERAL	JNLE	Jump if Condition Is Met	Jcc
JNLE	REL8	8086	JNLE REL8	Jump short if not less or equal (ZF=0 and SF=OF).
JNLE	REL16	386	JNLE REL16	Jump near if not less or equal (ZF=0 and SF=OF). Not supported in 64-bit mode.
JNLE	REL32	386	JNLE REL32	Jump near if not less or equal (ZF=0 and SF=OF).
GENERAL	JNO	Jump if Condition Is Met	Jcc
JNO	REL8	8086	JNO REL8	Jump short if not overflow (OF=0).
JNO	REL16	386	JNO REL16	Jump near if not overflow (OF=0). Not supported in 64-bit mode.
JNO	REL32	386	JNO REL32	Jump near if not overflow (OF=0).
GENERAL	JNP	Jump if Condition Is Met	Jcc
JNP	REL8	8086	JNP REL8	Jump short if not parity (PF=0).
JNP	REL16	386	JNP REL16	Jump near if not parity (PF=0). Not supported in 64-bit mode.
JNP	REL32	386	JNP REL32	Jump near if not parity (PF=0).
GENERAL	JNS	Jump if Condition Is Met	Jcc
JNS	REL8	8086	JNS REL8	Jump short if not sign (SF=0).
JNS	REL16	386	JNS REL16	Jump near if not sign (SF=0). Not supported in 64-bit mode.
JNS	REL32	386	JNS REL32	Jump near if not sign (SF=0).
GENERAL	JNZ	Jump if Condition Is Met	Jcc
JNZ	REL8	8086	JNZ REL8	Jump short if not zero (ZF=0).
JNZ	REL16	386	JNZ REL16	Jump near if not zero (ZF=0). Not supported in 64-bit mode.
JNZ	REL32	386	JNZ REL32	Jump near if not zero (ZF=0).
GENERAL	JO	Jump if Condition Is Met	Jcc
JO	REL8	8086	JO REL8	Jump short if overflow (OF=1).
JO	REL16	386	JO REL16	Jump near if overflow (OF=1). Not supported in 64-bit mode.
JO	REL32	386	JO REL32	Jump near if overflow (OF=1).
GENERAL	JP	Jump if Condition Is Met	Jcc
JP	REL8	8086	JP REL8	Jump short if parity (PF=1).
JP	REL16	386	JP REL16	Jump near if parity (PF=1). Not supported in 64-bit mode.
JP	REL32	386	JP REL32	Jump near if parity (PF=1).
GENERAL	JPE	Jump if Condition Is Met	Jcc
JPE	REL8	8086	JPE REL8	Jump short if parity even (PF=1).
JPE	REL16	386	JPE REL16	Jump near if parity even (PF=1). Not supported in 64-bit mode.
JPE	REL32	386	JPE REL32	Jump near if parity even (PF=1).
GENERAL	JPO	Jump if Condition Is Met	Jcc
JPO	REL8	8086	JPO REL8	Jump short if parity odd (PF=0).
JPO	REL16	386	JPO REL16	Jump near if parity odd (PF=0). Not supported in 64-bit mode.
JPO	REL32	386	JPO REL32	Jump near if parity odd (PF=0).
GENERAL	JS	Jump if Condition Is Met	Jcc
JS	REL8	8086	JS REL8	Jump short if sign (SF=1).
JS	REL16	386	JS REL16	Jump near if sign (SF=1). Not supported in 64- bit mode.
JS	REL32	386	JS REL32	Jump near if sign (SF=1).
GENERAL	JZ	Jump if Condition Is Met	Jcc
JZ	REL8	8086	JZ REL8	Jump short if zero (ZF = 1).
JZ	REL16	386	JZ REL16	Jump near if 0 (ZF=1). Not supported in 64-bit mode.
JZ	REL32	386	JZ REL32	Jump near if 0 (ZF=1).
JZ	REL16	386	JZ REL16	Jump near if 0 (ZF=1). Not supported in 64-bit mode.
JZ	REL32	386	JZ REL32	Jump near if 0 (ZF=1).
;--------------------------------------------------------
GENERAL	JMP	Jump	JMP
JMP	REL8	8086	JMP REL8	Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits
JMP	REL16	386	JMP REL16	Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode.
JMP	REL32	386	JMP REL32	Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits
JMP	R/M16	8086	JMP R/M16	Jump near, absolute indirect, address = zero- extended r/m16. Not supported in 64-bit mode.
JMP	R/M32	386	JMP R/M32	Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode.
JMP	R/M64	X64	JMP R/M64	Jump near, absolute indirect, RIP = 64-Bit offset from register or memory
JMP	PTR16:16	8086	JMP PTR16:16	Jump far, absolute, address given in operand
JMP	PTR16:32	8086	JMP PTR16:32	Jump far, absolute, address given in operand
JMP	M16:16	8086	JMP M16:16	Jump far, absolute indirect, address given in m16:16
JMP	M16:32	8086	JMP M16:32	Jump far, absolute indirect, address given in m16:32.
JMP	M16:64	8086	JMP M16:64	Jump far, absolute indirect, address given in m16:64.
;--------------------------------------------------------
GENERAL	KADDW	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD
KADDW	K,K,K	AVX512_DQ	KADDW K1,K2,K3	Add 16 bits masks in k2 and k3 and place result in k1.
GENERAL	KADDB	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD
KADDB	K,K,K	AVX512_DQ	KADDB K1,K2,K3	Add 8 bits masks in k2 and k3 and place result in k1.
GENERAL	KADDQ	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD
KADDQ	K,K,K	AVX512_BW	KADDQ K1,K2,K3	Add 64 bits masks in k2 and k3 and place result in k1.
GENERAL	KADDD	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD
KADDD	K,K,K	AVX512_BW	KADDD K1,K2,K3	Add 32 bits masks in k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	KANDNW	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND
KANDNW	K,K,K	AVX512_F	KANDNW K1,K2,K3	Bitwise AND NOT 16 bits masks k2 and k3 and place result in k1.
GENERAL	KANDNB	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND
KANDNB	K,K,K	AVX512_DQ	KANDNB K1,K2,K3	Bitwise AND NOT 8 bits masks k1 and k2 and place result in k1.
GENERAL	KANDNQ	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND
KANDNQ	K,K,K	AVX512_BW	KANDNQ K1,K2,K3	Bitwise AND NOT 64 bits masks k2 and k3 and place result in k1.
GENERAL	KANDND	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND
KANDND	K,K,K	AVX512_BW	KANDND K1,K2,K3	Bitwise AND NOT 32 bits masks k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	KANDW	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD
KANDW	K,K,K	AVX512_F	KANDW K1,K2,K3	Bitwise AND 16 bits masks k2 and k3 and place result in k1.
GENERAL	KANDB	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD
KANDB	K,K,K	AVX512_DQ	KANDB K1,K2,K3	Bitwise AND 8 bits masks k2 and k3 and place result in k1.
GENERAL	KANDQ	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD
KANDQ	K,K,K	AVX512_BW	KANDQ K1,K2,K3	Bitwise AND 64 bits masks k2 and k3 and place result in k1.
GENERAL	KANDD	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD
KANDD	K,K,K	AVX512_BW	KANDD K1,K2,K3	Bitwise AND 32 bits masks k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	KMOVW	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD
KMOVW	K,K/M16	AVX512_F	KMOVW K1,K2/M16	Move 16 bits mask from k2/m16 and store the result in k1.
KMOVW	M16,K	AVX512_F	KMOVW M16,K1	Move 16 bits mask from k1 and store the result in m16.
KMOVW	K,R32	AVX512_F	KMOVW K1,R32	Move 16 bits mask from r32 to k1.
KMOVW	R32,K	AVX512_F	KMOVW R32,K1	Move 16 bits mask from k1 to r32.
GENERAL	KMOVB	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD
KMOVB	K,K/M8	AVX512_DQ	KMOVB K1,K2/M8	Move 8 bits mask from k2/m8 and store the result in k1.
KMOVB	M8,K	AVX512_DQ	KMOVB M8,K1	Move 8 bits mask from k1 and store the result in m8.
KMOVB	K,R32	AVX512_DQ	KMOVB K1,R32	Move 8 bits mask from r32 to k1.
KMOVB	R32,K	AVX512_DQ	KMOVB R32,K1	Move 8 bits mask from k1 to r32.
GENERAL	KMOVQ	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD
KMOVQ	K,K/M64	AVX512_BW	KMOVQ K1,K2/M64	Move 64 bits mask from k2/m64 and store the result in k1.
KMOVQ	M64,K	AVX512_BW	KMOVQ M64,K1	Move 64 bits mask from k1 and store the result in m64.
KMOVQ	K,R64	AVX512_BW	KMOVQ K1,R64	Move 64 bits mask from r64 to k1.
KMOVQ	R64,K	AVX512_BW	KMOVQ R64,K1	Move 64 bits mask from k1 to r64.
GENERAL	KMOVD	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD
KMOVD	K,K/M32	AVX512_BW	KMOVD K1,K2/M32	Move 32 bits mask from k2/m32 and store the result in k1.
KMOVD	M32,K	AVX512_BW	KMOVD M32,K1	Move 32 bits mask from k1 and store the result in m32.
KMOVD	K,R32	AVX512_BW	KMOVD K1,R32	Move 32 bits mask from r32 to k1.
KMOVD	R32,K	AVX512_BW	KMOVD R32,K1	Move 32 bits mask from k1 to r32.
;--------------------------------------------------------
GENERAL	KNOTW	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD
KNOTW	K,K	AVX512_F	KNOTW K1,K2	Bitwise NOT of 16 bits mask k2.
GENERAL	KNOTB	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD
KNOTB	K,K	AVX512_DQ	KNOTB K1,K2	Bitwise NOT of 8 bits mask k2.
GENERAL	KNOTQ	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD
KNOTQ	K,K	AVX512_BW	KNOTQ K1,K2	Bitwise NOT of 64 bits mask k2.
GENERAL	KNOTD	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD
KNOTD	K,K	AVX512_BW	KNOTD K1,K2	Bitwise NOT of 32 bits mask k2.
;--------------------------------------------------------
GENERAL	KORTESTW	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD
KORTESTW	K,K	AVX512_F	KORTESTW K1,K2	Bitwise OR 16 bits masks k1 and k2 and update ZF and CF accordingly.
GENERAL	KORTESTB	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD
KORTESTB	K,K	AVX512_DQ	KORTESTB K1,K2	Bitwise OR 8 bits masks k1 and k2 and update ZF and CF accordingly.
GENERAL	KORTESTQ	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD
KORTESTQ	K,K	AVX512_BW	KORTESTQ K1,K2	Bitwise OR 64 bits masks k1 and k2 and update ZF and CF accordingly.
GENERAL	KORTESTD	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD
KORTESTD	K,K	AVX512_BW	KORTESTD K1,K2	Bitwise OR 32 bits masks k1 and k2 and update ZF and CF accordingly.
;--------------------------------------------------------
GENERAL	KORW	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD
KORW	K,K,K	AVX512_F	KORW K1,K2,K3	Bitwise OR 16 bits masks k2 and k3 and place result in k1.
GENERAL	KORB	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD
KORB	K,K,K	AVX512_DQ	KORB K1,K2,K3	Bitwise OR 8 bits masks k2 and k3 and place result in k1.
GENERAL	KORQ	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD
KORQ	K,K,K	AVX512_BW	KORQ K1,K2,K3	Bitwise OR 64 bits masks k2 and k3 and place result in k1.
GENERAL	KORD	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD
KORD	K,K,K	AVX512_BW	KORD K1,K2,K3	Bitwise OR 32 bits masks k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	KSHIFTLW	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD
KSHIFTLW	K,K,IMM8	AVX512_F	KSHIFTLW K1,K2,IMM8	Shift left 16 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTLB	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD
KSHIFTLB	K,K,IMM8	AVX512_DQ	KSHIFTLB K1,K2,IMM8	Shift left 8 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTLQ	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD
KSHIFTLQ	K,K,IMM8	AVX512_BW	KSHIFTLQ K1,K2,IMM8	Shift left 64 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTLD	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD
KSHIFTLD	K,K,IMM8	AVX512_BW	KSHIFTLD K1,K2,IMM8	Shift left 32 bits in k2 by immediate and write result in k1.
;--------------------------------------------------------
GENERAL	KSHIFTRW	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD
KSHIFTRW	K,K,IMM8	AVX512_F	KSHIFTRW K1,K2,IMM8	Shift right 16 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTRB	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD
KSHIFTRB	K,K,IMM8	AVX512_DQ	KSHIFTRB K1,K2,IMM8	Shift right 8 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTRQ	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD
KSHIFTRQ	K,K,IMM8	AVX512_BW	KSHIFTRQ K1,K2,IMM8	Shift right 64 bits in k2 by immediate and write result in k1.
GENERAL	KSHIFTRD	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD
KSHIFTRD	K,K,IMM8	AVX512_BW	KSHIFTRD K1,K2,IMM8	Shift right 32 bits in k2 by immediate and write result in k1.
;--------------------------------------------------------
GENERAL	KTESTW	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD
KTESTW	K,K	AVX512_DQ	KTESTW K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 16 bits mask register sources.
GENERAL	KTESTB	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD
KTESTB	K,K	AVX512_DQ	KTESTB K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 8 bits mask register sources.
GENERAL	KTESTQ	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD
KTESTQ	K,K	AVX512_BW	KTESTQ K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 64 bits mask register sources.
GENERAL	KTESTD	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD
KTESTD	K,K	AVX512_BW	KTESTD K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 32 bits mask register sources.
;--------------------------------------------------------
GENERAL	KUNPCKBW	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ
KUNPCKBW	K,K,K	AVX512_F	KUNPCKBW K1,K2,K3	Unpack and interleave 8 bits masks in k2 and k3 and write word result in k1.
GENERAL	KUNPCKWD	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ
KUNPCKWD	K,K,K	AVX512_BW	KUNPCKWD K1,K2,K3	Unpack and interleave 16 bits in k2 and k3 and write double-word result in k1.
GENERAL	KUNPCKDQ	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ
KUNPCKDQ	K,K,K	AVX512_BW	KUNPCKDQ K1,K2,K3	Unpack and interleave 32 bits masks in k2 and k3 and write quadword result in k1.
;--------------------------------------------------------
GENERAL	KXNORW	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD
KXNORW	K,K,K	AVX512_F	KXNORW K1,K2,K3	Bitwise XNOR 16 bits masks k2 and k3 and place result in k1.
GENERAL	KXNORB	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD
KXNORB	K,K,K	AVX512_DQ	KXNORB K1,K2,K3	Bitwise XNOR 8 bits masks k2 and k3 and place result in k1.
GENERAL	KXNORQ	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD
KXNORQ	K,K,K	AVX512_BW	KXNORQ K1,K2,K3	Bitwise XNOR 64 bits masks k2 and k3 and place result in k1.
GENERAL	KXNORD	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD
KXNORD	K,K,K	AVX512_BW	KXNORD K1,K2,K3	Bitwise XNOR 32 bits masks k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	KXORW	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD
KXORW	K,K,K	AVX512_F	KXORW K1,K2,K3	Bitwise XOR 16 bits masks k2 and k3 and place result in k1.
GENERAL	KXORB	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD
KXORB	K,K,K	AVX512_DQ	KXORB K1,K2,K3	Bitwise XOR 8 bits masks k2 and k3 and place result in k1.
GENERAL	KXORQ	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD
KXORQ	K,K,K	AVX512_BW	KXORQ K1,K2,K3	Bitwise XOR 64 bits masks k2 and k3 and place result in k1.
GENERAL	KXORD	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD
KXORD	K,K,K	AVX512_BW	KXORD K1,K2,K3	Bitwise XOR 32 bits masks k2 and k3 and place result in k1.
;--------------------------------------------------------
GENERAL	LAHF	Load Status Flags into AH Register	LAHF
LAHF		8086	LAHF	Load: AH ← EFLAGS(SF:ZF:0:AF:0:PF:1:CF).
;--------------------------------------------------------
GENERAL	LAR	Load Access Rights Byte	LAR
LAR	R16,R16/M16	8086	LAR R16,R16/M16	r16 ← access rights referenced by r16/m16
LAR	REG,R32/M16	386	LAR REG,R32/M16	reg ← access rights referenced by r32/m16
;--------------------------------------------------------
GENERAL	LDDQU	Load Unaligned Integer 128 Bits	LDDQU
LDDQU	XMM,MEM	SSE3	LDDQU XMM1,MEM	Load unaligned data from mem and return double quadword in xmm1.
GENERAL	VLDDQU	Load Unaligned Integer 128 Bits	LDDQU
VLDDQU	XMM,M128	AVX	VLDDQU XMM1,M128	Load unaligned packed integer values from mem to xmm1.
VLDDQU	YMM,M256	AVX	VLDDQU YMM1,M256	Load unaligned packed integer values from mem to ymm1.
;--------------------------------------------------------
GENERAL	LDMXCSR	Load MXCSR Register	LDMXCSR
LDMXCSR	M32	SSE	LDMXCSR M32	Load MXCSR register from m32.
GENERAL	VLDMXCSR	Load MXCSR Register	LDMXCSR
VLDMXCSR	M32	AVX	VLDMXCSR M32	Load MXCSR register from m32.
;--------------------------------------------------------
GENERAL	LDS	Load Far Pointer	LDS_LES_LFS_LGS_LSS
LDS	R16,M16:16	8086	LDS R16,M16:16	Load DS:r16 with far pointer from memory.
LDS	R32,M16:32	386	LDS R32,M16:32	Load DS:r32 with far pointer from memory.
GENERAL	LSS	Load Far Pointer	LDS_LES_LFS_LGS_LSS
LSS	R16,M16:16	8086	LSS R16,M16:16	Load SS:r16 with far pointer from memory.
LSS	R32,M16:32	386	LSS R32,M16:32	Load SS:r32 with far pointer from memory.
LSS	R64,M16:64	X64	LSS R64,M16:64	Load SS:r64 with far pointer from memory.
GENERAL	LES	Load Far Pointer	LDS_LES_LFS_LGS_LSS
LES	R16,M16:16	8086	LES R16,M16:16	Load ES:r16 with far pointer from memory.
LES	R32,M16:32	386	LES R32,M16:32	Load ES:r32 with far pointer from memory.
GENERAL	LFS	Load Far Pointer	LDS_LES_LFS_LGS_LSS
LFS	R16,M16:16	8086	LFS R16,M16:16	Load FS:r16 with far pointer from memory.
LFS	R32,M16:32	386	LFS R32,M16:32	Load FS:r32 with far pointer from memory.
LFS	R64,M16:64	X64	LFS R64,M16:64	Load FS:r64 with far pointer from memory.
GENERAL	LGS	Load Far Pointer	LDS_LES_LFS_LGS_LSS
LGS	R16,M16:16	8086	LGS R16,M16:16	Load GS:r16 with far pointer from memory.
LGS	R32,M16:32	386	LGS R32,M16:32	Load GS:r32 with far pointer from memory.
LGS	R64,M16:64	X64	LGS R64,M16:64	Load GS:r64 with far pointer from memory.
;--------------------------------------------------------
GENERAL	LEA	Load Effective Address	LEA
LEA	R16,M	8086	LEA R16,M	Store effective address for m in register r16.
LEA	R32,M	386	LEA R32,M	Store effective address for m in register r32.
LEA	R64,M	X64	LEA R64,M	Store effective address for m in register r64.
;--------------------------------------------------------
GENERAL	LEAVE	High Level Procedure Exit	LEAVE
LEAVE		8086	LEAVE	Set SP to BP, then pop BP.
LEAVE		8086	LEAVE	Set ESP to EBP, then pop EBP.
LEAVE		8086	LEAVE	Set RSP to RBP, then pop RBP.
;--------------------------------------------------------
GENERAL	LFENCE	Load Fence	LFENCE
LFENCE		8086	LFENCE	Serializes load operations.
;--------------------------------------------------------
GENERAL	LGDT	Load Global/Interrupt Descriptor Table Register	LGDT_LIDT
LGDT	M16&32	8086	LGDT M16&32	Load m into GDTR.
LGDT	M16&64	8086	LGDT M16&64	Load m into GDTR.
GENERAL	LIDT	Load Global/Interrupt Descriptor Table Register	LGDT_LIDT
LIDT	M16&32	8086	LIDT M16&32	Load m into IDTR.
LIDT	M16&64	8086	LIDT M16&64	Load m into IDTR.
;--------------------------------------------------------
GENERAL	LLDT	Load Local Descriptor Table Register	LLDT
LLDT	R/M16	8086	LLDT R/M16	Load segment selector r/m16 into LDTR.
;--------------------------------------------------------
GENERAL	LMSW	Load Machine Status Word	LMSW
LMSW	R/M16	8086	LMSW R/M16	Loads r/m16 in machine status word of CR0.
;--------------------------------------------------------
GENERAL	LOCK	Assert LOCK# Signal Prefix	LOCK
LOCK		8086	LOCK	Asserts LOCK# signal for duration of the accompanying instruction.
;--------------------------------------------------------
GENERAL	LODS	Load String	LODS_LODSB_LODSW_LODSD_LODSQ
LODS	M8	8086	LODS M8	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
LODS	M16	8086	LODS M16	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
LODS	M32	386	LODS M32	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
LODS	M64	X64	LODS M64	Load qword at address (R)SI into RAX.
GENERAL	LODSB	Load String	LODS_LODSB_LODSW_LODSD_LODSQ
LODSB		8086	LODSB	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
GENERAL	LODSW	Load String	LODS_LODSB_LODSW_LODSD_LODSQ
LODSW		8086	LODSW	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
GENERAL	LODSD	Load String	LODS_LODSB_LODSW_LODSD_LODSQ
LODSD		8086	LODSD	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
GENERAL	LODSQ	Load String	LODS_LODSB_LODSW_LODSD_LODSQ
LODSQ		8086	LODSQ	Load qword at address (R)SI into RAX.
;--------------------------------------------------------
GENERAL	LOOP	Loop According to ECX Counter	LOOP_LOOPcc
LOOP	REL8	8086	LOOP REL8	Decrement count; jump short if count ≠ 0.
GENERAL	LOOPE	Loop According to ECX Counter	LOOP_LOOPcc
LOOPE	REL8	8086	LOOPE REL8	Decrement count; jump short if count ≠ 0 and ZF = 1.
GENERAL	LOOPNE	Loop According to ECX Counter	LOOP_LOOPcc
LOOPNE	REL8	8086	LOOPNE REL8	Decrement count; jump short if count ≠ 0 and ZF = 0.
;--------------------------------------------------------
GENERAL	LSL	Load Segment Limit	LSL
LSL	R16,R16/M16	8086	LSL R16,R16/M16	Load: r16 ← segment limit, selector r16/m16.
LSL	R32,R32/M16	386	LSL R32,R32/M16	Load: r32 ← segment limit, selector r32/m16.
LSL	R64,R32/M16	X64	LSL R64,R32/M16	Load: r64 ← segment limit, selector r32/m16
;--------------------------------------------------------
GENERAL	LTR	Load Task Register	LTR
LTR	R/M16	8086	LTR R/M16	Load r/m16 into task register.
;--------------------------------------------------------
GENERAL	LZCNT	Count the Number of Leading Zero Bits	LZCNT
LZCNT	R16,R/M16	LZCNT	LZCNT R16,R/M16	Count the number of leading zero bits in r/m16, return result in r16.
LZCNT	R32,R/M32	LZCNT	LZCNT R32,R/M32	Count the number of leading zero bits in r/m32, return result in r32.
LZCNT	R64,R/M64	LZCNT	LZCNT R64,R/M64	Count the number of leading zero bits in r/m64, return result in r64.
;--------------------------------------------------------
GENERAL	MASKMOVDQU	Store Selected Bytes of Double Quadword	MASKMOVDQU
MASKMOVDQU	XMM,XMM	SSE2	MASKMOVDQU XMM1,XMM2	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
GENERAL	VMASKMOVDQU	Store Selected Bytes of Double Quadword	MASKMOVDQU
VMASKMOVDQU	XMM,XMM	AVX	VMASKMOVDQU XMM1,XMM2	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
;--------------------------------------------------------
GENERAL	MASKMOVQ	Store Selected Bytes of Quadword	MASKMOVQ
MASKMOVQ	MM,MM		MASKMOVQ MM1,MM2	Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI.
;--------------------------------------------------------
GENERAL	MAXPD	Maximum of Packed Double-Precision Floating-Point Values	MAXPD
MAXPD	XMM,XMM/M128	SSE2	MAXPD XMM1,XMM2/M128	Return the maximum DP FP values between xmm1 and xmm2/m128.
GENERAL	VMAXPD	Maximum of Packed Double-Precision Floating-Point Values	MAXPD
VMAXPD	XMM,XMM,XMM/M128	AVX	VMAXPD XMM1,XMM2,XMM3/M128	Return the maximum DP FP values between xmm2 and xmm3/m128.
VMAXPD	YMM,YMM,YMM/M256	AVX	VMAXPD YMM1,YMM2,YMM3/M256	Return the maximum packed DP FP values between ymm2 and ymm3/m256.
VMAXPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VMAXPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the maximum packed DP FP values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1.
VMAXPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VMAXPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the maximum packed DP FP values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1.
VMAXPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE}	AVX512_F	VMAXPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE}	Return the maximum packed DP FP values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	MAXPS	Maximum of Packed Single-Precision Floating-Point Values	MAXPS
MAXPS	XMM,XMM/M128	SSE	MAXPS XMM1,XMM2/M128	Return the maximum SP FP values between xmm1 and xmm2/mem.
GENERAL	VMAXPS	Maximum of Packed Single-Precision Floating-Point Values	MAXPS
VMAXPS	XMM,XMM,XMM/M128	AVX	VMAXPS XMM1,XMM2,XMM3/M128	Return the maximum SP FP values between xmm2 and xmm3/mem.
VMAXPS	YMM,YMM,YMM/M256	AVX	VMAXPS YMM1,YMM2,YMM3/M256	Return the maximum SP FP values between ymm2 and ymm3/mem.
VMAXPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VMAXPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the maximum packed SP FP values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1.
VMAXPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VMAXPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the maximum packed SP FP values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1.
VMAXPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE}	AVX512_F	VMAXPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE}	Return the maximum packed SP FP values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	MAXSD	Return Maximum Scalar Double-Precision Floating-Point Value	MAXSD
MAXSD	XMM,XMM/M64	SSE2	MAXSD XMM1,XMM2/M64	Return the maximum scalar DP FP value between xmm2/m64 and xmm1.
GENERAL	VMAXSD	Return Maximum Scalar Double-Precision Floating-Point Value	MAXSD
VMAXSD	XMM,XMM,XMM/M64	AVX	VMAXSD XMM1,XMM2,XMM3/M64	Return the maximum scalar DP FP value between xmm3/m64 and xmm2.
VMAXSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512_F	VMAXSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Return the maximum scalar DP FP value between xmm3/m64 and xmm2.
;--------------------------------------------------------
GENERAL	MAXSS	Return Maximum Scalar Single-Precision Floating-Point Value	MAXSS
MAXSS	XMM,XMM/M32	SSE	MAXSS XMM1,XMM2/M32	Return the maximum scalar SP FP value between xmm2/m32 and xmm1.
GENERAL	VMAXSS	Return Maximum Scalar Single-Precision Floating-Point Value	MAXSS
VMAXSS	XMM,XMM,XMM/M32	AVX	VMAXSS XMM1,XMM2,XMM3/M32	Return the maximum scalar SP FP value between xmm3/m32 and xmm2.
VMAXSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_F	VMAXSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Return the maximum scalar SP FP value between xmm3/m32 and xmm2.
;--------------------------------------------------------
GENERAL	MFENCE	Memory Fence	MFENCE
MFENCE		8086	MFENCE	Serializes load and store operations.
;--------------------------------------------------------
GENERAL	MINPD	Minimum of Packed Double-Precision Floating-Point Values	MINPD
MINPD	XMM,XMM/M128	SSE2	MINPD XMM1,XMM2/M128	Return the minimum DP FP values between xmm1 and xmm2/mem
GENERAL	VMINPD	Minimum of Packed Double-Precision Floating-Point Values	MINPD
VMINPD	XMM,XMM,XMM/M128	AVX	VMINPD XMM1,XMM2,XMM3/M128	Return the minimum DP FP values between xmm2 and xmm3/mem.
VMINPD	YMM,YMM,YMM/M256	AVX	VMINPD YMM1,YMM2,YMM3/M256	Return the minimum packed DP FP values between ymm2 and ymm3/mem.
VMINPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VMINPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the minimum packed DP FP values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1.
VMINPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VMINPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the minimum packed DP FP values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1.
VMINPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE}	AVX512_F	VMINPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE}	Return the minimum packed DP FP values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	MINPS	Minimum of Packed Single-Precision Floating-Point Values	MINPS
MINPS	XMM,XMM/M128	SSE	MINPS XMM1,XMM2/M128	Return the minimum SP FP values between xmm1 and xmm2/mem.
GENERAL	VMINPS	Minimum of Packed Single-Precision Floating-Point Values	MINPS
VMINPS	XMM,XMM,XMM/M128	AVX	VMINPS XMM1,XMM2,XMM3/M128	Return the minimum SP FP values between xmm2 and xmm3/mem.
VMINPS	YMM,YMM,YMM/M256	AVX	VMINPS YMM1,YMM2,YMM3/M256	Return the minimum single DP FP values between ymm2 and ymm3/mem.
VMINPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VMINPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the minimum packed SP FP values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1.
VMINPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VMINPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the minimum packed SP FP values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1.
VMINPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE}	AVX512_F	VMINPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE}	Return the minimum packed SP FP values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	MINSD	Return Minimum Scalar Double-Precision Floating-Point Value	MINSD
MINSD	XMM,XMM/M64	SSE2	MINSD XMM1,XMM2/M64	Return the minimum scalar DP FP value between xmm2/m64 and xmm1.
GENERAL	VMINSD	Return Minimum Scalar Double-Precision Floating-Point Value	MINSD
VMINSD	XMM,XMM,XMM/M64	AVX	VMINSD XMM1,XMM2,XMM3/M64	Return the minimum scalar DP FP value between xmm3/m64 and xmm2.
VMINSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512_F	VMINSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Return the minimum scalar DP FP value between xmm3/m64 and xmm2.
;--------------------------------------------------------
GENERAL	MINSS	Return Minimum Scalar Single-Precision Floating-Point Value	MINSS
MINSS	XMM,XMM/M32	SSE	MINSS XMM1,XMM2/M32	Return the minimum scalar SP FP value between xmm2/m32 and xmm1.
GENERAL	VMINSS	Return Minimum Scalar Single-Precision Floating-Point Value	MINSS
VMINSS	XMM,XMM,XMM/M32	AVX	VMINSS XMM1,XMM2,XMM3/M32	Return the minimum scalar SP FP value between xmm3/m32 and xmm2.
VMINSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_F	VMINSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Return the minimum scalar SP FP value between xmm3/m32 and xmm2.
;--------------------------------------------------------
GENERAL	MONITOR	Set Up Monitor Address	MONITOR
MONITOR		8086	MONITOR	Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a writeback memory caching type. The address is DS:RAX/EAX/AX.
;--------------------------------------------------------
GENERAL	MOV	Move	MOV
MOV	R/M8,R8	8086	MOV R/M8,R8	Move r8 to r/m8.
MOV	R/M8,R8	8086	MOV R/M8,R8	Move r8 to r/m8.
MOV	R/M16,R16	8086	MOV R/M16,R16	Move r16 to r/m16.
MOV	R/M32,R32	386	MOV R/M32,R32	Move r32 to r/m32.
MOV	R/M64,R64	X64	MOV R/M64,R64	Move r64 to r/m64.
MOV	R8,R/M8	8086	MOV R8,R/M8	Move r/m8 to r8.
MOV	R8,R/M8	8086	MOV R8,R/M8	Move r/m8 to r8.
MOV	R16,R/M16	8086	MOV R16,R/M16	Move r/m16 to r16.
MOV	R32,R/M32	386	MOV R32,R/M32	Move r/m32 to r32.
MOV	R64,R/M64	X64	MOV R64,R/M64	Move r/m64 to r64.
MOV	R/M16,SREG	8086	MOV R/M16,SREG	Move segment register to r/m16.
MOV	R16/R32/M16,SREG	386	MOV R16/R32/M16,SREG	Move zero extended 16-bit segment register to r16/r32/r64/m16.
MOV	R64/M16,SREG	X64	MOV R64/M16,SREG	Move zero extended 16-bit segment register to r64/m16.
MOV	SREG,R/M16	8086	MOV SREG,R/M16	Move r/m16 to segment register.
MOV	SREG,R/M64	X64	MOV SREG,R/M64	Move lower 16 bits of r/m64 to segment register.
MOV	AL,MOFFS8	8086	MOV AL,MOFFS8	Move byte at (seg:offset) to AL.
MOV	AL,MOFFS8	8086	MOV AL,MOFFS8	Move byte at (offset) to AL.
MOV	AX,MOFFS16	8086	MOV AX,MOFFS16	Move word at (seg:offset) to AX.
MOV	EAX,MOFFS32	8086	MOV EAX,MOFFS32	Move doubleword at (seg:offset) to EAX.
MOV	RAX,MOFFS64	8086	MOV RAX,MOFFS64	Move quadword at (offset) to RAX.
MOV	MOFFS8,AL	8086	MOV MOFFS8,AL	Move AL to (seg:offset).
MOV	MOFFS8,AL	8086	MOV MOFFS8,AL	Move AL to (offset).
MOV	MOFFS16,AX	8086	MOV MOFFS16,AX	Move AX to (seg:offset).
MOV	MOFFS32,EAX	8086	MOV MOFFS32,EAX	Move EAX to (seg:offset).
MOV	MOFFS64,RAX	8086	MOV MOFFS64,RAX	Move RAX to (offset).
MOV	R8,IMM8	8086	MOV R8,IMM8	Move imm8 to r8.
MOV	R8,IMM8	8086	MOV R8,IMM8	Move imm8 to r8.
MOV	R16,IMM16	8086	MOV R16,IMM16	Move imm16 to r16.
MOV	R32,IMM32	386	MOV R32,IMM32	Move imm32 to r32.
MOV	R64,IMM64	X64	MOV R64,IMM64	Move imm64 to r64.
MOV	R/M8,IMM8	8086	MOV R/M8,IMM8	Move imm8 to r/m8.
MOV	R/M8,IMM8	8086	MOV R/M8,IMM8	Move imm8 to r/m8.
MOV	R/M16,IMM16	8086	MOV R/M16,IMM16	Move imm16 to r/m16.
MOV	R/M32,IMM32	386	MOV R/M32,IMM32	Move imm32 to r/m32.
MOV	R/M64,IMM32	X64	MOV R/M64,IMM32	Move imm32 sign extended to 64-bits to r/m64.
;--------------------------------------------------------
GENERAL	MOVAPD	Move Aligned Packed Double-Precision Floating-Point Values	MOVAPD
MOVAPD	XMM,XMM/M128	SSE2	MOVAPD XMM1,XMM2/M128	Move aligned packed DP FP values from xmm2/mem to xmm1.
MOVAPD	XMM/M128,XMM	SSE2	MOVAPD XMM2/M128,XMM1	Move aligned packed DP FP values from xmm1 to xmm2/mem.
GENERAL	VMOVAPD	Move Aligned Packed Double-Precision Floating-Point Values	MOVAPD
VMOVAPD	XMM,XMM/M128	AVX	VMOVAPD XMM1,XMM2/M128	Move aligned packed DP FP values from xmm2/mem to xmm1.
VMOVAPD	XMM/M128,XMM	AVX	VMOVAPD XMM2/M128,XMM1	Move aligned packed DP FP values from xmm1 to xmm2/mem.
VMOVAPD	YMM,YMM/M256	AVX	VMOVAPD YMM1,YMM2/M256	Move aligned packed DP FP values from ymm2/mem to ymm1.
VMOVAPD	YMM/M256,YMM	AVX	VMOVAPD YMM2/M256,YMM1	Move aligned packed DP FP values from ymm1 to ymm2/mem.
VMOVAPD	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVAPD XMM1{K1}{Z},XMM2/M128	Move aligned packed DP FP values from xmm2/m128 to xmm1 using writemask k1.
VMOVAPD	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVAPD YMM1{K1}{Z},YMM2/M256	Move aligned packed DP FP values from ymm2/m256 to ymm1 using writemask k1.
VMOVAPD	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVAPD ZMM1{K1}{Z},ZMM2/M512	Move aligned packed DP FP values from zmm2/m512 to zmm1 using writemask k1.
VMOVAPD	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVAPD XMM2/M128{K1}{Z},XMM1	Move aligned packed DP FP values from xmm1 to xmm2/m128 using writemask k1.
VMOVAPD	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVAPD YMM2/M256{K1}{Z},YMM1	Move aligned packed DP FP values from ymm1 to ymm2/m256 using writemask k1.
VMOVAPD	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVAPD ZMM2/M512{K1}{Z},ZMM1	Move aligned packed DP FP values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVAPS	Move Aligned Packed Single-Precision Floating-Point Values	MOVAPS
MOVAPS	XMM,XMM/M128	SSE	MOVAPS XMM1,XMM2/M128	Move aligned packed SP FP values from xmm2/mem to xmm1.
MOVAPS	XMM/M128,XMM	SSE	MOVAPS XMM2/M128,XMM1	Move aligned packed SP FP values from xmm1 to xmm2/mem.
GENERAL	VMOVAPS	Move Aligned Packed Single-Precision Floating-Point Values	MOVAPS
VMOVAPS	XMM,XMM/M128	AVX	VMOVAPS XMM1,XMM2/M128	Move aligned packed SP FP values from xmm2/mem to xmm1.
VMOVAPS	XMM/M128,XMM	AVX	VMOVAPS XMM2/M128,XMM1	Move aligned packed SP FP values from xmm1 to xmm2/mem.
VMOVAPS	YMM,YMM/M256	AVX	VMOVAPS YMM1,YMM2/M256	Move aligned packed SP FP values from ymm2/mem to ymm1.
VMOVAPS	YMM/M256,YMM	AVX	VMOVAPS YMM2/M256,YMM1	Move aligned packed SP FP values from ymm1 to ymm2/mem.
VMOVAPS	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVAPS XMM1{K1}{Z},XMM2/M128	Move aligned packed SP FP values from xmm2/m128 to xmm1 using writemask k1.
VMOVAPS	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVAPS YMM1{K1}{Z},YMM2/M256	Move aligned packed SP FP values from ymm2/m256 to ymm1 using writemask k1.
VMOVAPS	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVAPS ZMM1{K1}{Z},ZMM2/M512	Move aligned packed SP FP values from zmm2/m512 to zmm1 using writemask k1.
VMOVAPS	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVAPS XMM2/M128{K1}{Z},XMM1	Move aligned packed SP FP values from xmm1 to xmm2/m128 using writemask k1.
VMOVAPS	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVAPS YMM2/M256{K1}{Z},YMM1	Move aligned packed SP FP values from ymm1 to ymm2/m256 using writemask k1.
VMOVAPS	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVAPS ZMM2/M512{K1}{Z},ZMM1	Move aligned packed SP FP values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVBE	Move Data After Swapping Bytes	MOVBE
MOVBE	R16,M16	8086	MOVBE R16,M16	Reverse byte order in m16 and move to r16.
MOVBE	R32,M32	386	MOVBE R32,M32	Reverse byte order in m32 and move to r32.
MOVBE	R64,M64	X64	MOVBE R64,M64	Reverse byte order in m64 and move to r64.
MOVBE	M16,R16	8086	MOVBE M16,R16	Reverse byte order in r16 and move to m16.
MOVBE	M32,R32	386	MOVBE M32,R32	Reverse byte order in r32 and move to m32.
MOVBE	M64,R64	X64	MOVBE M64,R64	Reverse byte order in r64 and move to m64.
;--------------------------------------------------------
GENERAL	MOVD	Move Doubleword/Move Quadword	MOVD_MOVQ
MOVD	MM,R/M32	MMX	MOVD MM,R/M32	Move doubleword from r/m32 to mm.
MOVD	R/M32,MM	MMX	MOVD R/M32,MM	Move doubleword from mm to r/m32.
MOVD	XMM,R/M32	SSE2	MOVD XMM,R/M32	Move doubleword from r/m32 to xmm.
MOVD	R/M32,XMM	SSE2	MOVD R/M32,XMM	Move doubleword from xmm register to r/m32.
GENERAL	MOVQ	Move Doubleword/Move Quadword	MOVD_MOVQ
MOVQ	MM,R/M64	MMX	MOVQ MM,R/M64	Move quadword from r/m64 to mm.
MOVQ	R/M64,MM	MMX	MOVQ R/M64,MM	Move quadword from mm to r/m64.
MOVQ	XMM,R/M64	SSE2	MOVQ XMM,R/M64	Move quadword from r/m64 to xmm.
MOVQ	R/M64,XMM	SSE2	MOVQ R/M64,XMM	Move quadword from xmm register to r/m64.
GENERAL	VMOVD	Move Doubleword/Move Quadword	MOVD_MOVQ
VMOVD	XMM,R32/M32	AVX	VMOVD XMM1,R32/M32	Move doubleword from r/m32 to xmm1.
VMOVD	R32/M32,XMM	AVX	VMOVD R32/M32,XMM1	Move doubleword from xmm1 register to r/m32.
VMOVD	XMM,R32/M32	AVX512_F	VMOVD XMM1,R32/M32	Move doubleword from r/m32 to xmm1.
VMOVD	R32/M32,XMM	AVX512_F	VMOVD R32/M32,XMM1	Move doubleword from xmm1 register to r/m32.
GENERAL	VMOVQ	Move Doubleword/Move Quadword	MOVD_MOVQ
VMOVQ	XMM,R64/M64	AVX	VMOVQ XMM1,R64/M64	Move quadword from r/m64 to xmm1.
VMOVQ	R64/M64,XMM	AVX	VMOVQ R64/M64,XMM1	Move quadword from xmm1 register to r/m64.
VMOVQ	XMM,R64/M64	AVX512_F	VMOVQ XMM1,R64/M64	Move quadword from r/m64 to xmm1.
VMOVQ	R64/M64,XMM	AVX512_F	VMOVQ R64/M64,XMM1	Move quadword from xmm1 register to r/m64.
;--------------------------------------------------------
GENERAL	MOVDDUP	Replicate Double FP Values	MOVDDUP
MOVDDUP	XMM,XMM/M64	SSE3	MOVDDUP XMM1,XMM2/M64	Move DP FP value from xmm2/m64 and duplicate into xmm1.
GENERAL	VMOVDDUP	Replicate Double FP Values	MOVDDUP
VMOVDDUP	XMM,XMM/M64	AVX	VMOVDDUP XMM1,XMM2/M64	Move DP FP value from xmm2/m64 and duplicate into xmm1.
VMOVDDUP	YMM,YMM/M256	AVX	VMOVDDUP YMM1,YMM2/M256	Move even index DP FP values from ymm2/mem and duplicate each element into ymm1.
VMOVDDUP	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VMOVDDUP XMM1{K1}{Z},XMM2/M64	Move DP FP value from xmm2/m64 and duplicate each element into xmm1 subject to writemask k1.
VMOVDDUP	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVDDUP YMM1{K1}{Z},YMM2/M256	Move even index DP FP values from ymm2/m256 and duplicate each element into ymm1 subject to writemask k1.
VMOVDDUP	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVDDUP ZMM1{K1}{Z},ZMM2/M512	Move even index DP FP values from zmm2/m512 and duplicate each element into zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	MOVDIR64B	Move 64 Bytes as Direct Store	MOVDIR64B
MOVDIR64B	R16/R32/R64,M512	MOVDIR64B	MOVDIR64B R16/R32/R64,M512	Move 64-bytes as direct-store with guaranteed 64-byte write atomicity from the source memory operand address to destination memory address specified as offset to ES segment in the register operand.
;--------------------------------------------------------
GENERAL	MOVDIRI	Move Doubleword as Direct Store	MOVDIRI
MOVDIRI	M32,R32	MOVDIRI	MOVDIRI M32,R32	Move doubleword from r32 to m32 using direct store.
MOVDIRI	M64,R64	MOVDIRI	MOVDIRI M64,R64	Move quadword from r64 to m64 using direct store.
;--------------------------------------------------------
GENERAL	MOVDQ2Q	Move Quadword from XMM to MMX Technology Register	MOVDQ2Q
MOVDQ2Q	MM,XMM	SSE	MOVDQ2Q MM,XMM	Move low quadword from xmm to mmx register.
;--------------------------------------------------------
GENERAL	MOVDQA	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64
MOVDQA	XMM,XMM/M128	SSE2	MOVDQA XMM1,XMM2/M128	Move aligned packed integer values from xmm2/mem to xmm1.
MOVDQA	XMM/M128,XMM	SSE2	MOVDQA XMM2/M128,XMM1	Move aligned packed integer values from xmm1 to xmm2/mem.
GENERAL	VMOVDQA	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64
VMOVDQA	XMM,XMM/M128	AVX	VMOVDQA XMM1,XMM2/M128	Move aligned packed integer values from xmm2/mem to xmm1.
VMOVDQA	XMM/M128,XMM	AVX	VMOVDQA XMM2/M128,XMM1	Move aligned packed integer values from xmm1 to xmm2/mem.
VMOVDQA	YMM,YMM/M256	AVX	VMOVDQA YMM1,YMM2/M256	Move aligned packed integer values from ymm2/mem to ymm1.
VMOVDQA	YMM/M256,YMM	AVX	VMOVDQA YMM2/M256,YMM1	Move aligned packed integer values from ymm1 to ymm2/mem.
GENERAL	VMOVDQA32	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64
VMOVDQA32	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVDQA32 XMM1{K1}{Z},XMM2/M128	Move aligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQA32	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVDQA32 YMM1{K1}{Z},YMM2/M256	Move aligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQA32	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVDQA32 ZMM1{K1}{Z},ZMM2/M512	Move aligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQA32	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVDQA32 XMM2/M128{K1}{Z},XMM1	Move aligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQA32	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVDQA32 YMM2/M256{K1}{Z},YMM1	Move aligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQA32	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVDQA32 ZMM2/M512{K1}{Z},ZMM1	Move aligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1.
GENERAL	VMOVDQA64	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64
VMOVDQA64	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVDQA64 XMM1{K1}{Z},XMM2/M128	Move aligned quadword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQA64	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVDQA64 YMM1{K1}{Z},YMM2/M256	Move aligned quadword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQA64	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVDQA64 ZMM1{K1}{Z},ZMM2/M512	Move aligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQA64	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVDQA64 XMM2/M128{K1}{Z},XMM1	Move aligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQA64	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVDQA64 YMM2/M256{K1}{Z},YMM1	Move aligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQA64	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVDQA64 ZMM2/M512{K1}{Z},ZMM1	Move aligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVDQU	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
MOVDQU	XMM,XMM/M128	SSE2	MOVDQU XMM1,XMM2/M128	Move unaligned packed integer values from xmm2/m128 to xmm1.
MOVDQU	XMM/M128,XMM	SSE2	MOVDQU XMM2/M128,XMM1	Move unaligned packed integer values from xmm1 to xmm2/m128.
GENERAL	VMOVDQU	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
VMOVDQU	XMM,XMM/M128	AVX	VMOVDQU XMM1,XMM2/M128	Move unaligned packed integer values from xmm2/m128 to xmm1.
VMOVDQU	XMM/M128,XMM	AVX	VMOVDQU XMM2/M128,XMM1	Move unaligned packed integer values from xmm1 to xmm2/m128.
VMOVDQU	YMM,YMM/M256	AVX	VMOVDQU YMM1,YMM2/M256	Move unaligned packed integer values from ymm2/m256 to ymm1.
VMOVDQU	YMM/M256,YMM	AVX	VMOVDQU YMM2/M256,YMM1	Move unaligned packed integer values from ymm1 to ymm2/m256.
GENERAL	VMOVDQU8	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
VMOVDQU8	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VMOVDQU8 XMM1{K1}{Z},XMM2/M128	Move unaligned packed byte integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU8	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_BW	VMOVDQU8 YMM1{K1}{Z},YMM2/M256	Move unaligned packed byte integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU8	ZMM{K}{Z},ZMM/M512	AVX512_BW	VMOVDQU8 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed byte integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU8	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_BW	VMOVDQU8 XMM2/M128{K1}{Z},XMM1	Move unaligned packed byte integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU8	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_BW	VMOVDQU8 YMM2/M256{K1}{Z},YMM1	Move unaligned packed byte integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU8	ZMM/M512{K}{Z},ZMM	AVX512_BW	VMOVDQU8 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed byte integer values from zmm1 to zmm2/m512 using writemask k1.
GENERAL	VMOVDQU16	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
VMOVDQU16	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VMOVDQU16 XMM1{K1}{Z},XMM2/M128	Move unaligned packed word integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU16	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_BW	VMOVDQU16 YMM1{K1}{Z},YMM2/M256	Move unaligned packed word integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU16	ZMM{K}{Z},ZMM/M512	AVX512_BW	VMOVDQU16 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed word integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU16	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_BW	VMOVDQU16 XMM2/M128{K1}{Z},XMM1	Move unaligned packed word integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU16	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_BW	VMOVDQU16 YMM2/M256{K1}{Z},YMM1	Move unaligned packed word integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU16	ZMM/M512{K}{Z},ZMM	AVX512_BW	VMOVDQU16 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed word integer values from zmm1 to zmm2/m512 using writemask k1.
GENERAL	VMOVDQU32	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
VMOVDQU32	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVDQU32 XMM1{K1}{Z},XMM2/M128	Move unaligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU32	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVDQU32 YMM1{K1}{Z},YMM2/M256	Move unaligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU32	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVDQU32 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU32	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVDQU32 XMM2/M128{K1}{Z},XMM1	Move unaligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU32	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVDQU32 YMM2/M256{K1}{Z},YMM1	Move unaligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU32	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVDQU32 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1.
GENERAL	VMOVDQU64	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64
VMOVDQU64	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVDQU64 XMM1{K1}{Z},XMM2/M128	Move unaligned packed quadword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU64	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVDQU64 YMM1{K1}{Z},YMM2/M256	Move unaligned packed quadword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU64	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVDQU64 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU64	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVDQU64 XMM2/M128{K1}{Z},XMM1	Move unaligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU64	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVDQU64 YMM2/M256{K1}{Z},YMM1	Move unaligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU64	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVDQU64 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVHLPS	Move Packed Single-Precision Floating-Point Values High to Low	MOVHLPS
MOVHLPS	XMM,XMM	SSE	MOVHLPS XMM1,XMM2	Move two packed SP FP values from high quadword of xmm2 to low quadword of xmm1.
GENERAL	VMOVHLPS	Move Packed Single-Precision Floating-Point Values High to Low	MOVHLPS
VMOVHLPS	XMM,XMM,XMM	AVX	VMOVHLPS XMM1,XMM2,XMM3	Merge two packed SP FP values from high quadword of xmm3 and low quadword of xmm2.
VMOVHLPS	XMM,XMM,XMM	AVX512_F	VMOVHLPS XMM1,XMM2,XMM3	Merge two packed SP FP values from high quadword of xmm3 and low quadword of xmm2.
;--------------------------------------------------------
GENERAL	MOVHPD	Move High Packed Double-Precision Floating-Point Value	MOVHPD
MOVHPD	XMM,M64	SSE2	MOVHPD XMM1,M64	Move DP FP value from m64 to high quadword of xmm1.
MOVHPD	M64,XMM	SSE2	MOVHPD M64,XMM1	Move DP FP value from high quadword of xmm1 to m64.
GENERAL	VMOVHPD	Move High Packed Double-Precision Floating-Point Value	MOVHPD
VMOVHPD	XMM,XMM,M64	AVX	VMOVHPD XMM2,XMM1,M64	Merge DP FP value from m64 and the low quadword of xmm1.
VMOVHPD	XMM,XMM,M64	AVX512_F	VMOVHPD XMM2,XMM1,M64	Merge DP FP value from m64 and the low quadword of xmm1.
VMOVHPD	M64,XMM	AVX	VMOVHPD M64,XMM1	Move DP FP value from high quadword of xmm1 to m64.
VMOVHPD	M64,XMM	AVX512_F	VMOVHPD M64,XMM1	Move DP FP value from high quadword of xmm1 to m64.
;--------------------------------------------------------
GENERAL	MOVHPS	Move High Packed Single-Precision Floating-Point Values	MOVHPS
MOVHPS	XMM,M64	SSE	MOVHPS XMM1,M64	Move two packed SP FP values from m64 to high quadword of xmm1.
MOVHPS	M64,XMM	SSE	MOVHPS M64,XMM1	Move two packed SP FP values from high quadword of xmm1 to m64.
GENERAL	VMOVHPS	Move High Packed Single-Precision Floating-Point Values	MOVHPS
VMOVHPS	XMM,XMM,M64	AVX	VMOVHPS XMM2,XMM1,M64	Merge two packed SP FP values from m64 and the low quadword of xmm1.
VMOVHPS	XMM,XMM,M64	AVX512_F	VMOVHPS XMM2,XMM1,M64	Merge two packed SP FP values from m64 and the low quadword of xmm1.
VMOVHPS	M64,XMM	AVX	VMOVHPS M64,XMM1	Move two packed SP FP values from high quadword of xmm1 to m64.
VMOVHPS	M64,XMM	AVX512_F	VMOVHPS M64,XMM1	Move two packed SP FP values from high quadword of xmm1 to m64.
;--------------------------------------------------------
GENERAL	MOVLHPS	Move Packed Single-Precision Floating-Point Values Low to High	MOVLHPS
MOVLHPS	XMM,XMM	SSE	MOVLHPS XMM1,XMM2	Move two packed SP FP values from low quadword of xmm2 to high quadword of xmm1.
GENERAL	VMOVLHPS	Move Packed Single-Precision Floating-Point Values Low to High	MOVLHPS
VMOVLHPS	XMM,XMM,XMM	AVX	VMOVLHPS XMM1,XMM2,XMM3	Merge two packed SP FP values from low quadword of xmm3 and low quadword of xmm2.
VMOVLHPS	XMM,XMM,XMM	AVX512_F	VMOVLHPS XMM1,XMM2,XMM3	Merge two packed SP FP values from low quadword of xmm3 and low quadword of xmm2.
;--------------------------------------------------------
GENERAL	MOVLPD	Move Low Packed Double-Precision Floating-Point Value	MOVLPD
MOVLPD	XMM,M64	SSE2	MOVLPD XMM1,M64	Move DP FP value from m64 to low quadword of xmm1.
MOVLPD	M64,XMM	SSE2	MOVLPD M64,XMM1	Move DP FP value from low quadword of xmm1 to m64.
GENERAL	VMOVLPD	Move Low Packed Double-Precision Floating-Point Value	MOVLPD
VMOVLPD	XMM,XMM,M64	AVX	VMOVLPD XMM2,XMM1,M64	Merge DP FP value from m64 and the high quadword of xmm1.
VMOVLPD	XMM,XMM,M64	AVX512_F	VMOVLPD XMM2,XMM1,M64	Merge DP FP value from m64 and the high quadword of xmm1.
VMOVLPD	M64,XMM	AVX	VMOVLPD M64,XMM1	Move DP FP value from low quadword of xmm1 to m64.
VMOVLPD	M64,XMM	AVX512_F	VMOVLPD M64,XMM1	Move DP FP value from low quadword of xmm1 to m64.
;--------------------------------------------------------
GENERAL	MOVLPS	Move Low Packed Single-Precision Floating-Point Values	MOVLPS
MOVLPS	XMM,M64	SSE	MOVLPS XMM1,M64	Move two packed SP FP values from m64 to low quadword of xmm1.
MOVLPS	M64,XMM	SSE	MOVLPS M64,XMM1	Move two packed SP FP values from low quadword of xmm1 to m64.
GENERAL	VMOVLPS	Move Low Packed Single-Precision Floating-Point Values	MOVLPS
VMOVLPS	XMM,XMM,M64	AVX	VMOVLPS XMM2,XMM1,M64	Merge two packed SP FP values from m64 and the high quadword of xmm1.
VMOVLPS	XMM,XMM,M64	AVX512_F	VMOVLPS XMM2,XMM1,M64	Merge two packed SP FP values from m64 and the high quadword of xmm1.
VMOVLPS	M64,XMM	AVX	VMOVLPS M64,XMM1	Move two packed SP FP values from low quadword of xmm1 to m64.
VMOVLPS	M64,XMM	AVX512_F	VMOVLPS M64,XMM1	Move two packed SP FP values from low quadword of xmm1 to m64.
;--------------------------------------------------------
GENERAL	MOVMSKPD	Extract Packed Double-Precision Floating-Point Sign Mask	MOVMSKPD
MOVMSKPD	REG,XMM	SSE2	MOVMSKPD REG,XMM	Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
GENERAL	VMOVMSKPD	Extract Packed Double-Precision Floating-Point Sign Mask	MOVMSKPD
VMOVMSKPD	REG,XMM	AVX	VMOVMSKPD REG,XMM2	Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVMSKPD	REG,YMM	AVX	VMOVMSKPD REG,YMM2	Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
;--------------------------------------------------------
GENERAL	MOVMSKPS	Extract Packed Single-Precision Floating-Point Sign Mask	MOVMSKPS
MOVMSKPS	REG,XMM	SSE	MOVMSKPS REG,XMM	Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
GENERAL	VMOVMSKPS	Extract Packed Single-Precision Floating-Point Sign Mask	MOVMSKPS
VMOVMSKPS	REG,XMM	AVX	VMOVMSKPS REG,XMM2	Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVMSKPS	REG,YMM	AVX	VMOVMSKPS REG,YMM2	Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
;--------------------------------------------------------
GENERAL	MOVNTDQ	Store Packed Integers Using Non-Temporal Hint	MOVNTDQ
MOVNTDQ	M128,XMM	SSE2	MOVNTDQ M128,XMM1	Move packed integer values in xmm1 to m128 using non- temporal hint.
GENERAL	VMOVNTDQ	Store Packed Integers Using Non-Temporal Hint	MOVNTDQ
VMOVNTDQ	M128,XMM	AVX	VMOVNTDQ M128,XMM1	Move packed integer values in xmm1 to m128 using non- temporal hint.
VMOVNTDQ	M256,YMM	AVX	VMOVNTDQ M256,YMM1	Move packed integer values in ymm1 to m256 using non- temporal hint.
VMOVNTDQ	M128,XMM	AVX512_VL,AVX512_F	VMOVNTDQ M128,XMM1	Move packed integer values in xmm1 to m128 using non- temporal hint.
VMOVNTDQ	M256,YMM	AVX512_VL,AVX512_F	VMOVNTDQ M256,YMM1	Move packed integer values in zmm1 to m256 using non- temporal hint.
VMOVNTDQ	M512,ZMM	AVX512_F	VMOVNTDQ M512,ZMM1	Move packed integer values in zmm1 to m512 using non- temporal hint.
;--------------------------------------------------------
GENERAL	MOVNTDQA	Load Double Quadword Non-Temporal Aligned Hint	MOVNTDQA
MOVNTDQA	XMM,M128	SSE4_1	MOVNTDQA XMM1,M128	Move double quadword from m128 to xmm1 using non- temporal hint if WC memory type.
GENERAL	VMOVNTDQA	Load Double Quadword Non-Temporal Aligned Hint	MOVNTDQA
VMOVNTDQA	XMM,M128	AVX	VMOVNTDQA XMM1,M128	Move double quadword from m128 to xmm using non- temporal hint if WC memory type.
VMOVNTDQA	YMM,M256	AVX2	VMOVNTDQA YMM1,M256	Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type.
VMOVNTDQA	XMM,M128	AVX512_VL,AVX512_F	VMOVNTDQA XMM1,M128	Move 128-bit data from m128 to xmm using non-temporal hint if WC memory type.
VMOVNTDQA	YMM,M256	AVX512_VL,AVX512_F	VMOVNTDQA YMM1,M256	Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type.
VMOVNTDQA	ZMM,M512	AVX512_F	VMOVNTDQA ZMM1,M512	Move 512-bit data from m512 to zmm using non-temporal hint if WC memory type.
;--------------------------------------------------------
GENERAL	MOVNTI	Store Doubleword Using Non-Temporal Hint	MOVNTI
MOVNTI	M32,R32	386	MOVNTI M32,R32	Move doubleword from r32 to m32 using non- temporal hint.
MOVNTI	M64,R64	X64	MOVNTI M64,R64	Move quadword from r64 to m64 using non- temporal hint.
;--------------------------------------------------------
GENERAL	MOVNTPD	Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPD
MOVNTPD	M128,XMM	SSE2	MOVNTPD M128,XMM1	Move packed DP values in xmm1 to m128 using non-temporal hint.
GENERAL	VMOVNTPD	Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPD
VMOVNTPD	M128,XMM	AVX	VMOVNTPD M128,XMM1	Move packed DP values in xmm1 to m128 using non-temporal hint.
VMOVNTPD	M256,YMM	AVX	VMOVNTPD M256,YMM1	Move packed DP values in ymm1 to m256 using non-temporal hint.
VMOVNTPD	M128,XMM	AVX512_VL,AVX512_F	VMOVNTPD M128,XMM1	Move packed DP values in xmm1 to m128 using non-temporal hint.
VMOVNTPD	M256,YMM	AVX512_VL,AVX512_F	VMOVNTPD M256,YMM1	Move packed DP values in ymm1 to m256 using non-temporal hint.
VMOVNTPD	M512,ZMM	AVX512_F	VMOVNTPD M512,ZMM1	Move packed DP values in zmm1 to m512 using non-temporal hint.
;--------------------------------------------------------
GENERAL	MOVNTPS	Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPS
MOVNTPS	M128,XMM	SSE	MOVNTPS M128,XMM1	Move packed SP values xmm1 to mem using non-temporal hint.
GENERAL	VMOVNTPS	Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPS
VMOVNTPS	M128,XMM	AVX	VMOVNTPS M128,XMM1	Move packed SP values xmm1 to mem using non-temporal hint.
VMOVNTPS	M256,YMM	AVX	VMOVNTPS M256,YMM1	Move packed SP values ymm1 to mem using non-temporal hint.
VMOVNTPS	M128,XMM	AVX512_VL,AVX512_F	VMOVNTPS M128,XMM1	Move packed SP values in xmm1 to m128 using non-temporal hint.
VMOVNTPS	M256,YMM	AVX512_VL,AVX512_F	VMOVNTPS M256,YMM1	Move packed SP values in ymm1 to m256 using non-temporal hint.
VMOVNTPS	M512,ZMM	AVX512_F	VMOVNTPS M512,ZMM1	Move packed SP values in zmm1 to m512 using non-temporal hint.
;--------------------------------------------------------
GENERAL	MOVNTQ	Store of Quadword Using Non-Temporal Hint	MOVNTQ
MOVNTQ	M64,MM	X64	MOVNTQ M64,MM	Move quadword from mm to m64 using non- temporal hint.
;--------------------------------------------------------
GENERAL	MOVQ	Move Quadword	MOVQ
MOVQ	MM,MM/M64	MMX	MOVQ MM,MM/M64	Move quadword from mm/m64 to mm.
MOVQ	MM/M64,MM	MMX	MOVQ MM/M64,MM	Move quadword from mm to mm/m64.
MOVQ	XMM,XMM/M64	SSE2	MOVQ XMM1,XMM2/M64	Move quadword from xmm2/mem64 to xmm1.
MOVQ	XMM/M64,XMM	SSE2	MOVQ XMM2/M64,XMM1	Move quadword from xmm1 to xmm2/mem64.
GENERAL	VMOVQ	Move Quadword	MOVQ
VMOVQ	XMM,XMM/M64	AVX	VMOVQ XMM1,XMM2/M64	Move quadword from xmm2 to xmm1.
VMOVQ	XMM,XMM/M64	AVX512_F	VMOVQ XMM1,XMM2/M64	Move quadword from xmm2/m64 to xmm1.
VMOVQ	XMM/M64,XMM	AVX	VMOVQ XMM1/M64,XMM2	Move quadword from xmm2 register to xmm1/m64.
VMOVQ	XMM/M64,XMM	AVX512_F	VMOVQ XMM1/M64,XMM2	Move quadword from xmm2 register to xmm1/m64.
;--------------------------------------------------------
GENERAL	MOVQ2DQ	Move Quadword from MMX Technology to XMM Register	MOVQ2DQ
MOVQ2DQ	XMM,MM	8086	MOVQ2DQ XMM,MM	Move quadword from mmx to low quadword of xmm.
;--------------------------------------------------------
GENERAL	MOVS	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ
MOVS	M8,M8	8086	MOVS M8,M8	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
MOVS	M16,M16	8086	MOVS M16,M16	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
MOVS	M32,M32	386	MOVS M32,M32	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
MOVS	M64,M64	X64	MOVS M64,M64	Move qword from address (R|E)SI to (R|E)DI.
GENERAL	MOVSB	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ
MOVSB		8086	MOVSB	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
GENERAL	MOVSW	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ
MOVSW		8086	MOVSW	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
GENERAL	MOVSD	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ
MOVSD		8086	MOVSD	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
GENERAL	MOVSQ	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ
MOVSQ		8086	MOVSQ	Move qword from address (R|E)SI to (R|E)DI.
;--------------------------------------------------------
GENERAL	MOVSD	Move or Merge Scalar Double-Precision Floating-Point Value	MOVSD
MOVSD	XMM,XMM	SSE2	MOVSD XMM1,XMM2	Move scalar DP FP value from xmm2 to xmm1 register.
MOVSD	XMM,M64	SSE2	MOVSD XMM1,M64	Load scalar DP FP value from m64 to xmm1 register.
MOVSD	XMM/M64,XMM	SSE2	MOVSD XMM1/M64,XMM2	Move scalar DP FP value from xmm2 register to xmm1/m64.
GENERAL	VMOVSD	Move or Merge Scalar Double-Precision Floating-Point Value	MOVSD
VMOVSD	XMM,XMM,XMM	AVX	VMOVSD XMM1,XMM2,XMM3	Merge scalar DP FP value from xmm2 and xmm3 to xmm1 register.
VMOVSD	XMM,M64	AVX	VMOVSD XMM1,M64	Load scalar DP FP value from m64 to xmm1 register.
VMOVSD	XMM,XMM,XMM	AVX	VMOVSD XMM1,XMM2,XMM3	Merge scalar DP FP value from xmm2 and xmm3 registers to xmm1.
VMOVSD	M64,XMM	AVX	VMOVSD M64,XMM1	Store scalar DP FP value from xmm1 register to m64.
VMOVSD	XMM{K}{Z},XMM,XMM	AVX512_F	VMOVSD XMM1{K1}{Z},XMM2,XMM3	Merge scalar DP FP value from xmm2 and xmm3 registers to xmm1 under writemask k1.
VMOVSD	XMM{K}{Z},M64	AVX512_F	VMOVSD XMM1{K1}{Z},M64	Load scalar DP FP value from m64 to xmm1 register under writemask k1.
VMOVSD	XMM{K}{Z},XMM,XMM	AVX512_F	VMOVSD XMM1{K1}{Z},XMM2,XMM3	Merge scalar DP FP value from xmm2 and xmm3 registers to xmm1 under writemask k1.
VMOVSD	M64{K},XMM	AVX512_F	VMOVSD M64{K1},XMM1	Store scalar DP FP value from xmm1 register to m64 under writemask k1.
;--------------------------------------------------------
GENERAL	MOVSHDUP	Replicate Single FP Values	MOVSHDUP
MOVSHDUP	XMM,XMM/M128	SSE3	MOVSHDUP XMM1,XMM2/M128	Move odd index SP FP values from xmm2/mem and duplicate each element into xmm1.
GENERAL	VMOVSHDUP	Replicate Single FP Values	MOVSHDUP
VMOVSHDUP	XMM,XMM/M128	AVX	VMOVSHDUP XMM1,XMM2/M128	Move odd index SP FP values from xmm2/mem and duplicate each element into xmm1.
VMOVSHDUP	YMM,YMM/M256	AVX	VMOVSHDUP YMM1,YMM2/M256	Move odd index SP FP values from ymm2/mem and duplicate each element into ymm1.
VMOVSHDUP	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVSHDUP XMM1{K1}{Z},XMM2/M128	Move odd index SP FP values from xmm2/m128 and duplicate each element into xmm1 under writemask.
VMOVSHDUP	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVSHDUP YMM1{K1}{Z},YMM2/M256	Move odd index SP FP values from ymm2/m256 and duplicate each element into ymm1 under writemask.
VMOVSHDUP	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVSHDUP ZMM1{K1}{Z},ZMM2/M512	Move odd index SP FP values from zmm2/m512 and duplicate each element into zmm1 under writemask.
;--------------------------------------------------------
GENERAL	MOVSLDUP	Replicate Single FP Values	MOVSLDUP
MOVSLDUP	XMM,XMM/M128	SSE3	MOVSLDUP XMM1,XMM2/M128	Move even index SP FP values from xmm2/mem and duplicate each element into xmm1.
GENERAL	VMOVSLDUP	Replicate Single FP Values	MOVSLDUP
VMOVSLDUP	XMM,XMM/M128	AVX	VMOVSLDUP XMM1,XMM2/M128	Move even index SP FP values from xmm2/mem and duplicate each element into xmm1.
VMOVSLDUP	YMM,YMM/M256	AVX	VMOVSLDUP YMM1,YMM2/M256	Move even index SP FP values from ymm2/mem and duplicate each element into ymm1.
VMOVSLDUP	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVSLDUP XMM1{K1}{Z},XMM2/M128	Move even index SP FP values from xmm2/m128 and duplicate each element into xmm1 under writemask.
VMOVSLDUP	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVSLDUP YMM1{K1}{Z},YMM2/M256	Move even index SP FP values from ymm2/m256 and duplicate each element into ymm1 under writemask.
VMOVSLDUP	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVSLDUP ZMM1{K1}{Z},ZMM2/M512	Move even index SP FP values from zmm2/m512 and duplicate each element into zmm1 under writemask.
;--------------------------------------------------------
GENERAL	MOVSS	Move or Merge Scalar Single-Precision Floating-Point Value	MOVSS
MOVSS	XMM,XMM	SSE	MOVSS XMM1,XMM2	Merge scalar SP FP value from xmm2 to xmm1 register.
MOVSS	XMM,M32	SSE	MOVSS XMM1,M32	Load scalar SP FP value from m32 to xmm1 register.
MOVSS	XMM/M32,XMM	SSE	MOVSS XMM2/M32,XMM1	Move scalar SP FP value from xmm1 register to xmm2/m32.
GENERAL	VMOVSS	Move or Merge Scalar Single-Precision Floating-Point Value	MOVSS
VMOVSS	XMM,XMM,XMM	AVX	VMOVSS XMM1,XMM2,XMM3	Merge scalar SP FP value from xmm2 and xmm3 to xmm1 register
VMOVSS	XMM,M32	AVX	VMOVSS XMM1,M32	Load scalar SP FP value from m32 to xmm1 register.
VMOVSS	XMM,XMM,XMM	AVX	VMOVSS XMM1,XMM2,XMM3	Move scalar SP FP value from xmm2 and xmm3 to xmm1 register.
VMOVSS	M32,XMM	AVX	VMOVSS M32,XMM1	Move scalar SP FP value from xmm1 register to m32.
VMOVSS	XMM{K}{Z},XMM,XMM	AVX512_F	VMOVSS XMM1{K1}{Z},XMM2,XMM3	Move scalar SP FP value from xmm2 and xmm3 to xmm1 register under writemask k1.
VMOVSS	XMM{K}{Z},M32	AVX512_F	VMOVSS XMM1{K1}{Z},M32	Move scalar SP FP values from m32 to xmm1 under writemask k1.
VMOVSS	XMM{K}{Z},XMM,XMM	AVX512_F	VMOVSS XMM1{K1}{Z},XMM2,XMM3	Move scalar SP FP value from xmm2 and xmm3 to xmm1 register under writemask k1.
VMOVSS	M32{K},XMM	AVX512_F	VMOVSS M32{K1},XMM1	Move scalar SP FP values from xmm1 to m32 under writemask k1.
;--------------------------------------------------------
GENERAL	MOVSX	Move with Sign-Extension	MOVSX_MOVSXD
MOVSX	R16,R/M8	8086	MOVSX R16,R/M8	Move byte to word with sign-extension.
MOVSX	R32,R/M8	386	MOVSX R32,R/M8	Move byte to doubleword with sign- extension.
MOVSX	R64,R/M8	X64	MOVSX R64,R/M8	Move byte to quadword with sign-extension.
MOVSX	R32,R/M16	386	MOVSX R32,R/M16	Move word to doubleword, with sign- extension.
MOVSX	R64,R/M16	X64	MOVSX R64,R/M16	Move word to quadword with sign-extension.
GENERAL	MOVSXD	Move with Sign-Extension	MOVSX_MOVSXD
MOVSXD	R16,R/M16	8086	MOVSXD R16,R/M16	Move word to word with sign-extension.
MOVSXD	R32,R/M32	386	MOVSXD R32,R/M32	Move doubleword to doubleword with sign- extension.
MOVSXD	R64,R/M32	X64	MOVSXD R64,R/M32	Move doubleword to quadword with sign- extension.
;--------------------------------------------------------
GENERAL	MOVUPD	Move Unaligned Packed Double-Precision Floating-Point Values	MOVUPD
MOVUPD	XMM,XMM/M128	SSE2	MOVUPD XMM1,XMM2/M128	Move unaligned packed DP FP from xmm2/mem to xmm1.
MOVUPD	XMM/M128,XMM	SSE2	MOVUPD XMM2/M128,XMM1	Move unaligned packed DP FP from xmm1 to xmm2/mem.
GENERAL	VMOVUPD	Move Unaligned Packed Double-Precision Floating-Point Values	MOVUPD
VMOVUPD	XMM,XMM/M128	AVX	VMOVUPD XMM1,XMM2/M128	Move unaligned packed DP FP from xmm2/mem to xmm1.
VMOVUPD	XMM/M128,XMM	AVX	VMOVUPD XMM2/M128,XMM1	Move unaligned packed DP FP from xmm1 to xmm2/mem.
VMOVUPD	YMM,YMM/M256	AVX	VMOVUPD YMM1,YMM2/M256	Move unaligned packed DP FP from ymm2/mem to ymm1.
VMOVUPD	YMM/M256,YMM	AVX	VMOVUPD YMM2/M256,YMM1	Move unaligned packed DP FP from ymm1 to ymm2/mem.
VMOVUPD	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVUPD XMM1{K1}{Z},XMM2/M128	Move unaligned packed DP FP from xmm2/m128 to xmm1 using writemask k1.
VMOVUPD	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVUPD XMM2/M128{K1}{Z},XMM1	Move unaligned packed DP FP from xmm1 to xmm2/m128 using writemask k1.
VMOVUPD	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVUPD YMM1{K1}{Z},YMM2/M256	Move unaligned packed DP FP from ymm2/m256 to ymm1 using writemask k1.
VMOVUPD	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVUPD YMM2/M256{K1}{Z},YMM1	Move unaligned packed DP FP from ymm1 to ymm2/m256 using writemask k1.
VMOVUPD	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVUPD ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed DP FP values from zmm2/m512 to zmm1 using writemask k1.
VMOVUPD	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVUPD ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed DP FP values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVUPS	Move Unaligned Packed Single-Precision Floating-Point Values	MOVUPS
MOVUPS	XMM,XMM/M128	SSE	MOVUPS XMM1,XMM2/M128	Move unaligned packed SP FP from xmm2/mem to xmm1.
MOVUPS	XMM/M128,XMM	SSE	MOVUPS XMM2/M128,XMM1	Move unaligned packed SP FP from xmm1 to xmm2/mem.
GENERAL	VMOVUPS	Move Unaligned Packed Single-Precision Floating-Point Values	MOVUPS
VMOVUPS	XMM,XMM/M128	AVX	VMOVUPS XMM1,XMM2/M128	Move unaligned packed SP FP from xmm2/mem to xmm1.
VMOVUPS	XMM/M128,XMM	AVX	VMOVUPS XMM2/M128,XMM1	Move unaligned packed SP FP from xmm1 to xmm2/mem.
VMOVUPS	YMM,YMM/M256	AVX	VMOVUPS YMM1,YMM2/M256	Move unaligned packed SP FP from ymm2/mem to ymm1.
VMOVUPS	YMM/M256,YMM	AVX	VMOVUPS YMM2/M256,YMM1	Move unaligned packed SP FP from ymm1 to ymm2/mem.
VMOVUPS	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VMOVUPS XMM1{K1}{Z},XMM2/M128	Move unaligned packed SP FP values from xmm2/m128 to xmm1 using writemask k1.
VMOVUPS	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VMOVUPS YMM1{K1}{Z},YMM2/M256	Move unaligned packed SP FP values from ymm2/m256 to ymm1 using writemask k1.
VMOVUPS	ZMM{K}{Z},ZMM/M512	AVX512_F	VMOVUPS ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed SP FP values from zmm2/m512 to zmm1 using writemask k1.
VMOVUPS	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VMOVUPS XMM2/M128{K1}{Z},XMM1	Move unaligned packed SP FP values from xmm1 to xmm2/m128 using writemask k1.
VMOVUPS	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VMOVUPS YMM2/M256{K1}{Z},YMM1	Move unaligned packed SP FP values from ymm1 to ymm2/m256 using writemask k1.
VMOVUPS	ZMM/M512{K}{Z},ZMM	AVX512_F	VMOVUPS ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed SP FP values from zmm1 to zmm2/m512 using writemask k1.
;--------------------------------------------------------
GENERAL	MOVZX	Move with Zero-Extend	MOVZX
MOVZX	R16,R/M8	8086	MOVZX R16,R/M8	Move byte to word with zero-extension.
MOVZX	R32,R/M8	386	MOVZX R32,R/M8	Move byte to doubleword, zero-extension.
MOVZX	R64,R/M8	X64	MOVZX R64,R/M8	Move byte to quadword, zero-extension.
MOVZX	R32,R/M16	386	MOVZX R32,R/M16	Move word to doubleword, zero-extension.
MOVZX	R64,R/M16	X64	MOVZX R64,R/M16	Move word to quadword, zero-extension.
;--------------------------------------------------------
GENERAL	MPSADBW	Compute Multiple Packed Sums of Absolute Difference	MPSADBW
MPSADBW	XMM,XMM/M128,IMM8	SSE4_1	MPSADBW XMM1,XMM2/M128,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8.
GENERAL	VMPSADBW	Compute Multiple Packed Sums of Absolute Difference	MPSADBW
VMPSADBW	XMM,XMM,XMM/M128,IMM8	AVX	VMPSADBW XMM1,XMM2,XMM3/M128,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8.
VMPSADBW	YMM,YMM,YMM/M256,IMM8	AVX2	VMPSADBW YMM1,YMM2,YMM3/M256,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by imm8.
;--------------------------------------------------------
GENERAL	MUL	Unsigned Multiply	MUL
MUL	R/M8	8086	MUL R/M8	Unsigned multiply (AX ← AL ∗ r/m8).
MUL	R/M8	8086	MUL R/M8	Unsigned multiply (AX ← AL ∗ r/m8).
MUL	R/M16	8086	MUL R/M16	Unsigned multiply (DX:AX ← AX ∗ r/m16).
MUL	R/M32	386	MUL R/M32	Unsigned multiply (EDX:EAX ← EAX ∗ r/m32).
MUL	R/M64	X64	MUL R/M64	Unsigned multiply (RDX:RAX ← RAX ∗ r/m64).
;--------------------------------------------------------
GENERAL	MULPD	Multiply Packed Double-Precision Floating-Point Values	MULPD
MULPD	XMM,XMM/M128	SSE2	MULPD XMM1,XMM2/M128	Multiply packed DP FP values in xmm2/m128 with xmm1 and store result in xmm1.
GENERAL	VMULPD	Multiply Packed Double-Precision Floating-Point Values	MULPD
VMULPD	XMM,XMM,XMM/M128	AVX	VMULPD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values in xmm3/m128 with xmm2 and store result in xmm1.
VMULPD	YMM,YMM,YMM/M256	AVX	VMULPD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values in ymm3/m256 with ymm2 and store result in ymm1.
VMULPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VMULPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm3/m128/m64bcst to xmm2 and store result in xmm1.
VMULPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VMULPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm3/m256/m64bcst to ymm2 and store result in ymm1.
VMULPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VMULPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values in zmm3/m512/m64bcst with zmm2 and store result in zmm1.
;--------------------------------------------------------
GENERAL	MULPS	Multiply Packed Single-Precision Floating-Point Values	MULPS
MULPS	XMM,XMM/M128	SSE	MULPS XMM1,XMM2/M128	Multiply packed SP FP values in xmm2/m128 with xmm1 and store result in xmm1.
GENERAL	VMULPS	Multiply Packed Single-Precision Floating-Point Values	MULPS
VMULPS	XMM,XMM,XMM/M128	AVX	VMULPS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values in xmm3/m128 with xmm2 and store result in xmm1.
VMULPS	YMM,YMM,YMM/M256	AVX	VMULPS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values in ymm3/m256 with ymm2 and store result in ymm1.
VMULPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VMULPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm3/m128/m32bcst to xmm2 and store result in xmm1.
VMULPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VMULPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm3/m256/m32bcst to ymm2 and store result in ymm1.
VMULPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VMULPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values in zmm3/m512/m32bcst with zmm2 and store result in zmm1.
;--------------------------------------------------------
GENERAL	MULSD	Multiply Scalar Double-Precision Floating-Point Value	MULSD
MULSD	XMM,XMM/M64	SSE2	MULSD XMM1,XMM2/M64	Multiply the low DP FP value in xmm2/m64 by low DP FP value in xmm1.
GENERAL	VMULSD	Multiply Scalar Double-Precision Floating-Point Value	MULSD
VMULSD	XMM,XMM,XMM/M64	AVX	VMULSD XMM1,XMM2,XMM3/M64	Multiply the low DP FP value in xmm3/m64 by low DP FP value in xmm2.
VMULSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VMULSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply the low DP FP value in xmm3/m64 by low DP FP value in xmm2.
;--------------------------------------------------------
GENERAL	MULSS	Multiply Scalar Single-Precision Floating-Point Values	MULSS
MULSS	XMM,XMM/M32	SSE	MULSS XMM1,XMM2/M32	Multiply the low SP FP value in xmm2/m32 by the low SP FP value in xmm1.
GENERAL	VMULSS	Multiply Scalar Single-Precision Floating-Point Values	MULSS
VMULSS	XMM,XMM,XMM/M32	AVX	VMULSS XMM1,XMM2,XMM3/M32	Multiply the low SP FP value in xmm3/m32 by the low SP FP value in xmm2.
VMULSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VMULSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply the low SP FP value in xmm3/m32 by the low SP FP value in xmm2.
;--------------------------------------------------------
GENERAL	MULX	Unsigned Multiply Without Affecting Flags	MULX
MULX	R32,R32,R/M32	BMI2	MULX R32A,R32B,R/M32	Unsigned multiply of r/m32 with EDX without affecting arithmetic flags.
MULX	R64,R64,R/M64	BMI2	MULX R64A,R64B,R/M64	Unsigned multiply of r/m64 with RDX without affecting arithmetic flags.
;--------------------------------------------------------
GENERAL	MWAIT	Monitor Wait	MWAIT
MWAIT		8086	MWAIT	A hint that allows the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.
;--------------------------------------------------------
GENERAL	NEG	Two's Complement Negation	NEG
NEG	R/M8	8086	NEG R/M8	Two's complement negate r/m8.
NEG	R/M8	8086	NEG R/M8	Two's complement negate r/m8.
NEG	R/M16	8086	NEG R/M16	Two's complement negate r/m16.
NEG	R/M32	386	NEG R/M32	Two's complement negate r/m32.
NEG	R/M64	X64	NEG R/M64	Two's complement negate r/m64.
;--------------------------------------------------------
GENERAL	NOP	No Operation	NOP
NOP		8086	NOP	One byte no-operation instruction.
NOP	R/M16	8086	NOP R/M16	Multi-byte no-operation instruction.
NOP	R/M32	386	NOP R/M32	Multi-byte no-operation instruction.
;--------------------------------------------------------
GENERAL	NOT	One's Complement Negation	NOT
NOT	R/M8	8086	NOT R/M8	Reverse each bit of r/m8.
NOT	R/M8	8086	NOT R/M8	Reverse each bit of r/m8.
NOT	R/M16	8086	NOT R/M16	Reverse each bit of r/m16.
NOT	R/M32	386	NOT R/M32	Reverse each bit of r/m32.
NOT	R/M64	X64	NOT R/M64	Reverse each bit of r/m64.
;--------------------------------------------------------
GENERAL	OR	Logical Inclusive OR	OR
OR	AL,IMM8	8086	OR AL,IMM8	AL OR imm8.
OR	AX,IMM16	8086	OR AX,IMM16	AX OR imm16.
OR	EAX,IMM32	386	OR EAX,IMM32	EAX OR imm32.
OR	RAX,IMM32	386	OR RAX,IMM32	RAX OR imm32 (sign-extended).
OR	R/M8,IMM8	8086	OR R/M8,IMM8	r/m8 OR imm8.
OR	R/M8,IMM8	8086	OR R/M8,IMM8	r/m8 OR imm8.
OR	R/M16,IMM16	8086	OR R/M16,IMM16	r/m16 OR imm16.
OR	R/M32,IMM32	386	OR R/M32,IMM32	r/m32 OR imm32.
OR	R/M64,IMM32	X64	OR R/M64,IMM32	r/m64 OR imm32 (sign-extended).
OR	R/M16,IMM8	8086	OR R/M16,IMM8	r/m16 OR imm8 (sign-extended).
OR	R/M32,IMM8	386	OR R/M32,IMM8	r/m32 OR imm8 (sign-extended).
OR	R/M64,IMM8	X64	OR R/M64,IMM8	r/m64 OR imm8 (sign-extended).
OR	R/M8,R8	8086	OR R/M8,R8	r/m8 OR r8.
OR	R/M8,R8	8086	OR R/M8,R8	r/m8 OR r8.
OR	R/M16,R16	8086	OR R/M16,R16	r/m16 OR r16.
OR	R/M32,R32	386	OR R/M32,R32	r/m32 OR r32.
OR	R/M64,R64	X64	OR R/M64,R64	r/m64 OR r64.
OR	R8,R/M8	8086	OR R8,R/M8	r8 OR r/m8.
OR	R8,R/M8	8086	OR R8,R/M8	r8 OR r/m8.
OR	R16,R/M16	8086	OR R16,R/M16	r16 OR r/m16.
OR	R32,R/M32	386	OR R32,R/M32	r32 OR r/m32.
OR	R64,R/M64	X64	OR R64,R/M64	r64 OR r/m64.
;--------------------------------------------------------
GENERAL	ORPD	Bitwise Logical OR of Packed Double Precision Floating-Point Values	ORPD
ORPD	XMM,XMM/M128	SSE2	ORPD XMM1,XMM2/M128	Return the bitwise logical OR of packed DP FP values in xmm1 and xmm2/mem.
GENERAL	VORPD	Bitwise Logical OR of Packed Double Precision Floating-Point Values	ORPD
VORPD	XMM,XMM,XMM/M128	AVX	VORPD XMM1,XMM2,XMM3/M128	Return the bitwise logical OR of packed DP FP values in xmm2 and xmm3/mem.
VORPD	YMM,YMM,YMM/M256	AVX	VORPD YMM1,YMM2,YMM3/M256	Return the bitwise logical OR of packed DP FP values in ymm2 and ymm3/mem.
VORPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VORPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical OR of packed DP FP values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VORPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VORPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical OR of packed DP FP values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VORPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_DQ	VORPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical OR of packed DP FP values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	ORPS	Bitwise Logical OR of Packed Single Precision Floating-Point Values	ORPS
ORPS	XMM,XMM/M128	SSE	ORPS XMM1,XMM2/M128	Return the bitwise logical OR of packed SP FP values in xmm1 and xmm2/mem.
GENERAL	VORPS	Bitwise Logical OR of Packed Single Precision Floating-Point Values	ORPS
VORPS	XMM,XMM,XMM/M128	AVX	VORPS XMM1,XMM2,XMM3/M128	Return the bitwise logical OR of packed SP FP values in xmm2 and xmm3/mem.
VORPS	YMM,YMM,YMM/M256	AVX	VORPS YMM1,YMM2,YMM3/M256	Return the bitwise logical OR of packed SP FP values in ymm2 and ymm3/mem.
VORPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VORPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical OR of packed SP FP values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VORPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_DQ	VORPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical OR of packed SP FP values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VORPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_DQ	VORPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical OR of packed SP FP values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	OUT	Output to Port	OUT
OUT	IMM8,AL	8086	OUT IMM8,AL	Output byte in AL to I/O port address imm8.
OUT	IMM8,AX	8086	OUT IMM8,AX	Output word in AX to I/O port address imm8.
OUT	IMM8,EAX	8086	OUT IMM8,EAX	Output doubleword in EAX to I/O port address imm8.
OUT	DX,AL	8086	OUT DX,AL	Output byte in AL to I/O port address in DX.
OUT	DX,AX	8086	OUT DX,AX	Output word in AX to I/O port address in DX.
OUT	DX,EAX	8086	OUT DX,EAX	Output doubleword in EAX to I/O port address in DX.
;--------------------------------------------------------
GENERAL	OUTS	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD
OUTS	DX,M8	8086	OUTS DX,M8	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
OUTS	DX,M16	8086	OUTS DX,M16	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
OUTS	DX,M32	386	OUTS DX,M32	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
GENERAL	OUTSB	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD
OUTSB		8086	OUTSB	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
GENERAL	OUTSW	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD
OUTSW		8086	OUTSW	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
GENERAL	OUTSD	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD
OUTSD		8086	OUTSD	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX**.
;--------------------------------------------------------
GENERAL	PABSB	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
PABSB	MM,MM/M64	SSSE3	PABSB MM1,MM2/M64	Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1.
PABSB	XMM,XMM/M128	SSSE3	PABSB XMM1,XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
GENERAL	PABSW	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
PABSW	MM,MM/M64	SSSE3	PABSW MM1,MM2/M64	Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1.
PABSW	XMM,XMM/M128	SSSE3	PABSW XMM1,XMM2/M128	Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
GENERAL	PABSD	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
PABSD	MM,MM/M64	SSSE3	PABSD MM1,MM2/M64	Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1.
PABSD	XMM,XMM/M128	SSSE3	PABSD XMM1,XMM2/M128	Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
GENERAL	VPABSB	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
VPABSB	XMM,XMM/M128	AVX	VPABSB XMM1,XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSB	YMM,YMM/M256	AVX2	VPABSB YMM1,YMM2/M256	Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1.
VPABSB	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VPABSB XMM1{K1}{Z},XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1.
VPABSB	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_BW	VPABSB YMM1{K1}{Z},YMM2/M256	Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1.
VPABSB	ZMM{K}{Z},ZMM/M512	AVX512_BW	VPABSB ZMM1{K1}{Z},ZMM2/M512	Compute the absolute value of bytes in zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1.
GENERAL	VPABSW	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
VPABSW	XMM,XMM/M128	AVX	VPABSW XMM1,XMM2/M128	Compute the absolute value of 16- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSW	YMM,YMM/M256	AVX2	VPABSW YMM1,YMM2/M256	Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1.
VPABSW	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VPABSW XMM1{K1}{Z},XMM2/M128	Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1.
VPABSW	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_BW	VPABSW YMM1{K1}{Z},YMM2/M256	Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1.
VPABSW	ZMM{K}{Z},ZMM/M512	AVX512_BW	VPABSW ZMM1{K1}{Z},ZMM2/M512	Compute the absolute value of 16-bit integers in zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1.
GENERAL	VPABSD	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
VPABSD	XMM,XMM/M128	AVX	VPABSD XMM1,XMM2/M128	Compute the absolute value of 32- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSD	YMM,YMM/M256	AVX2	VPABSD YMM1,YMM2/M256	Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1.
VPABSD	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPABSD XMM1{K1}{Z},XMM2/M128/M32BCST	Compute the absolute value of 32-bit integers in xmm2/m128/m32bcst and store UNSIGNED result in xmm1 using writemask k1.
VPABSD	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPABSD YMM1{K1}{Z},YMM2/M256/M32BCST	Compute the absolute value of 32-bit integers in ymm2/m256/m32bcst and store UNSIGNED result in ymm1 using writemask k1.
VPABSD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_F	VPABSD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Compute the absolute value of 32-bit integers in zmm2/m512/m32bcst and store UNSIGNED result in zmm1 using writemask k1.
GENERAL	VPABSQ	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ
VPABSQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPABSQ XMM1{K1}{Z},XMM2/M128/M64BCST	Compute the absolute value of 64-bit integers in xmm2/m128/m64bcst and store UNSIGNED result in xmm1 using writemask k1.
VPABSQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPABSQ YMM1{K1}{Z},YMM2/M256/M64BCST	Compute the absolute value of 64-bit integers in ymm2/m256/m64bcst and store UNSIGNED result in ymm1 using writemask k1.
VPABSQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_F	VPABSQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Compute the absolute value of 64-bit integers in zmm2/m512/m64bcst and store UNSIGNED result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PACKSSWB	Pack with Signed Saturation	PACKSSWB_PACKSSDW
PACKSSWB	MM,MM/M64	MMX	PACKSSWB MM1,MM2/M64	Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation.
PACKSSWB	XMM,XMM/M128	SSE2	PACKSSWB XMM1,XMM2/M128	Converts 8 packed signed word integers from xmm1 and from xxm2/m128 into 16 packed signed byte integers in xxm1 using signed saturation.
GENERAL	PACKSSDW	Pack with Signed Saturation	PACKSSWB_PACKSSDW
PACKSSDW	MM,MM/M64	MMX	PACKSSDW MM1,MM2/M64	Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation.
PACKSSDW	XMM,XMM/M128	SSE2	PACKSSDW XMM1,XMM2/M128	Converts 4 packed signed doubleword integers from xmm1 and from xxm2/m128 into 8 packed signed word integers in xxm1 using signed saturation.
GENERAL	VPACKSSWB	Pack with Signed Saturation	PACKSSWB_PACKSSDW
VPACKSSWB	XMM,XMM,XMM/M128	AVX	VPACKSSWB XMM1,XMM2,XMM3/M128	Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation.
VPACKSSWB	YMM,YMM,YMM/M256	AVX2	VPACKSSWB YMM1,YMM2,YMM3/M256	Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation.
VPACKSSWB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPACKSSWB XMM1{K1}{Z},XMM2,XMM3/M128	Converts packed signed word integers from xmm2 and from xmm3/m128 into packed signed byte integers in xmm1 using signed saturation under writemask k1.
VPACKSSWB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPACKSSWB YMM1{K1}{Z},YMM2,YMM3/M256	Converts packed signed word integers from ymm2 and from ymm3/m256 into packed signed byte integers in ymm1 using signed saturation under writemask k1.
VPACKSSWB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPACKSSWB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Converts packed signed word integers from zmm2 and from zmm3/m512 into packed signed byte integers in zmm1 using signed saturation under writemask k1.
GENERAL	VPACKSSDW	Pack with Signed Saturation	PACKSSWB_PACKSSDW
VPACKSSDW	XMM,XMM,XMM/M128	AVX	VPACKSSDW XMM1,XMM2,XMM3/M128	Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation.
VPACKSSDW	YMM,YMM,YMM/M256	AVX2	VPACKSSDW YMM1,YMM2,YMM3/M256	Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation.
VPACKSSDW	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_BW	VPACKSSDW XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Converts packed signed doubleword integers from xmm2 and from xmm3/m128/m32bcst into packed signed word integers in xmm1 using signed saturation under writemask k1.
VPACKSSDW	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_BW	VPACKSSDW YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Converts packed signed doubleword integers from ymm2 and from ymm3/m256/m32bcst into packed signed word integers in ymm1 using signed saturation under writemask k1.
VPACKSSDW	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_BW	VPACKSSDW ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Converts packed signed doubleword integers from zmm2 and from zmm3/m512/m32bcst into packed signed word integers in zmm1 using signed saturation under writemask k1.
;--------------------------------------------------------
GENERAL	PACKUSDW	Pack with Unsigned Saturation	PACKUSDW
PACKUSDW	XMM,XMM/M128	SSE4_1	PACKUSDW XMM1,XMM2/M128	Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
GENERAL	VPACKUSDW	Pack with Unsigned Saturation	PACKUSDW
VPACKUSDW	XMM,XMM,XMM/M128	AVX	VPACKUSDW XMM1,XMM2,XMM3/M128	Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
VPACKUSDW	YMM,YMM,YMM/M256	AVX2	VPACKUSDW YMM1,YMM2,YMM3/M256	Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m256 into 16 packed unsigned word integers in ymm1 using unsigned saturation.
VPACKUSDW	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_BW	VPACKUSDW XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Convert packed signed doubleword integers from xmm2 and packed signed doubleword integers from xmm3/m128/m32bcst into packed unsigned word integers in xmm1 using unsigned saturation under writemask k1.
VPACKUSDW	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_BW	VPACKUSDW YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Convert packed signed doubleword integers from ymm2 and packed signed doubleword integers from ymm3/m256/m32bcst into packed unsigned word integers in ymm1 using unsigned saturation under writemask k1.
VPACKUSDW	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_BW	VPACKUSDW ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Convert packed signed doubleword integers from zmm2 and packed signed doubleword integers from zmm3/m512/m32bcst into packed unsigned word integers in zmm1 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	PACKUSWB	Pack with Unsigned Saturation	PACKUSWB
PACKUSWB	MM,MM/M64	MMX	PACKUSWB MM,MM/M64	Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation.
PACKUSWB	XMM,XMM/M128	SSE2	PACKUSWB XMM1,XMM2/M128	Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
GENERAL	VPACKUSWB	Pack with Unsigned Saturation	PACKUSWB
VPACKUSWB	XMM,XMM,XMM/M128	AVX	VPACKUSWB XMM1,XMM2,XMM3/M128	Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
VPACKUSWB	YMM,YMM,YMM/M256	AVX2	VPACKUSWB YMM1,YMM2,YMM3/M256	Converts 16 signed word integers from ymm2 and 16signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation.
VPACKUSWB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPACKUSWB XMM1{K1}{Z},XMM2,XMM3/M128	Converts signed word integers from xmm2 and signed word integers from xmm3/m128 into unsigned byte integers in xmm1 using unsigned saturation under writemask k1.
VPACKUSWB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPACKUSWB YMM1{K1}{Z},YMM2,YMM3/M256	Converts signed word integers from ymm2 and signed word integers from ymm3/m256 into unsigned byte integers in ymm1 using unsigned saturation under writemask k1.
VPACKUSWB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPACKUSWB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Converts signed word integers from zmm2 and signed word integers from zmm3/m512 into unsigned byte integers in zmm1 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	PADDB	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
PADDB	MM,MM/M64	MMX	PADDB MM,MM/M64	Add packed byte integers from mm/m64 and mm.
PADDB	XMM,XMM/M128	SSE2	PADDB XMM1,XMM2/M128	Add packed byte integers from xmm2/m128 and xmm1.
GENERAL	PADDW	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
PADDW	MM,MM/M64	MMX	PADDW MM,MM/M64	Add packed word integers from mm/m64 and mm.
PADDW	XMM,XMM/M128	SSE2	PADDW XMM1,XMM2/M128	Add packed word integers from xmm2/m128 and xmm1.
GENERAL	PADDD	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
PADDD	MM,MM/M64	MMX	PADDD MM,MM/M64	Add packed doubleword integers from mm/m64 and mm.
PADDD	XMM,XMM/M128	SSE2	PADDD XMM1,XMM2/M128	Add packed doubleword integers from xmm2/m128 and xmm1.
GENERAL	PADDQ	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
PADDQ	MM,MM/M64	MMX	PADDQ MM,MM/M64	Add packed quadword integers from mm/m64 and mm.
PADDQ	XMM,XMM/M128	SSE2	PADDQ XMM1,XMM2/M128	Add packed quadword integers from xmm2/m128 and xmm1.
GENERAL	VPADDB	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
VPADDB	XMM,XMM,XMM/M128	AVX	VPADDB XMM1,XMM2,XMM3/M128	Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1.
VPADDB	YMM,YMM,YMM/M256	AVX2	VPADDB YMM1,YMM2,YMM3/M256	Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1.
VPADDB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1.
VPADDB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1.
VPADDB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed byte integers from zmm2, and zmm3/m512 and store in zmm1 using writemask k1.
GENERAL	VPADDW	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
VPADDW	XMM,XMM,XMM/M128	AVX	VPADDW XMM1,XMM2,XMM3/M128	Add packed word integers from xmm2, xmm3/m128 and store in xmm1.
VPADDW	YMM,YMM,YMM/M256	AVX2	VPADDW YMM1,YMM2,YMM3/M256	Add packed word integers from ymm2, ymm3/m256 and store in ymm1.
VPADDW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed word integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1.
VPADDW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed word integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1.
VPADDW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed word integers from zmm2, and zmm3/m512 and store in zmm1 using writemask k1.
GENERAL	VPADDD	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
VPADDD	XMM,XMM,XMM/M128	AVX	VPADDD XMM1,XMM2,XMM3/M128	Add packed doubleword integers from xmm2, xmm3/m128 and store in xmm1.
VPADDD	YMM,YMM,YMM/M256	AVX2	VPADDD YMM1,YMM2,YMM3/M256	Add packed doubleword integers from ymm2, ymm3/m256 and store in ymm1.
VPADDD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPADDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Add packed doubleword integers from xmm2, and xmm3/m128/m32bcst and store in xmm1 using writemask k1.
VPADDD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPADDD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Add packed doubleword integers from ymm2, ymm3/m256/m32bcst and store in ymm1 using writemask k1.
VPADDD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPADDD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Add packed doubleword integers from zmm2, zmm3/m512/m32bcst and store in zmm1 using writemask k1.
GENERAL	VPADDQ	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ
VPADDQ	XMM,XMM,XMM/M128	AVX	VPADDQ XMM1,XMM2,XMM3/M128	Add packed quadword integers from xmm2, xmm3/m128 and store in xmm1.
VPADDQ	YMM,YMM,YMM/M256	AVX2	VPADDQ YMM1,YMM2,YMM3/M256	Add packed quadword integers from ymm2, ymm3/m256 and store in ymm1.
VPADDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPADDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Add packed quadword integers from xmm2, and xmm3/m128/m64bcst and store in xmm1 using writemask k1.
VPADDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPADDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Add packed quadword integers from ymm2, ymm3/m256/m64bcst and store in ymm1 using writemask k1.
VPADDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPADDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Add packed quadword integers from zmm2, zmm3/m512/m64bcst and store in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PADDSB	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW
PADDSB	MM,MM/M64	MMX	PADDSB MM,MM/M64	Add packed signed byte integers from mm/m64 and mm and saturate the results.
PADDSB	XMM,XMM/M128	SSE2	PADDSB XMM1,XMM2/M128	Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results.
GENERAL	PADDSW	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW
PADDSW	MM,MM/M64	MMX	PADDSW MM,MM/M64	Add packed signed word integers from mm/m64 and mm and saturate the results.
PADDSW	XMM,XMM/M128	SSE2	PADDSW XMM1,XMM2/M128	Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results.
GENERAL	VPADDSB	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW
VPADDSB	XMM,XMM,XMM/M128	AVX	VPADDSB XMM1,XMM2,XMM3/M128	Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results.
VPADDSB	YMM,YMM,YMM/M256	AVX2	VPADDSB YMM1,YMM2,YMM3/M256	Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDSB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed signed byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDSB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed signed byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
GENERAL	VPADDSW	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW
VPADDSW	XMM,XMM,XMM/M128	AVX	VPADDSW XMM1,XMM2,XMM3/M128	Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results.
VPADDSW	YMM,YMM,YMM/M256	AVX2	VPADDSW YMM1,YMM2,YMM3/M256	Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDSW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed signed word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDSW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed signed word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PADDUSB	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW
PADDUSB	MM,MM/M64	MMX	PADDUSB MM,MM/M64	Add packed unsigned byte integers from mm/m64 and mm and saturate the results.
PADDUSB	XMM,XMM/M128	SSE2	PADDUSB XMM1,XMM2/M128	Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results.
GENERAL	PADDUSW	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW
PADDUSW	MM,MM/M64	MMX	PADDUSW MM,MM/M64	Add packed unsigned word integers from mm/m64 and mm and saturate the results.
PADDUSW	XMM,XMM/M128	SSE2	PADDUSW XMM1,XMM2/M128	Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results.
GENERAL	VPADDUSB	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW
VPADDUSB	XMM,XMM,XMM/M128	AVX	VPADDUSB XMM1,XMM2,XMM3/M128	Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results.
VPADDUSB	YMM,YMM,YMM/M256	AVX2	VPADDUSB YMM1,YMM2,YMM3/M256	Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDUSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDUSB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed unsigned byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDUSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDUSB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDUSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDUSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed unsigned byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
GENERAL	VPADDUSW	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW
VPADDUSW	XMM,XMM,XMM/M128	AVX	VPADDUSW XMM1,XMM2,XMM3/M128	Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results.
VPADDUSW	YMM,YMM,YMM/M256	AVX2	VPADDUSW YMM1,YMM2,YMM3/M256	Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDUSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPADDUSW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed unsigned word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDUSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPADDUSW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDUSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPADDUSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed unsigned word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PALIGNR	Packed Align Right	PALIGNR
PALIGNR	MM,MM/M64,IMM8	SSSE3	PALIGNR MM1,MM2/M64,IMM8	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1.
PALIGNR	XMM,XMM/M128,IMM8	SSSE3	PALIGNR XMM1,XMM2/M128,IMM8	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1.
GENERAL	VPALIGNR	Packed Align Right	PALIGNR
VPALIGNR	XMM,XMM,XMM/M128,IMM8	AVX	VPALIGNR XMM1,XMM2,XMM3/M128,IMM8	Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
VPALIGNR	YMM,YMM,YMM/M256,IMM8	AVX2	VPALIGNR YMM1,YMM2,YMM3/M256,IMM8	Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1.
VPALIGNR	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPALIGNR XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Concatenate xmm2 and xmm3/m128 into a 32- byte intermediate result, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
VPALIGNR	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPALIGNR YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1.
VPALIGNR	ZMM{K}{Z},ZMM,ZMM/M512,IMM8	AVX512_BW	VPALIGNR ZMM1{K1}{Z},ZMM2,ZMM3/M512,IMM8	Concatenate pairs of 16 bytes in zmm2 and zmm3/m512 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and four 16-byte results are stored in zmm1.
;--------------------------------------------------------
GENERAL	PAND	Logical AND	PAND
PAND	MM,MM/M64	MMX	PAND MM,MM/M64	Bitwise AND mm/m64 and mm.
PAND	XMM,XMM/M128	SSE2	PAND XMM1,XMM2/M128	Bitwise AND of xmm2/m128 and xmm1.
GENERAL	VPAND	Logical AND	PAND
VPAND	XMM,XMM,XMM/M128	AVX	VPAND XMM1,XMM2,XMM3/M128	Bitwise AND of xmm3/m128 and xmm.
VPAND	YMM,YMM,YMM/M256	AVX2	VPAND YMM1,YMM2,YMM3/M256	Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1.
GENERAL	VPANDD	Logical AND	PAND
VPANDD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPANDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1.
VPANDD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPANDD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1.
VPANDD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPANDD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1.
GENERAL	VPANDQ	Logical AND	PAND
VPANDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPANDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1.
VPANDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPANDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1.
VPANDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPANDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PANDN	Logical AND NOT	PANDN
PANDN	MM,MM/M64	MMX	PANDN MM,MM/M64	Bitwise AND NOT of mm/m64 and mm.
PANDN	XMM,XMM/M128	SSE2	PANDN XMM1,XMM2/M128	Bitwise AND NOT of xmm2/m128 and xmm1.
GENERAL	VPANDN	Logical AND NOT	PANDN
VPANDN	XMM,XMM,XMM/M128	AVX	VPANDN XMM1,XMM2,XMM3/M128	Bitwise AND NOT of xmm3/m128 and xmm2.
VPANDN	YMM,YMM,YMM/M256	AVX2	VPANDN YMM1,YMM2,YMM3/M256	Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1.
GENERAL	VPANDND	Logical AND NOT	PANDN
VPANDND	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPANDND XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise AND NOT of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1.
VPANDND	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPANDND YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise AND NOT of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1.
VPANDND	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPANDND ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise AND NOT of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1.
GENERAL	VPANDNQ	Logical AND NOT	PANDN
VPANDNQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPANDNQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise AND NOT of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1.
VPANDNQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPANDNQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise AND NOT of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1.
VPANDNQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPANDNQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise AND NOT of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PAUSE	Spin Loop Hint	PAUSE
PAUSE		8086	PAUSE	Gives hint to processor that improves performance of spin-wait loops.
;--------------------------------------------------------
GENERAL	PAVGB	Average Packed Integers	PAVGB_PAVGW
PAVGB	MM,MM/M64	SSE	PAVGB MM1,MM2/M64	Average packed unsigned byte integers from mm2/m64 and mm1 with rounding.
PAVGB	XMM,XMM/M128	SSE2	PAVGB XMM1,XMM2/M128	Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding.
GENERAL	PAVGW	Average Packed Integers	PAVGB_PAVGW
PAVGW	MM,MM/M64	SSE	PAVGW MM1,MM2/M64	Average packed unsigned word integers from mm2/m64 and mm1 with rounding.
PAVGW	XMM,XMM/M128	SSE2	PAVGW XMM1,XMM2/M128	Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding.
GENERAL	VPAVGB	Average Packed Integers	PAVGB_PAVGW
VPAVGB	XMM,XMM,XMM/M128	AVX	VPAVGB XMM1,XMM2,XMM3/M128	Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding.
VPAVGB	YMM,YMM,YMM/M256	AVX2	VPAVGB YMM1,YMM2,YMM3/M256	Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1.
VPAVGB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPAVGB XMM1{K1}{Z},XMM2,XMM3/M128	Average packed unsigned byte integers from xmm2, and xmm3/m128 with rounding and store to xmm1 under writemask k1.
VPAVGB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPAVGB YMM1{K1}{Z},YMM2,YMM3/M256	Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1 under writemask k1.
VPAVGB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPAVGB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Average packed unsigned byte integers from zmm2, and zmm3/m512 with rounding and store to zmm1 under writemask k1.
GENERAL	VPAVGW	Average Packed Integers	PAVGB_PAVGW
VPAVGW	XMM,XMM,XMM/M128	AVX	VPAVGW XMM1,XMM2,XMM3/M128	Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding.
VPAVGW	YMM,YMM,YMM/M256	AVX2	VPAVGW YMM1,YMM2,YMM3/M256	Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1.
VPAVGW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPAVGW XMM1{K1}{Z},XMM2,XMM3/M128	Average packed unsigned word integers from xmm2, xmm3/m128 with rounding to xmm1 under writemask k1.
VPAVGW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPAVGW YMM1{K1}{Z},YMM2,YMM3/M256	Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1 under writemask k1.
VPAVGW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPAVGW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Average packed unsigned word integers from zmm2, zmm3/m512 with rounding to zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PBLENDVB	Variable Blend Packed Bytes	PBLENDVB
PBLENDVB	XMM,XMM/M128,XMM_ZERO	SSE4_1	PBLENDVB XMM1,XMM2/M128,<XMM0>	Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1.
GENERAL	VPBLENDVB	Variable Blend Packed Bytes	PBLENDVB
VPBLENDVB	XMM,XMM,XMM/M128,XMM	AVX	VPBLENDVB XMM1,XMM2,XMM3/M128,XMM4	Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1.
VPBLENDVB	YMM,YMM,YMM/M256,YMM	AVX2	VPBLENDVB YMM1,YMM2,YMM3/M256,YMM4	Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1.
;--------------------------------------------------------
GENERAL	PBLENDW	Blend Packed Words	PBLENDW
PBLENDW	XMM,XMM/M128,IMM8	SSE4_1	PBLENDW XMM1,XMM2/M128,IMM8	Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
GENERAL	VPBLENDW	Blend Packed Words	PBLENDW
VPBLENDW	XMM,XMM,XMM/M128,IMM8	AVX	VPBLENDW XMM1,XMM2,XMM3/M128,IMM8	Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VPBLENDW	YMM,YMM,YMM/M256,IMM8	AVX2	VPBLENDW YMM1,YMM2,YMM3/M256,IMM8	Select words from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
;--------------------------------------------------------
GENERAL	PCLMULQDQ	Carry-Less Multiplication Quadword	PCLMULQDQ
PCLMULQDQ	XMM,XMM/M128,IMM8	PCLMULQDQ	PCLMULQDQ XMM1,XMM2/M128,IMM8	Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm1 and xmm2/m128 should be used.
GENERAL	VPCLMULQDQ	Carry-Less Multiplication Quadword	PCLMULQDQ
VPCLMULQDQ	XMM,XMM,XMM/M128,IMM8	PCLMULQDQ,AVX	VPCLMULQDQ XMM1,XMM2,XMM3/M128,IMM8	Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm2 and xmm3/m128 should be used.
;--------------------------------------------------------
GENERAL	PCMPEQB	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
PCMPEQB	MM,MM/M64	MMX	PCMPEQB MM,MM/M64	Compare packed bytes in mm/m64 and mm for equality.
PCMPEQB	XMM,XMM/M128	SSE2	PCMPEQB XMM1,XMM2/M128	Compare packed bytes in xmm2/m128 and xmm1 for equality.
GENERAL	PCMPEQW	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
PCMPEQW	MM,MM/M64	MMX	PCMPEQW MM,MM/M64	Compare packed words in mm/m64 and mm for equality.
PCMPEQW	XMM,XMM/M128	SSE2	PCMPEQW XMM1,XMM2/M128	Compare packed words in xmm2/m128 and xmm1 for equality.
GENERAL	PCMPEQD	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
PCMPEQD	MM,MM/M64	MMX	PCMPEQD MM,MM/M64	Compare packed doublewords in mm/m64 and mm for equality.
PCMPEQD	XMM,XMM/M128	SSE2	PCMPEQD XMM1,XMM2/M128	Compare packed doublewords in xmm2/m128 and xmm1 for equality.
GENERAL	VPCMPEQB	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
VPCMPEQB	XMM,XMM,XMM/M128	AVX	VPCMPEQB XMM1,XMM2,XMM3/M128	Compare packed bytes in xmm3/m128 and xmm2 for equality.
VPCMPEQB	YMM,YMM,YMM/M256	AVX2	VPCMPEQB YMM1,YMM2,YMM3/M256	Compare packed bytes in ymm3/m256 and ymm2 for equality.
VPCMPEQB	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPCMPEQB K1{K2},XMM2,XMM3/M128	Compare packed bytes in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQB	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPCMPEQB K1{K2},YMM2,YMM3/M256	Compare packed bytes in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQB	K{K},ZMM,ZMM/M512	AVX512_BW	VPCMPEQB K1{K2},ZMM2,ZMM3/M512	Compare packed bytes in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
GENERAL	VPCMPEQW	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
VPCMPEQW	XMM,XMM,XMM/M128	AVX	VPCMPEQW XMM1,XMM2,XMM3/M128	Compare packed words in xmm3/m128 and xmm2 for equality.
VPCMPEQW	YMM,YMM,YMM/M256	AVX2	VPCMPEQW YMM1,YMM2,YMM3/M256	Compare packed words in ymm3/m256 and ymm2 for equality.
VPCMPEQW	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPCMPEQW K1{K2},XMM2,XMM3/M128	Compare packed words in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQW	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPCMPEQW K1{K2},YMM2,YMM3/M256	Compare packed words in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQW	K{K},ZMM,ZMM/M512	AVX512_BW	VPCMPEQW K1{K2},ZMM2,ZMM3/M512	Compare packed words in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
GENERAL	VPCMPEQD	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD
VPCMPEQD	XMM,XMM,XMM/M128	AVX	VPCMPEQD XMM1,XMM2,XMM3/M128	Compare packed doublewords in xmm3/m128 and xmm2 for equality.
VPCMPEQD	YMM,YMM,YMM/M256	AVX2	VPCMPEQD YMM1,YMM2,YMM3/M256	Compare packed doublewords in ymm3/m256 and ymm2 for equality.
VPCMPEQD	K{K},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPCMPEQD K1{K2},XMM2,XMM3/M128/M32BCST	Compare Equal between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQD	K{K},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPCMPEQD K1{K2},YMM2,YMM3/M256/M32BCST	Compare Equal between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQD	K{K},ZMM,ZMM/M512/M32BCST	AVX512_F	VPCMPEQD K1{K2},ZMM2,ZMM3/M512/M32BCST	Compare Equal between int32 vectors in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask k2.
;--------------------------------------------------------
GENERAL	PCMPEQQ	Compare Packed Qword Data for Equal	PCMPEQQ
PCMPEQQ	XMM,XMM/M128	SSE4_1	PCMPEQQ XMM1,XMM2/M128	Compare packed qwords in xmm2/m128 and xmm1 for equality.
GENERAL	VPCMPEQQ	Compare Packed Qword Data for Equal	PCMPEQQ
VPCMPEQQ	XMM,XMM,XMM/M128	AVX	VPCMPEQQ XMM1,XMM2,XMM3/M128	Compare packed quadwords in xmm3/m128 and xmm2 for equality.
VPCMPEQQ	YMM,YMM,YMM/M256	AVX2	VPCMPEQQ YMM1,YMM2,YMM3/M256	Compare packed quadwords in ymm3/m256 and ymm2 for equality.
VPCMPEQQ	K{K},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPCMPEQQ K1{K2},XMM2,XMM3/M128/M64BCST	Compare Equal between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQQ	K{K},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPCMPEQQ K1{K2},YMM2,YMM3/M256/M64BCST	Compare Equal between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512_F	VPCMPEQQ K1{K2},ZMM2,ZMM3/M512/M64BCST	Compare Equal between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
;--------------------------------------------------------
GENERAL	PCMPESTRI	Packed Compare Explicit Length Strings, Return Index	PCMPESTRI
PCMPESTRI	XMM,XMM/M128,IMM8	SSE4_2	PCMPESTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
GENERAL	VPCMPESTRI	Packed Compare Explicit Length Strings, Return Index	PCMPESTRI
VPCMPESTRI	XMM,XMM/M128,IMM8	AVX	VPCMPESTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
;--------------------------------------------------------
GENERAL	PCMPESTRM	Packed Compare Explicit Length Strings, Return Mask	PCMPESTRM
PCMPESTRM	XMM,XMM/M128,IMM8	SSE4_2	PCMPESTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0.
GENERAL	VPCMPESTRM	Packed Compare Explicit Length Strings, Return Mask	PCMPESTRM
VPCMPESTRM	XMM,XMM/M128,IMM8	AVX	VPCMPESTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0.
;--------------------------------------------------------
GENERAL	PCMPGTB	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
PCMPGTB	MM,MM/M64	MMX	PCMPGTB MM,MM/M64	Compare packed signed byte integers in mm and mm/m64 for greater than.
PCMPGTB	XMM,XMM/M128	SSE2	PCMPGTB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than.
GENERAL	PCMPGTW	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
PCMPGTW	MM,MM/M64	MMX	PCMPGTW MM,MM/M64	Compare packed signed word integers in mm and mm/m64 for greater than.
PCMPGTW	XMM,XMM/M128	SSE2	PCMPGTW XMM1,XMM2/M128	Compare packed signed word integers in xmm1 and xmm2/m128 for greater than.
GENERAL	PCMPGTD	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
PCMPGTD	MM,MM/M64	MMX	PCMPGTD MM,MM/M64	Compare packed signed doubleword integers in mm and mm/m64 for greater than.
PCMPGTD	XMM,XMM/M128	SSE2	PCMPGTD XMM1,XMM2/M128	Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than.
GENERAL	VPCMPGTB	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
VPCMPGTB	XMM,XMM,XMM/M128	AVX	VPCMPGTB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTB	YMM,YMM,YMM/M256	AVX2	VPCMPGTB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than.
VPCMPGTB	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPCMPGTB K1{K2},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTB	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPCMPGTB K1{K2},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTB	K{K},ZMM,ZMM/M512	AVX512_BW	VPCMPGTB K1{K2},ZMM2,ZMM3/M512	Compare packed signed byte integers in zmm2 and zmm3/m512 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
GENERAL	VPCMPGTW	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
VPCMPGTW	XMM,XMM,XMM/M128	AVX	VPCMPGTW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTW	YMM,YMM,YMM/M256	AVX2	VPCMPGTW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 for greater than.
VPCMPGTW	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPCMPGTW K1{K2},XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTW	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPCMPGTW K1{K2},YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTW	K{K},ZMM,ZMM/M512	AVX512_BW	VPCMPGTW K1{K2},ZMM2,ZMM3/M512	Compare packed signed word integers in zmm2 and zmm3/m512 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
GENERAL	VPCMPGTD	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD
VPCMPGTD	XMM,XMM,XMM/M128	AVX	VPCMPGTD XMM1,XMM2,XMM3/M128	Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTD	YMM,YMM,YMM/M256	AVX2	VPCMPGTD YMM1,YMM2,YMM3/M256	Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than.
VPCMPGTD	K{K},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPCMPGTD K1{K2},XMM2,XMM3/M128/M32BCST	Compare Greater between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTD	K{K},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPCMPGTD K1{K2},YMM2,YMM3/M256/M32BCST	Compare Greater between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTD	K{K},ZMM,ZMM/M512/M32BCST	AVX512_F	VPCMPGTD K1{K2},ZMM2,ZMM3/M512/M32BCST	Compare Greater between int32 elements in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask. k2.
;--------------------------------------------------------
GENERAL	PCMPGTQ	Compare Packed Data for Greater Than	PCMPGTQ
PCMPGTQ	XMM,XMM/M128	SSE4_2	PCMPGTQ XMM1,XMM2/M128	Compare packed signed qwords in xmm2/m128 and xmm1 for greater than.
GENERAL	VPCMPGTQ	Compare Packed Data for Greater Than	PCMPGTQ
VPCMPGTQ	XMM,XMM,XMM/M128	AVX	VPCMPGTQ XMM1,XMM2,XMM3/M128	Compare packed signed qwords in xmm2 and xmm3/m128 for greater than.
VPCMPGTQ	YMM,YMM,YMM/M256	AVX2	VPCMPGTQ YMM1,YMM2,YMM3/M256	Compare packed signed qwords in ymm2 and ymm3/m256 for greater than.
VPCMPGTQ	K{K},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPCMPGTQ K1{K2},XMM2,XMM3/M128/M64BCST	Compare Greater between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTQ	K{K},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPCMPGTQ K1{K2},YMM2,YMM3/M256/M64BCST	Compare Greater between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512_F	VPCMPGTQ K1{K2},ZMM2,ZMM3/M512/M64BCST	Compare Greater between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
;--------------------------------------------------------
GENERAL	PCMPISTRI	Packed Compare Implicit Length Strings, Return Index	PCMPISTRI
PCMPISTRI	XMM,XMM/M128,IMM8	SSE4_2	PCMPISTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
GENERAL	VPCMPISTRI	Packed Compare Implicit Length Strings, Return Index	PCMPISTRI
VPCMPISTRI	XMM,XMM/M128,IMM8	AVX	VPCMPISTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
;--------------------------------------------------------
GENERAL	PCMPISTRM	Packed Compare Implicit Length Strings, Return Mask	PCMPISTRM
PCMPISTRM	XMM,XMM/M128,IMM8	SSE4_2	PCMPISTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0.
GENERAL	VPCMPISTRM	Packed Compare Implicit Length Strings, Return Mask	PCMPISTRM
VPCMPISTRM	XMM,XMM/M128,IMM8	AVX	VPCMPISTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0.
;--------------------------------------------------------
GENERAL	PCONFIG	Platform Configuration	PCONFIG
PCONFIG		PCONFIG	PCONFIG	This instruction is used to execute functions for configuring platform features. EAX: Leaf function to be invoked. RBX/RCX/RDX: Leaf-specific purpose.
;--------------------------------------------------------
GENERAL	PDEP	Parallel Bits Deposit	PDEP
PDEP	R32,R32,R/M32	BMI2	PDEP R32A,R32B,R/M32	Parallel deposit of bits from r32b using mask in r/m32, result is writ- ten to r32a.
PDEP	R64,R64,R/M64	BMI2	PDEP R64A,R64B,R/M64	Parallel deposit of bits from r64b using mask in r/m64, result is writ- ten to r64a.
;--------------------------------------------------------
GENERAL	PEXT	Parallel Bits Extract	PEXT
PEXT	R32,R32,R/M32	BMI2	PEXT R32A,R32B,R/M32	Parallel extract of bits from r32b using mask in r/m32, result is writ- ten to r32a.
PEXT	R64,R64,R/M64	BMI2	PEXT R64A,R64B,R/M64	Parallel extract of bits from r64b using mask in r/m64, result is writ- ten to r64a.
;--------------------------------------------------------
GENERAL	PEXTRB	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
PEXTRB	REG/M8,XMM,IMM8	SSE4_1	PEXTRB REG/M8,XMM2,IMM8	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r32 or r64 are zeroed.
GENERAL	PEXTRD	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
PEXTRD	R/M32,XMM,IMM8	SSE4_1	PEXTRD R/M32,XMM2,IMM8	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32.
GENERAL	PEXTRQ	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
PEXTRQ	R/M64,XMM,IMM8	SSE4_1	PEXTRQ R/M64,XMM2,IMM8	Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64.
GENERAL	VPEXTRB	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
VPEXTRB	REG/M8,XMM,IMM8	AVX	VPEXTRB REG/M8,XMM2,IMM8	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros.
VPEXTRB	REG/M8,XMM,IMM8	AVX512_BW	VPEXTRB REG/M8,XMM2,IMM8	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros.
GENERAL	VPEXTRD	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
VPEXTRD	R32/M32,XMM,IMM8	AVX	VPEXTRD R32/M32,XMM2,IMM8	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
VPEXTRD	R32/M32,XMM,IMM8	AVX512_DQ	VPEXTRD R32/M32,XMM2,IMM8	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
GENERAL	VPEXTRQ	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ
VPEXTRQ	R64/M64,XMM,IMM8	AVX	VPEXTRQ R64/M64,XMM2,IMM8	Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
VPEXTRQ	R64/M64,XMM,IMM8	AVX512_DQ	VPEXTRQ R64/M64,XMM2,IMM8	Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
;--------------------------------------------------------
GENERAL	PEXTRW	Extract Word	PEXTRW
PEXTRW	REG,MM,IMM8	SSE	PEXTRW REG,MM,IMM8	Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
PEXTRW	REG,XMM,IMM8	SSE2	PEXTRW REG,XMM,IMM8	Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
PEXTRW	REG/M16,XMM,IMM8	SSE4_1	PEXTRW REG/M16,XMM,IMM8	Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero-extend the result in the destination, r32 or r64.
GENERAL	VPEXTRW	Extract Word	PEXTRW
VPEXTRW	REG,XMM,IMM8	AVX	VPEXTRW REG,XMM1,IMM8	Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero- extend the result. The upper bits of r64/r32 is filled with zeros.
VPEXTRW	REG/M16,XMM,IMM8	AVX	VPEXTRW REG/M16,XMM2,IMM8	Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros.
VPEXTRW	REG,XMM,IMM8	AVX512_BW	VPEXTRW REG,XMM1,IMM8	Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros.
VPEXTRW	REG/M16,XMM,IMM8	AVX512_BW	VPEXTRW REG/M16,XMM2,IMM8	Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros.
;--------------------------------------------------------
GENERAL	PHADDSW	Packed Horizontal Add and Saturate	PHADDSW
PHADDSW	MM,MM/M64	SSSE3	PHADDSW MM1,MM2/M64	Add 16-bit signed integers horizontally, pack saturated integers to mm1.
PHADDSW	XMM,XMM/M128	SSSE3	PHADDSW XMM1,XMM2/M128	Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
GENERAL	VPHADDSW	Packed Horizontal Add and Saturate	PHADDSW
VPHADDSW	XMM,XMM,XMM/M128	AVX	VPHADDSW XMM1,XMM2,XMM3/M128	Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
VPHADDSW	YMM,YMM,YMM/M256	AVX2	VPHADDSW YMM1,YMM2,YMM3/M256	Add 16-bit signed integers horizontally, pack saturated integers to ymm1.
;--------------------------------------------------------
GENERAL	PHADDW	Packed Horizontal Add	PHADDW_PHADDD
PHADDW	MM,MM/M64	SSSE3	PHADDW MM1,MM2/M64	Add 16-bit integers horizontally, pack to mm1.
PHADDW	XMM,XMM/M128	SSSE3	PHADDW XMM1,XMM2/M128	Add 16-bit integers horizontally, pack to xmm1.
GENERAL	PHADDD	Packed Horizontal Add	PHADDW_PHADDD
PHADDD	MM,MM/M64	SSSE3	PHADDD MM1,MM2/M64	Add 32-bit integers horizontally, pack to mm1.
PHADDD	XMM,XMM/M128	SSSE3	PHADDD XMM1,XMM2/M128	Add 32-bit integers horizontally, pack to xmm1.
GENERAL	VPHADDW	Packed Horizontal Add	PHADDW_PHADDD
VPHADDW	XMM,XMM,XMM/M128	AVX	VPHADDW XMM1,XMM2,XMM3/M128	Add 16-bit integers horizontally, pack to xmm1.
VPHADDW	YMM,YMM,YMM/M256	AVX2	VPHADDW YMM1,YMM2,YMM3/M256	Add 16-bit signed integers horizontally, pack to ymm1.
GENERAL	VPHADDD	Packed Horizontal Add	PHADDW_PHADDD
VPHADDD	XMM,XMM,XMM/M128	AVX	VPHADDD XMM1,XMM2,XMM3/M128	Add 32-bit integers horizontally, pack to xmm1.
VPHADDD	YMM,YMM,YMM/M256	AVX2	VPHADDD YMM1,YMM2,YMM3/M256	Add 32-bit signed integers horizontally, pack to ymm1.
;--------------------------------------------------------
GENERAL	PHMINPOSUW	Packed Horizontal Word Minimum	PHMINPOSUW
PHMINPOSUW	XMM,XMM/M128	SSE4_1	PHMINPOSUW XMM1,XMM2/M128	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second- lowest word of xmm1.
GENERAL	VPHMINPOSUW	Packed Horizontal Word Minimum	PHMINPOSUW
VPHMINPOSUW	XMM,XMM/M128	AVX	VPHMINPOSUW XMM1,XMM2/M128	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second- lowest word of xmm1.
;--------------------------------------------------------
GENERAL	PHSUBSW	Packed Horizontal Subtract and Saturate	PHSUBSW
PHSUBSW	MM,MM/M64	SSSE3	PHSUBSW MM1,MM2/M64	Subtract 16-bit signed integer horizontally, pack saturated integers to mm1.
PHSUBSW	XMM,XMM/M128	SSSE3	PHSUBSW XMM1,XMM2/M128	Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
GENERAL	VPHSUBSW	Packed Horizontal Subtract and Saturate	PHSUBSW
VPHSUBSW	XMM,XMM,XMM/M128	AVX	VPHSUBSW XMM1,XMM2,XMM3/M128	Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
VPHSUBSW	YMM,YMM,YMM/M256	AVX2	VPHSUBSW YMM1,YMM2,YMM3/M256	Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1.
;--------------------------------------------------------
GENERAL	PHSUBW	Packed Horizontal Subtract	PHSUBW_PHSUBD
PHSUBW	MM,MM/M64	SSSE3	PHSUBW MM1,MM2/M64	Subtract 16-bit signed integers horizontally, pack to mm1.
PHSUBW	XMM,XMM/M128	SSSE3	PHSUBW XMM1,XMM2/M128	Subtract 16-bit signed integers horizontally, pack to xmm1.
GENERAL	PHSUBD	Packed Horizontal Subtract	PHSUBW_PHSUBD
PHSUBD	MM,MM/M64	SSSE3	PHSUBD MM1,MM2/M64	Subtract 32-bit signed integers horizontally, pack to mm1.
PHSUBD	XMM,XMM/M128	SSSE3	PHSUBD XMM1,XMM2/M128	Subtract 32-bit signed integers horizontally, pack to xmm1.
GENERAL	VPHSUBW	Packed Horizontal Subtract	PHSUBW_PHSUBD
VPHSUBW	XMM,XMM,XMM/M128	AVX	VPHSUBW XMM1,XMM2,XMM3/M128	Subtract 16-bit signed integers horizontally, pack to xmm1.
VPHSUBW	YMM,YMM,YMM/M256	AVX2	VPHSUBW YMM1,YMM2,YMM3/M256	Subtract 16-bit signed integers horizontally, pack to ymm1.
GENERAL	VPHSUBD	Packed Horizontal Subtract	PHSUBW_PHSUBD
VPHSUBD	XMM,XMM,XMM/M128	AVX	VPHSUBD XMM1,XMM2,XMM3/M128	Subtract 32-bit signed integers horizontally, pack to xmm1.
VPHSUBD	YMM,YMM,YMM/M256	AVX2	VPHSUBD YMM1,YMM2,YMM3/M256	Subtract 32-bit signed integers horizontally, pack to ymm1.
;--------------------------------------------------------
GENERAL	PINSRB	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
PINSRB	XMM,R32/M8,IMM8	SSE4_1	PINSRB XMM1,R32/M8,IMM8	Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8.
GENERAL	PINSRD	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
PINSRD	XMM,R/M32,IMM8	SSE4_1	PINSRD XMM1,R/M32,IMM8	Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8.
GENERAL	PINSRQ	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
PINSRQ	XMM,R/M64,IMM8	SSE4_1	PINSRQ XMM1,R/M64,IMM8	Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8.
GENERAL	VPINSRB	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
VPINSRB	XMM,XMM,R32/M8,IMM8	AVX	VPINSRB XMM1,XMM2,R32/M8,IMM8	Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
VPINSRB	XMM,XMM,R32/M8,IMM8	AVX512_BW	VPINSRB XMM1,XMM2,R32/M8,IMM8	Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
GENERAL	VPINSRD	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
VPINSRD	XMM,XMM,R/M32,IMM8	AVX	VPINSRD XMM1,XMM2,R/M32,IMM8	Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
VPINSRD	XMM,XMM,R32/M32,IMM8	AVX512_DQ	VPINSRD XMM1,XMM2,R32/M32,IMM8	Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
GENERAL	VPINSRQ	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ
VPINSRQ	XMM,XMM,R/M64,IMM8	AVX	VPINSRQ XMM1,XMM2,R/M64,IMM8	Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
VPINSRQ	XMM,XMM,R64/M64,IMM8	AVX512_DQ	VPINSRQ XMM1,XMM2,R64/M64,IMM8	Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
;--------------------------------------------------------
GENERAL	PINSRW	Insert Word	PINSRW
PINSRW	MM,R32/M16,IMM8	SSE	PINSRW MM,R32/M16,IMM8	Insert the low word from r32 or from m16 into mm at the word position specified by imm8.
PINSRW	XMM,R32/M16,IMM8	SSE2	PINSRW XMM,R32/M16,IMM8	Move the low word of r32 or from m16 into xmm at the word position specified by imm8.
GENERAL	VPINSRW	Insert Word	PINSRW
VPINSRW	XMM,XMM,R32/M16,IMM8	AVX	VPINSRW XMM1,XMM2,R32/M16,IMM8	Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8.
VPINSRW	XMM,XMM,R32/M16,IMM8	AVX512_BW	VPINSRW XMM1,XMM2,R32/M16,IMM8	Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8.
;--------------------------------------------------------
GENERAL	PMADDUBSW	Multiply and Add Packed Signed and Unsigned Bytes	PMADDUBSW
PMADDUBSW	MM,MM/M64	SSSE3	PMADDUBSW MM1,MM2/M64	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to mm1.
PMADDUBSW	XMM,XMM/M128	SSSE3	PMADDUBSW XMM1,XMM2/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
GENERAL	VPMADDUBSW	Multiply and Add Packed Signed and Unsigned Bytes	PMADDUBSW
VPMADDUBSW	XMM,XMM,XMM/M128	AVX	VPMADDUBSW XMM1,XMM2,XMM3/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
VPMADDUBSW	YMM,YMM,YMM/M256	AVX2	VPMADDUBSW YMM1,YMM2,YMM3/M256	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1.
VPMADDUBSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMADDUBSW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1 under writemask k1.
VPMADDUBSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMADDUBSW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1 under writemask k1.
VPMADDUBSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMADDUBSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMADDWD	Multiply and Add Packed Integers	PMADDWD
PMADDWD	MM,MM/M64	MMX	PMADDWD MM,MM/M64	Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm.
PMADDWD	XMM,XMM/M128	SSE2	PMADDWD XMM1,XMM2/M128	Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1.
GENERAL	VPMADDWD	Multiply and Add Packed Integers	PMADDWD
VPMADDWD	XMM,XMM,XMM/M128	AVX	VPMADDWD XMM1,XMM2,XMM3/M128	Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1.
VPMADDWD	YMM,YMM,YMM/M256	AVX2	VPMADDWD YMM1,YMM2,YMM3/M256	Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1.
VPMADDWD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMADDWD XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1 under writemask k1.
VPMADDWD	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMADDWD YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1 under writemask k1.
VPMADDWD	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMADDWD ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed word integers in zmm2 by the packed word integers in zmm3/m512, add adjacent doubleword results, and store in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMAXSW	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
PMAXSW	MM,MM/M64	SSE	PMAXSW MM1,MM2/M64	Compare signed word integers in mm2/m64 and mm1 and return maximum values.
PMAXSW	XMM,XMM/M128	SSE2	PMAXSW XMM1,XMM2/M128	Compare packed signed word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1.
GENERAL	PMAXSB	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
PMAXSB	XMM,XMM/M128	SSE4_1	PMAXSB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
GENERAL	PMAXSD	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
PMAXSD	XMM,XMM/M128	SSE4_1	PMAXSD XMM1,XMM2/M128	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
GENERAL	VPMAXSB	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
VPMAXSB	XMM,XMM,XMM/M128	AVX	VPMAXSB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXSB	YMM,YMM,YMM/M256	AVX2	VPMAXSB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMAXSB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMAXSB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMAXSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
GENERAL	VPMAXSW	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
VPMAXSW	XMM,XMM,XMM/M128	AVX	VPMAXSW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1.
VPMAXSW	YMM,YMM,YMM/M256	AVX2	VPMAXSW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm3/m256 and ymm2 and store packed maximum values in ymm1.
VPMAXSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMAXSW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMAXSW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMAXSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
GENERAL	VPMAXSD	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
VPMAXSD	XMM,XMM,XMM/M128	AVX	VPMAXSD XMM1,XMM2,XMM3/M128	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXSD	YMM,YMM,YMM/M256	AVX2	VPMAXSD YMM1,YMM2,YMM3/M256	Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXSD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPMAXSD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed signed dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 using writemask k1.
VPMAXSD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPMAXSD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed signed dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 using writemask k1.
VPMAXSD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPMAXSD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 using writemask k1.
GENERAL	VPMAXSQ	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ
VPMAXSQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMAXSQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed signed qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 using writemask k1.
VPMAXSQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMAXSQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed signed qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 using writemask k1.
VPMAXSQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMAXSQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PMAXUB	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW
PMAXUB	MM,MM/M64	SSE	PMAXUB MM1,MM2/M64	Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values.
PMAXUB	XMM,XMM/M128	SSE2	PMAXUB XMM1,XMM2/M128	Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
GENERAL	PMAXUW	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW
PMAXUW	XMM,XMM/M128	SSE4_1	PMAXUW XMM1,XMM2/M128	Compare packed unsigned word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1.
GENERAL	VPMAXUB	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW
VPMAXUB	XMM,XMM,XMM/M128	AVX	VPMAXUB XMM1,XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXUB	YMM,YMM,YMM/M256	AVX2	VPMAXUB YMM1,YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXUB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMAXUB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXUB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMAXUB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXUB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMAXUB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
GENERAL	VPMAXUW	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW
VPMAXUW	XMM,XMM,XMM/M128	AVX	VPMAXUW XMM1,XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1.
VPMAXUW	YMM,YMM,YMM/M256	AVX2	VPMAXUW YMM1,YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1.
VPMAXUW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMAXUW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXUW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMAXUW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMAXUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMAXUD	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ
PMAXUD	XMM,XMM/M128	SSE4_1	PMAXUD XMM1,XMM2/M128	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
GENERAL	VPMAXUD	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ
VPMAXUD	XMM,XMM,XMM/M128	AVX	VPMAXUD XMM1,XMM2,XMM3/M128	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXUD	YMM,YMM,YMM/M256	AVX2	VPMAXUD YMM1,YMM2,YMM3/M256	Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXUD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPMAXUD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 under writemask k1.
VPMAXUD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPMAXUD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 under writemask k1.
VPMAXUD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPMAXUD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 under writemask k1.
GENERAL	VPMAXUQ	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ
VPMAXUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMAXUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 under writemask k1.
VPMAXUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMAXUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 under writemask k1.
VPMAXUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMAXUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMINSW	Minimum of Packed Signed Integers	PMINSB_PMINSW
PMINSW	MM,MM/M64	SSE	PMINSW MM1,MM2/M64	Compare signed word integers in mm2/m64 and mm1 and return minimum values.
PMINSW	XMM,XMM/M128	SSE2	PMINSW XMM1,XMM2/M128	Compare packed signed word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1.
GENERAL	PMINSB	Minimum of Packed Signed Integers	PMINSB_PMINSW
PMINSB	XMM,XMM/M128	SSE4_1	PMINSB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
GENERAL	VPMINSB	Minimum of Packed Signed Integers	PMINSB_PMINSW
VPMINSB	XMM,XMM,XMM/M128	AVX	VPMINSB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINSB	YMM,YMM,YMM/M256	AVX2	VPMINSB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMINSB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMINSB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMINSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
GENERAL	VPMINSW	Minimum of Packed Signed Integers	PMINSB_PMINSW
VPMINSW	XMM,XMM,XMM/M128	AVX	VPMINSW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
VPMINSW	YMM,YMM,YMM/M256	AVX2	VPMINSW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1.
VPMINSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMINSW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMINSW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMINSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed word integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMINSD	Minimum of Packed Signed Integers	PMINSD_PMINSQ
PMINSD	XMM,XMM/M128	SSE4_1	PMINSD XMM1,XMM2/M128	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
GENERAL	VPMINSD	Minimum of Packed Signed Integers	PMINSD_PMINSQ
VPMINSD	XMM,XMM,XMM/M128	AVX	VPMINSD XMM1,XMM2,XMM3/M128	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINSD	YMM,YMM,YMM/M256	AVX2	VPMINSD YMM1,YMM2,YMM3/M256	Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed minimum values in ymm1.
VPMINSD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPMINSD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPMINSD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPMINSD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1.
GENERAL	VPMINSQ	Minimum of Packed Signed Integers	PMINSD_PMINSQ
VPMINSQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMINSQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed signed qword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMINSQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed signed qword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMINSQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMINUB	Minimum of Packed Unsigned Integers	PMINUB_PMINUW
PMINUB	MM,MM/M64	SSE	PMINUB MM1,MM2/M64	Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values.
PMINUB	XMM,XMM/M128	SSE2	PMINUB XMM1,XMM2/M128	Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
GENERAL	PMINUW	Minimum of Packed Unsigned Integers	PMINUB_PMINUW
PMINUW	XMM,XMM/M128	SSE4_1	PMINUW XMM1,XMM2/M128	Compare packed unsigned word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1.
GENERAL	VPMINUB	Minimum of Packed Unsigned Integers	PMINUB_PMINUW
VPMINUB	XMM,XMM,XMM/M128	AVX	VPMINUB XMM1,XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINUB	YMM,YMM,YMM/M256	AVX2	VPMINUB YMM1,YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINUB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMINUB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINUB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMINUB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINUB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMINUB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
GENERAL	VPMINUW	Minimum of Packed Unsigned Integers	PMINUB_PMINUW
VPMINUW	XMM,XMM,XMM/M128	AVX	VPMINUW XMM1,XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
VPMINUW	YMM,YMM,YMM/M256	AVX2	VPMINUW YMM1,YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1.
VPMINUW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMINUW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1 under writemask k1.
VPMINUW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMINUW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1 under writemask k1.
VPMINUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMINUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned word integers in zmm3/m512 and zmm2 and return packed minimum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMINUD	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ
PMINUD	XMM,XMM/M128	SSE4_1	PMINUD XMM1,XMM2/M128	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
GENERAL	VPMINUD	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ
VPMINUD	XMM,XMM,XMM/M128	AVX	VPMINUD XMM1,XMM2,XMM3/M128	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINUD	YMM,YMM,YMM/M256	AVX2	VPMINUD YMM1,YMM2,YMM3/M256	Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINUD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPMINUD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed minimum values in xmm1 under writemask k1.
VPMINUD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPMINUD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed minimum values in ymm1 under writemask k1.
VPMINUD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPMINUD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1.
GENERAL	VPMINUQ	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ
VPMINUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMINUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed minimum values in xmm1 under writemask k1.
VPMINUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMINUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed minimum values in ymm1 under writemask k1.
VPMINUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMINUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMOVMSKB	Move Byte Mask	PMOVMSKB
PMOVMSKB	REG,MM	SSE	PMOVMSKB REG,MM	Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed
PMOVMSKB	REG,XMM	SSE2	PMOVMSKB REG,XMM	Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed
GENERAL	VPMOVMSKB	Move Byte Mask	PMOVMSKB
VPMOVMSKB	REG,XMM	AVX	VPMOVMSKB REG,XMM1	Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros.
VPMOVMSKB	REG,YMM	AVX2	VPMOVMSKB REG,YMM1	Move a 32-bit mask of ymm1 to reg. The upper bits of r64 are filled with zeros.
;--------------------------------------------------------
GENERAL	PMOVSXBW	Packed Move with Sign Extend	PMOVSX
PMOVSXBW	XMM,XMM/M64	SSE4_1	PMOVSXBW XMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
GENERAL	PMOVSXBD	Packed Move with Sign Extend	PMOVSX
PMOVSXBD	XMM,XMM/M32	SSE4_1	PMOVSXBD XMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
GENERAL	PMOVSXBQ	Packed Move with Sign Extend	PMOVSX
PMOVSXBQ	XMM,XMM/M16	SSE4_1	PMOVSXBQ XMM1,XMM2/M16	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
GENERAL	PMOVSXWD	Packed Move with Sign Extend	PMOVSX
PMOVSXWD	XMM,XMM/M64	SSE4_1	PMOVSXWD XMM1,XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
GENERAL	PMOVSXWQ	Packed Move with Sign Extend	PMOVSX
PMOVSXWQ	XMM,XMM/M32	SSE4_1	PMOVSXWQ XMM1,XMM2/M32	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
GENERAL	PMOVSXDQ	Packed Move with Sign Extend	PMOVSX
PMOVSXDQ	XMM,XMM/M64	SSE4_1	PMOVSXDQ XMM1,XMM2/M64	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
GENERAL	VPMOVSXBW	Packed Move with Sign Extend	PMOVSX
VPMOVSXBW	XMM,XMM/M64	AVX	VPMOVSXBW XMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VPMOVSXBW	YMM,XMM/M128	AVX2	VPMOVSXBW YMM1,XMM2/M128	Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVSXBW	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_BW	VPMOVSXBW XMM1{K1}{Z},XMM2/M64	Sign extend 8 packed 8-bit integers in xmm2/m64 to 8 packed 16-bit integers in zmm1.
VPMOVSXBW	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VPMOVSXBW YMM1{K1}{Z},XMM2/M128	Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVSXBW	ZMM{K}{Z},YMM/M256	AVX512_BW	VPMOVSXBW ZMM1{K1}{Z},YMM2/M256	Sign extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1.
GENERAL	VPMOVSXBD	Packed Move with Sign Extend	PMOVSX
VPMOVSXBD	XMM,XMM/M32	AVX	VPMOVSXBD XMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VPMOVSXBD	YMM,XMM/M64	AVX2	VPMOVSXBD YMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VPMOVSXBD	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVSXBD XMM1{K1}{Z},XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1.
VPMOVSXBD	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVSXBD YMM1{K1}{Z},XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1.
VPMOVSXBD	ZMM{K}{Z},XMM/M128	AVX512_F	VPMOVSXBD ZMM1{K1}{Z},XMM2/M128	Sign extend 16 packed 8-bit integers in the low 16 bytes of xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVSXBQ	Packed Move with Sign Extend	PMOVSX
VPMOVSXBQ	XMM,XMM/M16	AVX	VPMOVSXBQ XMM1,XMM2/M16	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VPMOVSXBQ	YMM,XMM/M32	AVX2	VPMOVSXBQ YMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VPMOVSXBQ	XMM{K}{Z},XMM/M16	AVX512_VL,AVX512_F	VPMOVSXBQ XMM1{K1}{Z},XMM2/M16	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1.
VPMOVSXBQ	YMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVSXBQ YMM1{K1}{Z},XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1.
VPMOVSXBQ	ZMM{K}{Z},XMM/M64	AVX512_F	VPMOVSXBQ ZMM1{K1}{Z},XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVSXWD	Packed Move with Sign Extend	PMOVSX
VPMOVSXWD	XMM,XMM/M64	AVX	VPMOVSXWD XMM1,XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VPMOVSXWD	YMM,XMM/M128	AVX2	VPMOVSXWD YMM1,XMM2/M128	Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1.
VPMOVSXWD	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVSXWD XMM1{K1}{Z},XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of ymm2/mem to 4 packed 32-bit integers in xmm1 subject to writemask k1.
VPMOVSXWD	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPMOVSXWD YMM1{K1}{Z},XMM2/M128	Sign extend 8 packed 16-bit integers in the low 16 bytes of ymm2/m128 to 8 packed 32-bit integers in ymm1 subject to writemask k1.
VPMOVSXWD	ZMM{K}{Z},YMM/M256	AVX512_F	VPMOVSXWD ZMM1{K1}{Z},YMM2/M256	Sign extend 16 packed 16-bit integers in the low 32 bytes of ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVSXWQ	Packed Move with Sign Extend	PMOVSX
VPMOVSXWQ	XMM,XMM/M32	AVX	VPMOVSXWQ XMM1,XMM2/M32	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VPMOVSXWQ	YMM,XMM/M64	AVX2	VPMOVSXWQ YMM1,XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1.
VPMOVSXWQ	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVSXWQ XMM1{K1}{Z},XMM2/M32	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1.
VPMOVSXWQ	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVSXWQ YMM1{K1}{Z},XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1.
VPMOVSXWQ	ZMM{K}{Z},XMM/M128	AVX512_F	VPMOVSXWQ ZMM1{K1}{Z},XMM2/M128	Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVSXDQ	Packed Move with Sign Extend	PMOVSX
VPMOVSXDQ	XMM,XMM/M64	AVX	VPMOVSXDQ XMM1,XMM2/M64	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VPMOVSXDQ	YMM,XMM/M128	AVX2	VPMOVSXDQ YMM1,XMM2/M128	Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1.
VPMOVSXDQ	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVSXDQ XMM1{K1}{Z},XMM2/M64	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1.
VPMOVSXDQ	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPMOVSXDQ YMM1{K1}{Z},XMM2/M128	Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1.
VPMOVSXDQ	ZMM{K}{Z},YMM/M256	AVX512_F	VPMOVSXDQ ZMM1{K1}{Z},YMM2/M256	Sign extend 8 packed 32-bit integers in the low 32 bytes of ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PMOVZXBW	Packed Move with Zero Extend	PMOVZX
PMOVZXBW	XMM,XMM/M64	SSE4_1	PMOVZXBW XMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
GENERAL	PMOVZXBD	Packed Move with Zero Extend	PMOVZX
PMOVZXBD	XMM,XMM/M32	SSE4_1	PMOVZXBD XMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
GENERAL	PMOVZXBQ	Packed Move with Zero Extend	PMOVZX
PMOVZXBQ	XMM,XMM/M16	SSE4_1	PMOVZXBQ XMM1,XMM2/M16	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
GENERAL	PMOVZXWD	Packed Move with Zero Extend	PMOVZX
PMOVZXWD	XMM,XMM/M64	SSE4_1	PMOVZXWD XMM1,XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
GENERAL	PMOVZXWQ	Packed Move with Zero Extend	PMOVZX
PMOVZXWQ	XMM,XMM/M32	SSE4_1	PMOVZXWQ XMM1,XMM2/M32	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
GENERAL	PMOVZXDQ	Packed Move with Zero Extend	PMOVZX
PMOVZXDQ	XMM,XMM/M64	SSE4_1	PMOVZXDQ XMM1,XMM2/M64	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
GENERAL	VPMOVZXBW	Packed Move with Zero Extend	PMOVZX
VPMOVZXBW	XMM,XMM/M64	AVX	VPMOVZXBW XMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VPMOVZXBW	YMM,XMM/M128	AVX2	VPMOVZXBW YMM1,XMM2/M128	Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVZXBW	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_BW	VPMOVZXBW XMM1{K1}{Z},XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VPMOVZXBW	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_BW	VPMOVZXBW YMM1{K1}{Z},XMM2/M128	Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVZXBW	ZMM{K}{Z},YMM/M256	AVX512_BW	VPMOVZXBW ZMM1{K1}{Z},YMM2/M256	Zero extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1.
GENERAL	VPMOVZXBD	Packed Move with Zero Extend	PMOVZX
VPMOVZXBD	XMM,XMM/M32	AVX	VPMOVZXBD XMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VPMOVZXBD	YMM,XMM/M64	AVX2	VPMOVZXBD YMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VPMOVZXBD	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVZXBD XMM1{K1}{Z},XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1.
VPMOVZXBD	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVZXBD YMM1{K1}{Z},XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1.
VPMOVZXBD	ZMM{K}{Z},XMM/M128	AVX512_F	VPMOVZXBD ZMM1{K1}{Z},XMM2/M128	Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVZXBQ	Packed Move with Zero Extend	PMOVZX
VPMOVZXBQ	XMM,XMM/M16	AVX	VPMOVZXBQ XMM1,XMM2/M16	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VPMOVZXBQ	YMM,XMM/M32	AVX2	VPMOVZXBQ YMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VPMOVZXBQ	XMM{K}{Z},XMM/M16	AVX512_VL,AVX512_F	VPMOVZXBQ XMM1{K1}{Z},XMM2/M16	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1.
VPMOVZXBQ	YMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVZXBQ YMM1{K1}{Z},XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1.
VPMOVZXBQ	ZMM{K}{Z},XMM/M64	AVX512_F	VPMOVZXBQ ZMM1{K1}{Z},XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVZXWD	Packed Move with Zero Extend	PMOVZX
VPMOVZXWD	XMM,XMM/M64	AVX	VPMOVZXWD XMM1,XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VPMOVZXWD	YMM,XMM/M128	AVX2	VPMOVZXWD YMM1,XMM2/M128	Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1.
VPMOVZXWD	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVZXWD XMM1{K1}{Z},XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1 subject to writemask k1.
VPMOVZXWD	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPMOVZXWD YMM1{K1}{Z},XMM2/M128	Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 32-bit integers in zmm1 subject to writemask k1.
VPMOVZXWD	ZMM{K}{Z},YMM/M256	AVX512_F	VPMOVZXWD ZMM1{K1}{Z},YMM2/M256	Zero extend 16 packed 16-bit integers in ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVZXWQ	Packed Move with Zero Extend	PMOVZX
VPMOVZXWQ	XMM,XMM/M32	AVX	VPMOVZXWQ XMM1,XMM2/M32	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VPMOVZXWQ	YMM,XMM/M64	AVX2	VPMOVZXWQ YMM1,XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1.
VPMOVZXWQ	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPMOVZXWQ XMM1{K1}{Z},XMM2/M32	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1.
VPMOVZXWQ	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVZXWQ YMM1{K1}{Z},XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1.
VPMOVZXWQ	ZMM{K}{Z},XMM/M128	AVX512_F	VPMOVZXWQ ZMM1{K1}{Z},XMM2/M128	Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1.
GENERAL	VPMOVZXDQ	Packed Move with Zero Extend	PMOVZX
VPMOVZXDQ	XMM,XMM/M64	AVX	VPMOVZXDQ XMM1,XMM2/M64	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VPMOVZXDQ	YMM,XMM/M128	AVX2	VPMOVZXDQ YMM1,XMM2/M128	Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in ymm1.
VPMOVZXDQ	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPMOVZXDQ XMM1{K1}{Z},XMM2/M64	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1.
VPMOVZXDQ	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPMOVZXDQ YMM1{K1}{Z},XMM2/M128	Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1.
VPMOVZXDQ	ZMM{K}{Z},YMM/M256	AVX512_F	VPMOVZXDQ ZMM1{K1}{Z},YMM2/M256	Zero extend 8 packed 32-bit integers in ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PMULDQ	Multiply Packed Doubleword Integers	PMULDQ
PMULDQ	XMM,XMM/M128	SSE4_1	PMULDQ XMM1,XMM2/M128	Multiply packed signed doubleword integers in xmm1 by packed signed doubleword integers in xmm2/m128, and store the quadword results in xmm1.
GENERAL	VPMULDQ	Multiply Packed Doubleword Integers	PMULDQ
VPMULDQ	XMM,XMM,XMM/M128	AVX	VPMULDQ XMM1,XMM2,XMM3/M128	Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VPMULDQ	YMM,YMM,YMM/M256	AVX2	VPMULDQ YMM1,YMM2,YMM3/M256	Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1.
VPMULDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMULDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 using writemask k1.
VPMULDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMULDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 using writemask k1.
VPMULDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMULDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply packed signed doubleword integers in zmm2 by packed signed doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PMULHRSW	Packed Multiply High with Round and Scale	PMULHRSW
PMULHRSW	MM,MM/M64	SSSE3	PMULHRSW MM1,MM2/M64	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to mm1.
PMULHRSW	XMM,XMM/M128	SSSE3	PMULHRSW XMM1,XMM2/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
GENERAL	VPMULHRSW	Packed Multiply High with Round and Scale	PMULHRSW
VPMULHRSW	XMM,XMM,XMM/M128	AVX	VPMULHRSW XMM1,XMM2,XMM3/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
VPMULHRSW	YMM,YMM,YMM/M256	AVX2	VPMULHRSW YMM1,YMM2,YMM3/M256	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1.
VPMULHRSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMULHRSW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1 under writemask k1.
VPMULHRSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMULHRSW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1 under writemask k1.
VPMULHRSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMULHRSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMULHUW	Multiply Packed Unsigned Integers and Store High Result	PMULHUW
PMULHUW	MM,MM/M64	SSE	PMULHUW MM1,MM2/M64	Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
PMULHUW	XMM,XMM/M128	SSE2	PMULHUW XMM1,XMM2/M128	Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
GENERAL	VPMULHUW	Multiply Packed Unsigned Integers and Store High Result	PMULHUW
VPMULHUW	XMM,XMM,XMM/M128	AVX	VPMULHUW XMM1,XMM2,XMM3/M128	Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VPMULHUW	YMM,YMM,YMM/M256	AVX2	VPMULHUW YMM1,YMM2,YMM3/M256	Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
VPMULHUW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMULHUW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1.
VPMULHUW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMULHUW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1.
VPMULHUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMULHUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed unsigned word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMULHW	Multiply Packed Signed Integers and Store High Result	PMULHW
PMULHW	MM,MM/M64	MMX	PMULHW MM,MM/M64	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
PMULHW	XMM,XMM/M128	SSE2	PMULHW XMM1,XMM2/M128	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
GENERAL	VPMULHW	Multiply Packed Signed Integers and Store High Result	PMULHW
VPMULHW	XMM,XMM,XMM/M128	AVX	VPMULHW XMM1,XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VPMULHW	YMM,YMM,YMM/M256	AVX2	VPMULHW YMM1,YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
VPMULHW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMULHW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1.
VPMULHW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMULHW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1.
VPMULHW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMULHW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMULLD	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ
PMULLD	XMM,XMM/M128	SSE4_1	PMULLD XMM1,XMM2/M128	Multiply the packed dword signed integers in xmm1 and xmm2/m128 and store the low 32 bits of each product in xmm1.
GENERAL	VPMULLD	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ
VPMULLD	XMM,XMM,XMM/M128	AVX	VPMULLD XMM1,XMM2,XMM3/M128	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VPMULLD	YMM,YMM,YMM/M256	AVX2	VPMULLD YMM1,YMM2,YMM3/M256	Multiply the packed dword signed integers in ymm2 and ymm3/m256 and store the low 32 bits of each product in ymm1.
VPMULLD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPMULLD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply the packed dword signed integers in xmm2 and xmm3/m128/m32bcst and store the low 32 bits of each product in xmm1 under writemask k1.
VPMULLD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPMULLD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply the packed dword signed integers in ymm2 and ymm3/m256/m32bcst and store the low 32 bits of each product in ymm1 under writemask k1.
VPMULLD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPMULLD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply the packed dword signed integers in zmm2 and zmm3/m512/m32bcst and store the low 32 bits of each product in zmm1 under writemask k1.
GENERAL	VPMULLQ	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ
VPMULLQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VPMULLQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply the packed qword signed integers in xmm2 and xmm3/m128/m64bcst and store the low 64 bits of each product in xmm1 under writemask k1.
VPMULLQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VPMULLQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply the packed qword signed integers in ymm2 and ymm3/m256/m64bcst and store the low 64 bits of each product in ymm1 under writemask k1.
VPMULLQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_DQ	VPMULLQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply the packed qword signed integers in zmm2 and zmm3/m512/m64bcst and store the low 64 bits of each product in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMULLW	Multiply Packed Signed Integers and Store Low Result	PMULLW
PMULLW	MM,MM/M64	MMX	PMULLW MM,MM/M64	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1.
PMULLW	XMM,XMM/M128	SSE2	PMULLW XMM1,XMM2/M128	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1.
GENERAL	VPMULLW	Multiply Packed Signed Integers and Store Low Result	PMULLW
VPMULLW	XMM,XMM,XMM/M128	AVX	VPMULLW XMM1,XMM2,XMM3/M128	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VPMULLW	YMM,YMM,YMM/M256	AVX2	VPMULLW YMM1,YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1.
VPMULLW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPMULLW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the low 16 bits of the results in xmm1 under writemask k1.
VPMULLW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPMULLW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1 under writemask k1.
VPMULLW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPMULLW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the low 16 bits of the results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	PMULUDQ	Multiply Packed Unsigned Doubleword Integers	PMULUDQ
PMULUDQ	MM,MM/M64	SSE2	PMULUDQ MM1,MM2/M64	Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1.
PMULUDQ	XMM,XMM/M128	SSE2	PMULUDQ XMM1,XMM2/M128	Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1.
GENERAL	VPMULUDQ	Multiply Packed Unsigned Doubleword Integers	PMULUDQ
VPMULUDQ	XMM,XMM,XMM/M128	AVX	VPMULUDQ XMM1,XMM2,XMM3/M128	Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VPMULUDQ	YMM,YMM,YMM/M256	AVX2	VPMULUDQ YMM1,YMM2,YMM3/M256	Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1.
VPMULUDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPMULUDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 under writemask k1.
VPMULUDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPMULUDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 under writemask k1.
VPMULUDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPMULUDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply packed unsigned doubleword integers in zmm2 by packed unsigned doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	POP	Pop a Value from the Stack	POP
POP	R/M16	8086	POP R/M16	Pop top of stack into m16; increment stack pointer.
POP	R/M32	386	POP R/M32	Pop top of stack into m32; increment stack pointer.
POP	R/M64	X64	POP R/M64	Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size.
POP	R16	8086	POP R16	Pop top of stack into r16; increment stack pointer.
POP	R32	386	POP R32	Pop top of stack into r32; increment stack pointer.
POP	R64	X64	POP R64	Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size.
POP	DS	8086	POP DS	Pop top of stack into DS; increment stack pointer.
POP	ES	8086	POP ES	Pop top of stack into ES; increment stack pointer.
POP	SS	8086	POP SS	Pop top of stack into SS; increment stack pointer.
POP	FS	8086	POP FS	Pop top of stack into FS; increment stack pointer by 16 bits.
POP	FS	8086	POP FS	Pop top of stack into FS; increment stack pointer by 32 bits.
POP	FS	8086	POP FS	Pop top of stack into FS; increment stack pointer by 64 bits.
POP	GS	8086	POP GS	Pop top of stack into GS; increment stack pointer by 16 bits.
POP	GS	8086	POP GS	Pop top of stack into GS; increment stack pointer by 32 bits.
POP	GS	8086	POP GS	Pop top of stack into GS; increment stack pointer by 64 bits.
;--------------------------------------------------------
GENERAL	POPA	Pop All General-Purpose Registers	POPA_POPAD
POPA		8086	POPA	Pop DI, SI, BP, BX, DX, CX, and AX.
GENERAL	POPAD	Pop All General-Purpose Registers	POPA_POPAD
POPAD		8086	POPAD	Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX.
;--------------------------------------------------------
GENERAL	POPCNT	Return the Count of Number of Bits Set to 1	POPCNT
POPCNT	R16,R/M16	8086	POPCNT R16,R/M16	POPCNT on r/m16
POPCNT	R32,R/M32	386	POPCNT R32,R/M32	POPCNT on r/m32
POPCNT	R64,R/M64	X64	POPCNT R64,R/M64	POPCNT on r/m64
;--------------------------------------------------------
GENERAL	POPF	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ
POPF		8086	POPF	Pop top of stack into lower 16 bits of EFLAGS.
GENERAL	POPFD	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ
POPFD		8086	POPFD	Pop top of stack into EFLAGS.
GENERAL	POPFQ	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ
POPFQ		8086	POPFQ	Pop top of stack and zero-extend into RFLAGS.
;--------------------------------------------------------
GENERAL	POR	Bitwise Logical OR	POR
POR	MM,MM/M64	MMX	POR MM,MM/M64	Bitwise OR of mm/m64 and mm.
POR	XMM,XMM/M128	SSE2	POR XMM1,XMM2/M128	Bitwise OR of xmm2/m128 and xmm1.
GENERAL	VPOR	Bitwise Logical OR	POR
VPOR	XMM,XMM,XMM/M128	AVX	VPOR XMM1,XMM2,XMM3/M128	Bitwise OR of xmm2/m128 and xmm3.
VPOR	YMM,YMM,YMM/M256	AVX2	VPOR YMM1,YMM2,YMM3/M256	Bitwise OR of ymm2/m256 and ymm3.
GENERAL	VPORD	Bitwise Logical OR	POR
VPORD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPORD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise OR of packed doubleword integers in xmm2 and xmm3/m128/m32bcst using writemask k1.
VPORD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPORD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise OR of packed doubleword integers in ymm2 and ymm3/m256/m32bcst using writemask k1.
VPORD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPORD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise OR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1.
GENERAL	VPORQ	Bitwise Logical OR	POR
VPORQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPORQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise OR of packed quadword integers in xmm2 and xmm3/m128/m64bcst using writemask k1.
VPORQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPORQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise OR of packed quadword integers in ymm2 and ymm3/m256/m64bcst using writemask k1.
VPORQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPORQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise OR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1.
;--------------------------------------------------------
GENERAL	PREFETCHT0	Prefetch Data Into Caches	PREFETCHh
PREFETCHT0	M8	8086	PREFETCHT0 M8	Move data from m8 closer to the processor using T0 hint.
GENERAL	PREFETCHT1	Prefetch Data Into Caches	PREFETCHh
PREFETCHT1	M8	8086	PREFETCHT1 M8	Move data from m8 closer to the processor using T1 hint.
GENERAL	PREFETCHT2	Prefetch Data Into Caches	PREFETCHh
PREFETCHT2	M8	8086	PREFETCHT2 M8	Move data from m8 closer to the processor using T2 hint.
GENERAL	PREFETCHNTA	Prefetch Data Into Caches	PREFETCHh
PREFETCHNTA	M8	8086	PREFETCHNTA M8	Move data from m8 closer to the processor using NTA hint.
;--------------------------------------------------------
GENERAL	PREFETCHW	Prefetch Data into Caches in Anticipation of a Write	PREFETCHW
PREFETCHW	M8	PRFCHW	PREFETCHW M8	Move data from m8 closer to the processor in anticipation of a write.
;--------------------------------------------------------
GENERAL	PREFETCHWT1	Prefetch Vector Data Into Caches with Intent to Write and T1 Hint	PREFETCHWT1
PREFETCHWT1	M8	PREFETCHWT1	PREFETCHWT1 M8	Move data from m8 closer to the processor using T1 hint with intent to write.
;--------------------------------------------------------
GENERAL	PSADBW	Compute Sum of Absolute Differences	PSADBW
PSADBW	MM,MM/M64	SSE	PSADBW MM1,MM2/M64	Computes the absolute differences of the packed unsigned byte integers from mm2 /m64 and mm1; differences are then summed to produce an unsigned word integer result.
PSADBW	XMM,XMM/M128	SSE2	PSADBW XMM1,XMM2/M128	Computes the absolute differences of the packed unsigned byte integers from xmm2 /m128 and xmm1; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
GENERAL	VPSADBW	Compute Sum of Absolute Differences	PSADBW
VPSADBW	XMM,XMM,XMM/M128	AVX	VPSADBW XMM1,XMM2,XMM3/M128	Computes the absolute differences of the packed unsigned byte integers from xmm3 /m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
VPSADBW	YMM,YMM,YMM/M256	AVX2	VPSADBW YMM1,YMM2,YMM3/M256	Computes the absolute differences of the packed unsigned byte integers from ymm3 /m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
VPSADBW	XMM,XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSADBW XMM1,XMM2,XMM3/M128	Computes the absolute differences of the packed unsigned byte integers from xmm3 /m128 and xmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
VPSADBW	YMM,YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSADBW YMM1,YMM2,YMM3/M256	Computes the absolute differences of the packed unsigned byte integers from ymm3 /m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
VPSADBW	ZMM,ZMM,ZMM/M512	AVX512_BW	VPSADBW ZMM1,ZMM2,ZMM3/M512	Computes the absolute differences of the packed unsigned byte integers from zmm3 /m512 and zmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
;--------------------------------------------------------
GENERAL	PSHUFB	Packed Shuffle Bytes	PSHUFB
PSHUFB	MM,MM/M64	SSSE3	PSHUFB MM1,MM2/M64	Shuffle bytes in mm1 according to contents of mm2/m64.
PSHUFB	XMM,XMM/M128	SSSE3	PSHUFB XMM1,XMM2/M128	Shuffle bytes in xmm1 according to contents of xmm2/m128.
GENERAL	VPSHUFB	Packed Shuffle Bytes	PSHUFB
VPSHUFB	XMM,XMM,XMM/M128	AVX	VPSHUFB XMM1,XMM2,XMM3/M128	Shuffle bytes in xmm2 according to contents of xmm3/m128.
VPSHUFB	YMM,YMM,YMM/M256	AVX2	VPSHUFB YMM1,YMM2,YMM3/M256	Shuffle bytes in ymm2 according to contents of ymm3/m256.
VPSHUFB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSHUFB XMM1{K1}{Z},XMM2,XMM3/M128	Shuffle bytes in xmm2 according to contents of xmm3/m128 under write mask k1.
VPSHUFB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSHUFB YMM1{K1}{Z},YMM2,YMM3/M256	Shuffle bytes in ymm2 according to contents of ymm3/m256 under write mask k1.
VPSHUFB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSHUFB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shuffle bytes in zmm2 according to contents of zmm3/m512 under write mask k1.
;--------------------------------------------------------
GENERAL	PSHUFD	Shuffle Packed Doublewords	PSHUFD
PSHUFD	XMM,XMM/M128,IMM8	SSE2	PSHUFD XMM1,XMM2/M128,IMM8	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
GENERAL	VPSHUFD	Shuffle Packed Doublewords	PSHUFD
VPSHUFD	XMM,XMM/M128,IMM8	AVX	VPSHUFD XMM1,XMM2/M128,IMM8	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFD	YMM,YMM/M256,IMM8	AVX2	VPSHUFD YMM1,YMM2/M256,IMM8	Shuffle the doublewords in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSHUFD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Shuffle the doublewords in xmm2/m128/m32bcst based on the encoding in imm8 and store the result in xmm1 using writemask k1.
VPSHUFD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSHUFD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Shuffle the doublewords in ymm2/m256/m32bcst based on the encoding in imm8 and store the result in ymm1 using writemask k1.
VPSHUFD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPSHUFD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Shuffle the doublewords in zmm2/m512/m32bcst based on the encoding in imm8 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PSHUFHW	Shuffle Packed High Words	PSHUFHW
PSHUFHW	XMM,XMM/M128,IMM8	SSE2	PSHUFHW XMM1,XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
GENERAL	VPSHUFHW	Shuffle Packed High Words	PSHUFHW
VPSHUFHW	XMM,XMM/M128,IMM8	AVX	VPSHUFHW XMM1,XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFHW	YMM,YMM/M256,IMM8	AVX2	VPSHUFHW YMM1,YMM2/M256,IMM8	Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFHW	XMM{K}{Z},XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSHUFHW XMM1{K1}{Z},XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1.
VPSHUFHW	YMM{K}{Z},YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSHUFHW YMM1{K1}{Z},YMM2/M256,IMM8	Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1.
VPSHUFHW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512_BW	VPSHUFHW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shuffle the high words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1.
;--------------------------------------------------------
GENERAL	PSHUFLW	Shuffle Packed Low Words	PSHUFLW
PSHUFLW	XMM,XMM/M128,IMM8	SSE2	PSHUFLW XMM1,XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
GENERAL	VPSHUFLW	Shuffle Packed Low Words	PSHUFLW
VPSHUFLW	XMM,XMM/M128,IMM8	AVX	VPSHUFLW XMM1,XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFLW	YMM,YMM/M256,IMM8	AVX2	VPSHUFLW YMM1,YMM2/M256,IMM8	Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFLW	XMM{K}{Z},XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSHUFLW XMM1{K1}{Z},XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1.
VPSHUFLW	YMM{K}{Z},YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSHUFLW YMM1{K1}{Z},YMM2/M256,IMM8	Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1.
VPSHUFLW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512_BW	VPSHUFLW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shuffle the low words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1.
;--------------------------------------------------------
GENERAL	PSHUFW	Shuffle Packed Words	PSHUFW
PSHUFW	MM,MM/M64,IMM8		PSHUFW MM1,MM2/M64,IMM8	Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1.
;--------------------------------------------------------
GENERAL	PSIGNB	Packed SIGN	PSIGNB_PSIGNW_PSIGND
PSIGNB	MM,MM/M64	SSSE3	PSIGNB MM1,MM2/M64	Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64.
PSIGNB	XMM,XMM/M128	SSSE3	PSIGNB XMM1,XMM2/M128	Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128.
GENERAL	PSIGNW	Packed SIGN	PSIGNB_PSIGNW_PSIGND
PSIGNW	MM,MM/M64	SSSE3	PSIGNW MM1,MM2/M64	Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128.
PSIGNW	XMM,XMM/M128	SSSE3	PSIGNW XMM1,XMM2/M128	Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128.
GENERAL	PSIGND	Packed SIGN	PSIGNB_PSIGNW_PSIGND
PSIGND	MM,MM/M64	SSSE3	PSIGND MM1,MM2/M64	Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128.
PSIGND	XMM,XMM/M128	SSSE3	PSIGND XMM1,XMM2/M128	Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128.
GENERAL	VPSIGNB	Packed SIGN	PSIGNB_PSIGNW_PSIGND
VPSIGNB	XMM,XMM,XMM/M128	AVX	VPSIGNB XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGNB	YMM,YMM,YMM/M256	AVX2	VPSIGNB YMM1,YMM2,YMM3/M256	Negate packed byte integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
GENERAL	VPSIGNW	Packed SIGN	PSIGNB_PSIGNW_PSIGND
VPSIGNW	XMM,XMM,XMM/M128	AVX	VPSIGNW XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGNW	YMM,YMM,YMM/M256	AVX2	VPSIGNW YMM1,YMM2,YMM3/M256	Negate packed 16-bit integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
GENERAL	VPSIGND	Packed SIGN	PSIGNB_PSIGNW_PSIGND
VPSIGND	XMM,XMM,XMM/M128	AVX	VPSIGND XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGND	YMM,YMM,YMM/M256	AVX2	VPSIGND YMM1,YMM2,YMM3/M256	Negate packed doubleword integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
;--------------------------------------------------------
GENERAL	PSLLDQ	Shift Double Quadword Left Logical	PSLLDQ
PSLLDQ	XMM,IMM8	SSE2	PSLLDQ XMM1,IMM8	Shift xmm1 left by imm8 bytes while shifting in 0s.
GENERAL	VPSLLDQ	Shift Double Quadword Left Logical	PSLLDQ
VPSLLDQ	XMM,XMM,IMM8	AVX	VPSLLDQ XMM1,XMM2,IMM8	Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1.
VPSLLDQ	YMM,YMM,IMM8	AVX2	VPSLLDQ YMM1,YMM2,IMM8	Shift ymm2 left by imm8 bytes while shifting in 0s and store result in ymm1.
VPSLLDQ	XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSLLDQ XMM1,XMM2/M128,IMM8	Shift xmm2/m128 left by imm8 bytes while shifting in 0s and store result in xmm1.
VPSLLDQ	YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSLLDQ YMM1,YMM2/M256,IMM8	Shift ymm2/m256 left by imm8 bytes while shifting in 0s and store result in ymm1.
VPSLLDQ	ZMM,ZMM/M512,IMM8	AVX512_BW	VPSLLDQ ZMM1,ZMM2/M512,IMM8	Shift zmm2/m512 left by imm8 bytes while shifting in 0s and store result in zmm1.
;--------------------------------------------------------
GENERAL	PSLLW	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
PSLLW	MM,MM/M64	MMX	PSLLW MM,MM/M64	Shift words in mm left mm/m64 while shifting in 0s.
PSLLW	XMM,XMM/M128	SSE2	PSLLW XMM1,XMM2/M128	Shift words in xmm1 left by xmm2/m128 while shifting in 0s.
PSLLW	MM,IMM8	MMX	PSLLW MM1,IMM8	Shift words in mm left by imm8 while shifting in 0s.
PSLLW	XMM,IMM8	SSE2	PSLLW XMM1,IMM8	Shift words in xmm1 left by imm8 while shifting in 0s.
GENERAL	PSLLD	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
PSLLD	MM,MM/M64	MMX	PSLLD MM,MM/M64	Shift doublewords in mm left by mm/m64 while shifting in 0s.
PSLLD	XMM,XMM/M128	SSE2	PSLLD XMM1,XMM2/M128	Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s.
PSLLD	MM,IMM8	MMX	PSLLD MM,IMM8	Shift doublewords in mm left by imm8 while shifting in 0s.
PSLLD	XMM,IMM8	SSE2	PSLLD XMM1,IMM8	Shift doublewords in xmm1 left by imm8 while shifting in 0s.
GENERAL	PSLLQ	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
PSLLQ	MM,MM/M64	MMX	PSLLQ MM,MM/M64	Shift quadword in mm left by mm/m64 while shifting in 0s.
PSLLQ	XMM,XMM/M128	SSE2	PSLLQ XMM1,XMM2/M128	Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s.
PSLLQ	MM,IMM8	MMX	PSLLQ MM,IMM8	Shift quadword in mm left by imm8 while shifting in 0s.
PSLLQ	XMM,IMM8	SSE2	PSLLQ XMM1,IMM8	Shift quadwords in xmm1 left by imm8 while shifting in 0s.
GENERAL	VPSLLW	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
VPSLLW	XMM,XMM,XMM/M128	AVX	VPSLLW XMM1,XMM2,XMM3/M128	Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLW	XMM,XMM,IMM8	AVX	VPSLLW XMM1,XMM2,IMM8	Shift words in xmm2 left by imm8 while shifting in 0s.
VPSLLW	YMM,YMM,XMM/M128	AVX2	VPSLLW YMM1,YMM2,XMM3/M128	Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLW	YMM,YMM,IMM8	AVX2	VPSLLW YMM1,YMM2,IMM8	Shift words in ymm2 left by imm8 while shifting in 0s.
VPSLLW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSLLW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLW	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_BW	VPSLLW YMM1{K1}{Z},YMM2,XMM3/M128	Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLW	ZMM{K}{Z},ZMM,XMM/M128	AVX512_BW	VPSLLW ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift words in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLW	XMM{K}{Z},XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSLLW XMM1{K1}{Z},XMM2/M128,IMM8	Shift words in xmm2/m128 left by imm8 while shifting in 0s using writemask k1.
VPSLLW	YMM{K}{Z},YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSLLW YMM1{K1}{Z},YMM2/M256,IMM8	Shift words in ymm2/m256 left by imm8 while shifting in 0s using writemask k1.
VPSLLW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512_BW	VPSLLW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shift words in zmm2/m512 left by imm8 while shifting in 0 using writemask k1.
GENERAL	VPSLLD	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
VPSLLD	XMM,XMM,XMM/M128	AVX	VPSLLD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLD	XMM,XMM,IMM8	AVX	VPSLLD XMM1,XMM2,IMM8	Shift doublewords in xmm2 left by imm8 while shifting in 0s.
VPSLLD	YMM,YMM,XMM/M128	AVX2	VPSLLD YMM1,YMM2,XMM3/M128	Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLD	YMM,YMM,IMM8	AVX2	VPSLLD YMM1,YMM2,IMM8	Shift doublewords in ymm2 left by imm8 while shifting in 0s.
VPSLLD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSLLD XMM1{K1}{Z},XMM2,XMM3/M128	Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1.
VPSLLD	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSLLD YMM1{K1}{Z},YMM2,XMM3/M128	Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1.
VPSLLD	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSLLD ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift doublewords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1.
VPSLLD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSLLD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Shift doublewords in xmm2/m128/m32bcst left by imm8 while shifting in 0s using writemask k1.
VPSLLD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSLLD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Shift doublewords in ymm2/m256/m32bcst left by imm8 while shifting in 0s using writemask k1.
VPSLLD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPSLLD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Shift doublewords in zmm2/m512/m32bcst left by imm8 while shifting in 0s using writemask k1.
GENERAL	VPSLLQ	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ
VPSLLQ	XMM,XMM,XMM/M128	AVX	VPSLLQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLQ	XMM,XMM,IMM8	AVX	VPSLLQ XMM1,XMM2,IMM8	Shift quadwords in xmm2 left by imm8 while shifting in 0s.
VPSLLQ	YMM,YMM,XMM/M128	AVX2	VPSLLQ YMM1,YMM2,XMM3/M128	Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLQ	YMM,YMM,IMM8	AVX2	VPSLLQ YMM1,YMM2,IMM8	Shift quadwords in ymm2 left by imm8 while shifting in 0s.
VPSLLQ	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSLLQ XMM1{K1}{Z},XMM2,XMM3/M128	Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLQ	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSLLQ YMM1{K1}{Z},YMM2,XMM3/M128	Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLQ	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSLLQ ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift quadwords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSLLQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSLLQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Shift quadwords in xmm2/m128/m64bcst left by imm8 while shifting in 0s using writemask k1.
VPSLLQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSLLQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Shift quadwords in ymm2/m256/m64bcst left by imm8 while shifting in 0s using writemask k1.
VPSLLQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPSLLQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Shift quadwords in zmm2/m512/m64bcst left by imm8 while shifting in 0s using writemask k1.
;--------------------------------------------------------
GENERAL	PSRAW	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ
PSRAW	MM,MM/M64	MMX	PSRAW MM,MM/M64	Shift words in mm right by mm/m64 while shifting in sign bits.
PSRAW	XMM,XMM/M128	SSE2	PSRAW XMM1,XMM2/M128	Shift words in xmm1 right by xmm2/m128 while shifting in sign bits.
PSRAW	MM,IMM8	MMX	PSRAW MM,IMM8	Shift words in mm right by imm8 while shifting in sign bits
PSRAW	XMM,IMM8	SSE2	PSRAW XMM1,IMM8	Shift words in xmm1 right by imm8 while shifting in sign bits
GENERAL	PSRAD	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ
PSRAD	MM,MM/M64	MMX	PSRAD MM,MM/M64	Shift doublewords in mm right by mm/m64 while shifting in sign bits.
PSRAD	XMM,XMM/M128	SSE2	PSRAD XMM1,XMM2/M128	Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits.
PSRAD	MM,IMM8	MMX	PSRAD MM,IMM8	Shift doublewords in mm right by imm8 while shifting in sign bits.
PSRAD	XMM,IMM8	SSE2	PSRAD XMM1,IMM8	Shift doublewords in xmm1 right by imm8 while shifting in sign bits.
GENERAL	VPSRAW	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ
VPSRAW	XMM,XMM,XMM/M128	AVX	VPSRAW XMM1,XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAW	XMM,XMM,IMM8	AVX	VPSRAW XMM1,XMM2,IMM8	Shift words in xmm2 right by imm8 while shifting in sign bits.
VPSRAW	YMM,YMM,XMM/M128	AVX2	VPSRAW YMM1,YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAW	YMM,YMM,IMM8	AVX2	VPSRAW YMM1,YMM2,IMM8	Shift words in ymm2 right by imm8 while shifting in sign bits.
VPSRAW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRAW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAW	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRAW YMM1{K1}{Z},YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAW	ZMM{K}{Z},ZMM,XMM/M128	AVX512_BW	VPSRAW ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAW	XMM{K}{Z},XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSRAW XMM1{K1}{Z},XMM2/M128,IMM8	Shift words in xmm2/m128 right by imm8 while shifting in sign bits using writemask k1.
VPSRAW	YMM{K}{Z},YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSRAW YMM1{K1}{Z},YMM2/M256,IMM8	Shift words in ymm2/m256 right by imm8 while shifting in sign bits using writemask k1.
VPSRAW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512_BW	VPSRAW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shift words in zmm2/m512 right by imm8 while shifting in sign bits using writemask k1.
GENERAL	VPSRAD	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ
VPSRAD	XMM,XMM,XMM/M128	AVX	VPSRAD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAD	XMM,XMM,IMM8	AVX	VPSRAD XMM1,XMM2,IMM8	Shift doublewords in xmm2 right by imm8 while shifting in sign bits.
VPSRAD	YMM,YMM,XMM/M128	AVX2	VPSRAD YMM1,YMM2,XMM3/M128	Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAD	YMM,YMM,IMM8	AVX2	VPSRAD YMM1,YMM2,IMM8	Shift doublewords in ymm2 right by imm8 while shifting in sign bits.
VPSRAD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSRAD XMM1{K1}{Z},XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAD	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSRAD YMM1{K1}{Z},YMM2,XMM3/M128	Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAD	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSRAD ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSRAD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Shift doublewords in xmm2/m128/m32bcst right by imm8 while shifting in sign bits using writemask k1.
VPSRAD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSRAD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Shift doublewords in ymm2/m256/m32bcst right by imm8 while shifting in sign bits using writemask k1.
VPSRAD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPSRAD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Shift doublewords in zmm2/m512/m32bcst right by imm8 while shifting in sign bits using writemask k1.
GENERAL	VPSRAQ	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ
VPSRAQ	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSRAQ XMM1{K1}{Z},XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAQ	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSRAQ YMM1{K1}{Z},YMM2,XMM3/M128	Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAQ	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSRAQ ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSRAQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Shift quadwords in xmm2/m128/m64bcst right by imm8 while shifting in sign bits using writemask k1.
VPSRAQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSRAQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Shift quadwords in ymm2/m256/m64bcst right by imm8 while shifting in sign bits using writemask k1.
VPSRAQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPSRAQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Shift quadwords in zmm2/m512/m64bcst right by imm8 while shifting in sign bits using writemask k1.
;--------------------------------------------------------
GENERAL	PSRLDQ	Shift Double Quadword Right Logical	PSRLDQ
PSRLDQ	XMM,IMM8	SSE2	PSRLDQ XMM1,IMM8	Shift xmm1 right by imm8 while shifting in 0s.
GENERAL	VPSRLDQ	Shift Double Quadword Right Logical	PSRLDQ
VPSRLDQ	XMM,XMM,IMM8	AVX	VPSRLDQ XMM1,XMM2,IMM8	Shift xmm2 right by imm8 bytes while shifting in 0s.
VPSRLDQ	YMM,YMM,IMM8	AVX2	VPSRLDQ YMM1,YMM2,IMM8	Shift ymm1 right by imm8 bytes while shifting in 0s.
VPSRLDQ	XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSRLDQ XMM1,XMM2/M128,IMM8	Shift xmm2/m128 right by imm8 bytes while shifting in 0s and store result in xmm1.
VPSRLDQ	YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSRLDQ YMM1,YMM2/M256,IMM8	Shift ymm2/m256 right by imm8 bytes while shifting in 0s and store result in ymm1.
VPSRLDQ	ZMM,ZMM/M512,IMM8	AVX512_BW	VPSRLDQ ZMM1,ZMM2/M512,IMM8	Shift zmm2/m512 right by imm8 bytes while shifting in 0s and store result in zmm1.
;--------------------------------------------------------
GENERAL	PSRLW	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
PSRLW	MM,MM/M64	MMX	PSRLW MM,MM/M64	Shift words in mm right by amount specified in mm/m64 while shifting in 0s.
PSRLW	XMM,XMM/M128	SSE2	PSRLW XMM1,XMM2/M128	Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
PSRLW	MM,IMM8	MMX	PSRLW MM,IMM8	Shift words in mm right by imm8 while shifting in 0s.
PSRLW	XMM,IMM8	SSE2	PSRLW XMM1,IMM8	Shift words in xmm1 right by imm8 while shifting in 0s.
GENERAL	PSRLD	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
PSRLD	MM,MM/M64	MMX	PSRLD MM,MM/M64	Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s.
PSRLD	XMM,XMM/M128	SSE2	PSRLD XMM1,XMM2/M128	Shift doublewords in xmm1 right by amount specified in xmm2 /m128 while shifting in 0s.
PSRLD	MM,IMM8	MMX	PSRLD MM,IMM8	Shift doublewords in mm right by imm8 while shifting in 0s.
PSRLD	XMM,IMM8	SSE2	PSRLD XMM1,IMM8	Shift doublewords in xmm1 right by imm8 while shifting in 0s.
GENERAL	PSRLQ	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
PSRLQ	MM,MM/M64	MMX	PSRLQ MM,MM/M64	Shift mm right by amount specified in mm/m64 while shifting in 0s.
PSRLQ	XMM,XMM/M128	SSE2	PSRLQ XMM1,XMM2/M128	Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
PSRLQ	MM,IMM8	MMX	PSRLQ MM,IMM8	Shift mm right by imm8 while shifting in 0s.
PSRLQ	XMM,IMM8	SSE2	PSRLQ XMM1,IMM8	Shift quadwords in xmm1 right by imm8 while shifting in 0s.
GENERAL	VPSRLW	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
VPSRLW	XMM,XMM,XMM/M128	AVX	VPSRLW XMM1,XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLW	XMM,XMM,IMM8	AVX	VPSRLW XMM1,XMM2,IMM8	Shift words in xmm2 right by imm8 while shifting in 0s.
VPSRLW	YMM,YMM,XMM/M128	AVX2	VPSRLW YMM1,YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLW	YMM,YMM,IMM8	AVX2	VPSRLW YMM1,YMM2,IMM8	Shift words in ymm2 right by imm8 while shifting in 0s.
VPSRLW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRLW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLW	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRLW YMM1{K1}{Z},YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLW	ZMM{K}{Z},ZMM,XMM/M128	AVX512_BW	VPSRLW ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLW	XMM{K}{Z},XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPSRLW XMM1{K1}{Z},XMM2/M128,IMM8	Shift words in xmm2/m128 right by imm8 while shifting in 0s using writemask k1.
VPSRLW	YMM{K}{Z},YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPSRLW YMM1{K1}{Z},YMM2/M256,IMM8	Shift words in ymm2/m256 right by imm8 while shifting in 0s using writemask k1.
VPSRLW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512_BW	VPSRLW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shift words in zmm2/m512 right by imm8 while shifting in 0s using writemask k1.
GENERAL	VPSRLD	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
VPSRLD	XMM,XMM,XMM/M128	AVX	VPSRLD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLD	XMM,XMM,IMM8	AVX	VPSRLD XMM1,XMM2,IMM8	Shift doublewords in xmm2 right by imm8 while shifting in 0s.
VPSRLD	YMM,YMM,XMM/M128	AVX2	VPSRLD YMM1,YMM2,XMM3/M128	Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLD	YMM,YMM,IMM8	AVX2	VPSRLD YMM1,YMM2,IMM8	Shift doublewords in ymm2 right by imm8 while shifting in 0s.
VPSRLD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSRLD XMM1{K1}{Z},XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLD	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSRLD YMM1{K1}{Z},YMM2,XMM3/M128	Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLD	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSRLD ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSRLD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Shift doublewords in xmm2/m128/m32bcst right by imm8 while shifting in 0s using writemask k1.
VPSRLD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPSRLD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Shift doublewords in ymm2/m256/m32bcst right by imm8 while shifting in 0s using writemask k1.
VPSRLD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPSRLD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Shift doublewords in zmm2/m512/m32bcst right by imm8 while shifting in 0s using writemask k1.
GENERAL	VPSRLQ	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ
VPSRLQ	XMM,XMM,XMM/M128	AVX	VPSRLQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLQ	XMM,XMM,IMM8	AVX	VPSRLQ XMM1,XMM2,IMM8	Shift quadwords in xmm2 right by imm8 while shifting in 0s.
VPSRLQ	YMM,YMM,XMM/M128	AVX2	VPSRLQ YMM1,YMM2,XMM3/M128	Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLQ	YMM,YMM,IMM8	AVX2	VPSRLQ YMM1,YMM2,IMM8	Shift quadwords in ymm2 right by imm8 while shifting in 0s.
VPSRLQ	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_F	VPSRLQ XMM1{K1}{Z},XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLQ	YMM{K}{Z},YMM,XMM/M128	AVX512_VL,AVX512_F	VPSRLQ YMM1{K1}{Z},YMM2,XMM3/M128	Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLQ	ZMM{K}{Z},ZMM,XMM/M128	AVX512_F	VPSRLQ ZMM1{K1}{Z},ZMM2,XMM3/M128	Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.
VPSRLQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSRLQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Shift quadwords in xmm2/m128/m64bcst right by imm8 while shifting in 0s using writemask k1.
VPSRLQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPSRLQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Shift quadwords in ymm2/m256/m64bcst right by imm8 while shifting in 0s using writemask k1.
VPSRLQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPSRLQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Shift quadwords in zmm2/m512/m64bcst right by imm8 while shifting in 0s using writemask k1.
;--------------------------------------------------------
GENERAL	PSUBB	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
PSUBB	MM,MM/M64	MMX	PSUBB MM,MM/M64	Subtract packed byte integers in mm/m64 from packed byte integers in mm.
PSUBB	XMM,XMM/M128	SSE2	PSUBB XMM1,XMM2/M128	Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1.
GENERAL	PSUBW	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
PSUBW	MM,MM/M64	MMX	PSUBW MM,MM/M64	Subtract packed word integers in mm/m64 from packed word integers in mm.
PSUBW	XMM,XMM/M128	SSE2	PSUBW XMM1,XMM2/M128	Subtract packed word integers in xmm2/m128 from packed word integers in xmm1.
GENERAL	PSUBD	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
PSUBD	MM,MM/M64	MMX	PSUBD MM,MM/M64	Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm.
PSUBD	XMM,XMM/M128	SSE2	PSUBD XMM1,XMM2/M128	Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1.
GENERAL	VPSUBB	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
VPSUBB	XMM,XMM,XMM/M128	AVX	VPSUBB XMM1,XMM2,XMM3/M128	Subtract packed byte integers in xmm3/m128 from xmm2.
VPSUBB	YMM,YMM,YMM/M256	AVX2	VPSUBB YMM1,YMM2,YMM3/M256	Subtract packed byte integers in ymm3/m256 from ymm2.
VPSUBB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed byte integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1.
VPSUBB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed byte integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1.
VPSUBB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed byte integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1.
GENERAL	VPSUBW	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
VPSUBW	XMM,XMM,XMM/M128	AVX	VPSUBW XMM1,XMM2,XMM3/M128	Subtract packed word integers in xmm3/m128 from xmm2.
VPSUBW	YMM,YMM,YMM/M256	AVX2	VPSUBW YMM1,YMM2,YMM3/M256	Subtract packed word integers in ymm3/m256 from ymm2.
VPSUBW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed word integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1.
VPSUBW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed word integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1.
VPSUBW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBW ZMM1{K1}{Z},ZMM2,ZMM3/M512	zmm3/m512 from zmm2 and store in zmm1 using writemask k1.
GENERAL	VPSUBD	Subtract Packed Integers	PSUBB_PSUBW_PSUBD
VPSUBD	XMM,XMM,XMM/M128	AVX	VPSUBD XMM1,XMM2,XMM3/M128	Subtract packed doubleword integers in xmm3/m128 from xmm2.
VPSUBD	YMM,YMM,YMM/M256	AVX2	VPSUBD YMM1,YMM2,YMM3/M256	Subtract packed doubleword integers in ymm3/m256 from ymm2.
VPSUBD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPSUBD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Subtract packed doubleword integers in xmm3/m128/m32bcst from xmm2 and store in xmm1 using writemask k1.
VPSUBD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPSUBD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Subtract packed doubleword integers in ymm3/m256/m32bcst from ymm2 and store in ymm1 using writemask k1.
VPSUBD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPSUBD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Subtract packed doubleword integers in zmm3/m512/m32bcst from zmm2 and store in zmm1 using writemask k1
;--------------------------------------------------------
GENERAL	PSUBQ	Subtract Packed Quadword Integers	PSUBQ
PSUBQ	MM,MM/M64	SSE2	PSUBQ MM1,MM2/M64	Subtract quadword integer in mm1 from mm2 /m64.
PSUBQ	XMM,XMM/M128	SSE2	PSUBQ XMM1,XMM2/M128	Subtract packed quadword integers in xmm1 from xmm2 /m128.
GENERAL	VPSUBQ	Subtract Packed Quadword Integers	PSUBQ
VPSUBQ	XMM,XMM,XMM/M128	AVX	VPSUBQ XMM1,XMM2,XMM3/M128	Subtract packed quadword integers in xmm3/m128 from xmm2.
VPSUBQ	YMM,YMM,YMM/M256	AVX2	VPSUBQ YMM1,YMM2,YMM3/M256	Subtract packed quadword integers in ymm3/m256 from ymm2.
VPSUBQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPSUBQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Subtract packed quadword integers in xmm3/m128/m64bcst from xmm2 and store in xmm1 using writemask k1.
VPSUBQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPSUBQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Subtract packed quadword integers in ymm3/m256/m64bcst from ymm2 and store in ymm1 using writemask k1.
VPSUBQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPSUBQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Subtract packed quadword integers in zmm3/m512/m64bcst from zmm2 and store in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PSUBSB	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW
PSUBSB	MM,MM/M64	MMX	PSUBSB MM,MM/M64	Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results.
PSUBSB	XMM,XMM/M128	SSE2	PSUBSB XMM1,XMM2/M128	Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results.
GENERAL	PSUBSW	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW
PSUBSW	MM,MM/M64	MMX	PSUBSW MM,MM/M64	Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results.
PSUBSW	XMM,XMM/M128	SSE2	PSUBSW XMM1,XMM2/M128	Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results.
GENERAL	VPSUBSB	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW
VPSUBSB	XMM,XMM,XMM/M128	AVX	VPSUBSB XMM1,XMM2,XMM3/M128	Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results.
VPSUBSB	YMM,YMM,YMM/M256	AVX2	VPSUBSB YMM1,YMM2,YMM3/M256	Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results.
VPSUBSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBSB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBSB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results and store in ymm1 using writemask k1.
VPSUBSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed signed byte integers in zmm3/m512 from packed signed byte integers in zmm2 and saturate results and store in zmm1 using writemask k1.
GENERAL	VPSUBSW	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW
VPSUBSW	XMM,XMM,XMM/M128	AVX	VPSUBSW XMM1,XMM2,XMM3/M128	Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results.
VPSUBSW	YMM,YMM,YMM/M256	AVX2	VPSUBSW YMM1,YMM2,YMM3/M256	Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results.
VPSUBSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBSW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBSW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results and store in ymm1 using writemask k1.
VPSUBSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed signed word integers in zmm3/m512 from packed signed word integers in zmm2 and saturate results and store in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PSUBUSB	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW
PSUBUSB	MM,MM/M64	MMX	PSUBUSB MM,MM/M64	Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result.
PSUBUSB	XMM,XMM/M128	SSE2	PSUBUSB XMM1,XMM2/M128	Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result.
GENERAL	PSUBUSW	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW
PSUBUSW	MM,MM/M64	MMX	PSUBUSW MM,MM/M64	Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result.
PSUBUSW	XMM,XMM/M128	SSE2	PSUBUSW XMM1,XMM2/M128	Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result.
GENERAL	VPSUBUSB	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW
VPSUBUSB	XMM,XMM,XMM/M128	AVX	VPSUBUSB XMM1,XMM2,XMM3/M128	Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result.
VPSUBUSB	YMM,YMM,YMM/M256	AVX2	VPSUBUSB YMM1,YMM2,YMM3/M256	Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result.
VPSUBUSB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBUSB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2, saturate results and store in xmm1 using writemask k1.
VPSUBUSB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBUSB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2, saturate results and store in ymm1 using writemask k1.
VPSUBUSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBUSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed unsigned byte integers in zmm3/m512 from packed unsigned byte integers in zmm2, saturate results and store in zmm1 using writemask k1.
GENERAL	VPSUBUSW	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW
VPSUBUSW	XMM,XMM,XMM/M128	AVX	VPSUBUSW XMM1,XMM2,XMM3/M128	Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result.
VPSUBUSW	YMM,YMM,YMM/M256	AVX2	VPSUBUSW YMM1,YMM2,YMM3/M256	Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result.
VPSUBUSW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSUBUSW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBUSW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSUBUSW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2, saturate results and store in ymm1 using writemask k1.
VPSUBUSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSUBUSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed unsigned word integers in zmm3/m512 from packed unsigned word integers in zmm2, saturate results and store in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	PUNPCKHBW	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
PUNPCKHBW	MM,MM/M64	MMX	PUNPCKHBW MM,MM/M64	Unpack and interleave high-order bytes from mm and mm/m64 into mm.
PUNPCKHBW	XMM,XMM/M128	SSE2	PUNPCKHBW XMM1,XMM2/M128	Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKHWD	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
PUNPCKHWD	MM,MM/M64	MMX	PUNPCKHWD MM,MM/M64	Unpack and interleave high-order words from mm and mm/m64 into mm.
PUNPCKHWD	XMM,XMM/M128	SSE2	PUNPCKHWD XMM1,XMM2/M128	Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKHDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
PUNPCKHDQ	MM,MM/M64	MMX	PUNPCKHDQ MM,MM/M64	Unpack and interleave high-order doublewords from mm and mm/m64 into mm.
PUNPCKHDQ	XMM,XMM/M128	SSE2	PUNPCKHDQ XMM1,XMM2/M128	Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKHQDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
PUNPCKHQDQ	XMM,XMM/M128	SSE2	PUNPCKHQDQ XMM1,XMM2/M128	Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1.
GENERAL	VPUNPCKHBW	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
VPUNPCKHBW	XMM,XMM,XMM/M128	AVX	VPUNPCKHBW XMM1,XMM2,XMM3/M128	Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHBW	YMM,YMM,YMM/M256	AVX2	VPUNPCKHBW YMM1,YMM2,YMM3/M256	Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHBW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPUNPCKHBW XMM1{K1}{Z},XMM2,XMM3/M128	Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1 register using k1 write mask.
VPUNPCKHBW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPUNPCKHBW YMM1{K1}{Z},YMM2,YMM3/M256	Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register using k1 write mask.
VPUNPCKHBW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPUNPCKHBW ZMM1{K1}{Z},ZMM2,ZMM3/M512	zmm3/m512 into zmm1 register.
GENERAL	VPUNPCKHWD	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
VPUNPCKHWD	XMM,XMM,XMM/M128	AVX	VPUNPCKHWD XMM1,XMM2,XMM3/M128	Interleave high-order words from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHWD	YMM,YMM,YMM/M256	AVX2	VPUNPCKHWD YMM1,YMM2,YMM3/M256	Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHWD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPUNPCKHWD XMM1{K1}{Z},XMM2,XMM3/M128	Interleave high-order words from xmm2 and xmm3/m128 into xmm1 register using k1 write mask.
VPUNPCKHWD	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPUNPCKHWD YMM1{K1}{Z},YMM2,YMM3/M256	Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register using k1 write mask.
VPUNPCKHWD	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPUNPCKHWD ZMM1{K1}{Z},ZMM2,ZMM3/M512	zmm3/m512 into zmm1 register.
GENERAL	VPUNPCKHDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
VPUNPCKHDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKHDQ XMM1,XMM2,XMM3/M128	Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKHDQ YMM1,YMM2,YMM3/M256	Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHDQ	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPUNPCKHDQ XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Interleave high-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register using k1 write mask.
VPUNPCKHDQ	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPUNPCKHDQ YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Interleave high-order doublewords from ymm2 and ymm3/m256/m32bcst into ymm1 register using k1 write mask.
VPUNPCKHDQ	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPUNPCKHDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Interleave high-order doublewords from zmm2 and zmm3/m512/m32bcst into zmm1 register using k1 write mask.
GENERAL	VPUNPCKHQDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ
VPUNPCKHQDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKHQDQ XMM1,XMM2,XMM3/M128	Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VPUNPCKHQDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKHQDQ YMM1,YMM2,YMM3/M256	Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHQDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPUNPCKHQDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Interleave high-order quadword from xmm2 and xmm3/m128/m64bcst into xmm1 register using k1 write mask.
VPUNPCKHQDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPUNPCKHQDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Interleave high-order quadword from ymm2 and ymm3/m256/m64bcst into ymm1 register using k1 write mask.
VPUNPCKHQDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPUNPCKHQDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Interleave high-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register using k1 write mask.
;--------------------------------------------------------
GENERAL	PUNPCKLBW	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
PUNPCKLBW	MM,MM/M32	MMX	PUNPCKLBW MM,MM/M32	Interleave low-order bytes from mm and mm/m32 into mm.
PUNPCKLBW	XMM,XMM/M128	SSE2	PUNPCKLBW XMM1,XMM2/M128	Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKLWD	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
PUNPCKLWD	MM,MM/M32	MMX	PUNPCKLWD MM,MM/M32	Interleave low-order words from mm and mm/m32 into mm.
PUNPCKLWD	XMM,XMM/M128	SSE2	PUNPCKLWD XMM1,XMM2/M128	Interleave low-order words from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKLDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
PUNPCKLDQ	MM,MM/M32	MMX	PUNPCKLDQ MM,MM/M32	Interleave low-order doublewords from mm and mm/m32 into mm.
PUNPCKLDQ	XMM,XMM/M128	SSE2	PUNPCKLDQ XMM1,XMM2/M128	Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1.
GENERAL	PUNPCKLQDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
PUNPCKLQDQ	XMM,XMM/M128	SSE2	PUNPCKLQDQ XMM1,XMM2/M128	Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register.
GENERAL	VPUNPCKLBW	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
VPUNPCKLBW	XMM,XMM,XMM/M128	AVX	VPUNPCKLBW XMM1,XMM2,XMM3/M128	Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLBW	YMM,YMM,YMM/M256	AVX2	VPUNPCKLBW YMM1,YMM2,YMM3/M256	Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLBW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPUNPCKLBW XMM1{K1}{Z},XMM2,XMM3/M128	Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
VPUNPCKLBW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPUNPCKLBW YMM1{K1}{Z},YMM2,YMM3/M256	Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register subject to write mask k1.
VPUNPCKLBW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPUNPCKLBW ZMM1{K1}{Z},ZMM2,ZMM3/M512	zmm3/m512 into zmm1 register subject to write mask k1.
GENERAL	VPUNPCKLWD	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
VPUNPCKLWD	XMM,XMM,XMM/M128	AVX	VPUNPCKLWD XMM1,XMM2,XMM3/M128	Interleave low-order words from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLWD	YMM,YMM,YMM/M256	AVX2	VPUNPCKLWD YMM1,YMM2,YMM3/M256	Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLWD	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPUNPCKLWD XMM1{K1}{Z},XMM2,XMM3/M128	Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
VPUNPCKLWD	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPUNPCKLWD YMM1{K1}{Z},YMM2,YMM3/M256	Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register subject to write mask k1.
VPUNPCKLWD	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPUNPCKLWD ZMM1{K1}{Z},ZMM2,ZMM3/M512	zmm3/m512 into zmm1 register subject to write mask k1.
GENERAL	VPUNPCKLDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
VPUNPCKLDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKLDQ XMM1,XMM2,XMM3/M128	Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKLDQ YMM1,YMM2,YMM3/M256	Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLDQ	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPUNPCKLDQ XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1.
VPUNPCKLDQ	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPUNPCKLDQ YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Interleave low-order doublewords from ymm2 and ymm3/m256/m32bcst into ymm1 register subject to write mask k1.
VPUNPCKLDQ	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPUNPCKLDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Interleave low-order doublewords from zmm2 and zmm3/m512/m32bcst into zmm1 register subject to write mask k1.
GENERAL	VPUNPCKLQDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ
VPUNPCKLQDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKLQDQ XMM1,XMM2,XMM3/M128	Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VPUNPCKLQDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKLQDQ YMM1,YMM2,YMM3/M256	Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLQDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPUNPCKLQDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.
VPUNPCKLQDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPUNPCKLQDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Interleave low-order quadword from ymm2 and ymm3/m256/m64bcst into ymm1 register subject to write mask k1.
VPUNPCKLQDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPUNPCKLQDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.
;--------------------------------------------------------
GENERAL	PUSH	Push Word, Doubleword or Quadword Onto the Stack	PUSH
PUSH	R/M16	8086	PUSH R/M16	Push r/m16.
PUSH	R/M32	386	PUSH R/M32	Push r/m32.
PUSH	R/M64	X64	PUSH R/M64	Push r/m64.
PUSH	R16	8086	PUSH R16	Push r16.
PUSH	R32	386	PUSH R32	Push r32.
PUSH	R64	X64	PUSH R64	Push r64.
PUSH	IMM8	8086	PUSH IMM8	Push imm8.
PUSH	IMM16	8086	PUSH IMM16	Push imm16.
PUSH	IMM32	386	PUSH IMM32	Push imm32.
PUSH	CS	8086	PUSH CS	Push CS.
PUSH	SS	8086	PUSH SS	Push SS.
PUSH	DS	8086	PUSH DS	Push DS.
PUSH	ES	8086	PUSH ES	Push ES.
PUSH	FS	8086	PUSH FS	Push FS.
PUSH	GS	8086	PUSH GS	Push GS.
;--------------------------------------------------------
GENERAL	PUSHA	Push All General-Purpose Registers	PUSHA_PUSHAD
PUSHA		8086	PUSHA	Push AX, CX, DX, BX, original SP, BP, SI, and DI.
GENERAL	PUSHAD	Push All General-Purpose Registers	PUSHA_PUSHAD
PUSHAD		8086	PUSHAD	Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI.
;--------------------------------------------------------
GENERAL	PUSHF	Push EFLAGS Register onto the Stack	PUSHF_PUSHFD_PUSHFQ
PUSHF		8086	PUSHF	Push lower 16 bits of EFLAGS.
GENERAL	PUSHFD	Push EFLAGS Register onto the Stack	PUSHF_PUSHFD_PUSHFQ
PUSHFD		8086	PUSHFD	Push EFLAGS.
GENERAL	PUSHFQ	Push EFLAGS Register onto the Stack	PUSHF_PUSHFD_PUSHFQ
PUSHFQ		8086	PUSHFQ	Push RFLAGS.
;--------------------------------------------------------
GENERAL	PXOR	Logical Exclusive OR	PXOR
PXOR	MM,MM/M64	MMX	PXOR MM,MM/M64	Bitwise XOR of mm/m64 and mm.
PXOR	XMM,XMM/M128	SSE2	PXOR XMM1,XMM2/M128	Bitwise XOR of xmm2/m128 and xmm1.
GENERAL	VPXOR	Logical Exclusive OR	PXOR
VPXOR	XMM,XMM,XMM/M128	AVX	VPXOR XMM1,XMM2,XMM3/M128	Bitwise XOR of xmm3/m128 and xmm2.
VPXOR	YMM,YMM,YMM/M256	AVX2	VPXOR YMM1,YMM2,YMM3/M256	Bitwise XOR of ymm3/m256 and ymm2.
GENERAL	VPXORD	Logical Exclusive OR	PXOR
VPXORD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPXORD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise XOR of packed doubleword integers in xmm2 and xmm3/m128 using writemask k1.
VPXORD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPXORD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise XOR of packed doubleword integers in ymm2 and ymm3/m256 using writemask k1.
VPXORD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPXORD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise XOR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1.
GENERAL	VPXORQ	Logical Exclusive OR	PXOR
VPXORQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPXORQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise XOR of packed quadword integers in xmm2 and xmm3/m128 using writemask k1.
VPXORQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPXORQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise XOR of packed quadword integers in ymm2 and ymm3/m256 using writemask k1.
VPXORQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPXORQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise XOR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1.
;--------------------------------------------------------
GENERAL	RCL	Rotate	RCL_RCR_ROL_ROR
RCL	R/M8,1	8086	RCL R/M8,1	Rotate 9 bits (CF, r/m8) left once.
RCL	R/M8,1	8086	RCL R/M8,1	Rotate 9 bits (CF, r/m8) left once.
RCL	R/M8,CL	8086	RCL R/M8,CL	Rotate 9 bits (CF, r/m8) left CL times.
RCL	R/M8,CL	8086	RCL R/M8,CL	Rotate 9 bits (CF, r/m8) left CL times.
RCL	R/M8,IMM8	8086	RCL R/M8,IMM8	Rotate 9 bits (CF, r/m8) left imm8 times.
RCL	R/M8,IMM8	8086	RCL R/M8,IMM8	Rotate 9 bits (CF, r/m8) left imm8 times.
RCL	R/M16,1	8086	RCL R/M16,1	Rotate 17 bits (CF, r/m16) left once.
RCL	R/M16,CL	8086	RCL R/M16,CL	Rotate 17 bits (CF, r/m16) left CL times.
RCL	R/M16,IMM8	8086	RCL R/M16,IMM8	Rotate 17 bits (CF, r/m16) left imm8 times.
RCL	R/M32,1	386	RCL R/M32,1	Rotate 33 bits (CF, r/m32) left once.
RCL	R/M64,1	X64	RCL R/M64,1	Rotate 65 bits (CF, r/m64) left once. Uses a 6 bit count.
RCL	R/M32,CL	386	RCL R/M32,CL	Rotate 33 bits (CF, r/m32) left CL times.
RCL	R/M64,CL	X64	RCL R/M64,CL	Rotate 65 bits (CF, r/m64) left CL times. Uses a 6 bit count.
RCL	R/M32,IMM8	386	RCL R/M32,IMM8	Rotate 33 bits (CF, r/m32) left imm8 times.
RCL	R/M64,IMM8	X64	RCL R/M64,IMM8	Rotate 65 bits (CF, r/m64) left imm8 times. Uses a 6 bit count.
GENERAL	RCR	Rotate	RCL_RCR_ROL_ROR
RCR	R/M8,1	8086	RCR R/M8,1	Rotate 9 bits (CF, r/m8) right once.
RCR	R/M8,1	8086	RCR R/M8,1	Rotate 9 bits (CF, r/m8) right once.
RCR	R/M8,CL	8086	RCR R/M8,CL	Rotate 9 bits (CF, r/m8) right CL times.
RCR	R/M8,CL	8086	RCR R/M8,CL	Rotate 9 bits (CF, r/m8) right CL times.
RCR	R/M8,IMM8	8086	RCR R/M8,IMM8	Rotate 9 bits (CF, r/m8) right imm8 times.
RCR	R/M8,IMM8	8086	RCR R/M8,IMM8	Rotate 9 bits (CF, r/m8) right imm8 times.
RCR	R/M16,1	8086	RCR R/M16,1	Rotate 17 bits (CF, r/m16) right once.
RCR	R/M16,CL	8086	RCR R/M16,CL	Rotate 17 bits (CF, r/m16) right CL times.
RCR	R/M16,IMM8	8086	RCR R/M16,IMM8	Rotate 17 bits (CF, r/m16) right imm8 times.
RCR	R/M32,1	386	RCR R/M32,1	Rotate 33 bits (CF, r/m32) right once. Uses a 6 bit count.
RCR	R/M64,1	X64	RCR R/M64,1	Rotate 65 bits (CF, r/m64) right once. Uses a 6 bit count.
RCR	R/M32,CL	386	RCR R/M32,CL	Rotate 33 bits (CF, r/m32) right CL times.
RCR	R/M64,CL	X64	RCR R/M64,CL	Rotate 65 bits (CF, r/m64) right CL times. Uses a 6 bit count.
RCR	R/M32,IMM8	386	RCR R/M32,IMM8	Rotate 33 bits (CF, r/m32) right imm8 times.
RCR	R/M64,IMM8	X64	RCR R/M64,IMM8	Rotate 65 bits (CF, r/m64) right imm8 times. Uses a 6 bit count.
GENERAL	ROL	Rotate	RCL_RCR_ROL_ROR
ROL	R/M8,1	8086	ROL R/M8,1	Rotate 8 bits r/m8 left once.
ROL	R/M8,1	8086	ROL R/M8,1	Rotate 8 bits r/m8 left once
ROL	R/M8,CL	8086	ROL R/M8,CL	Rotate 8 bits r/m8 left CL times.
ROL	R/M8,CL	8086	ROL R/M8,CL	Rotate 8 bits r/m8 left CL times.
ROL	R/M8,IMM8	8086	ROL R/M8,IMM8	Rotate 8 bits r/m8 left imm8 times.
ROL	R/M8,IMM8	8086	ROL R/M8,IMM8	Rotate 8 bits r/m8 left imm8 times.
ROL	R/M16,1	8086	ROL R/M16,1	Rotate 16 bits r/m16 left once.
ROL	R/M16,CL	8086	ROL R/M16,CL	Rotate 16 bits r/m16 left CL times.
ROL	R/M16,IMM8	8086	ROL R/M16,IMM8	Rotate 16 bits r/m16 left imm8 times.
ROL	R/M32,1	386	ROL R/M32,1	Rotate 32 bits r/m32 left once.
ROL	R/M64,1	X64	ROL R/M64,1	Rotate 64 bits r/m64 left once. Uses a 6 bit count.
ROL	R/M32,CL	386	ROL R/M32,CL	Rotate 32 bits r/m32 left CL times.
ROL	R/M64,CL	X64	ROL R/M64,CL	Rotate 64 bits r/m64 left CL times. Uses a 6 bit count.
ROL	R/M32,IMM8	386	ROL R/M32,IMM8	Rotate 32 bits r/m32 left imm8 times.
ROL	R/M64,IMM8	X64	ROL R/M64,IMM8	Rotate 64 bits r/m64 left imm8 times. Uses a 6 bit count.
GENERAL	ROR	Rotate	RCL_RCR_ROL_ROR
ROR	R/M8,1	8086	ROR R/M8,1	Rotate 8 bits r/m8 right once.
ROR	R/M8,1	8086	ROR R/M8,1	Rotate 8 bits r/m8 right once.
ROR	R/M8,CL	8086	ROR R/M8,CL	Rotate 8 bits r/m8 right CL times.
ROR	R/M8,CL	8086	ROR R/M8,CL	Rotate 8 bits r/m8 right CL times.
ROR	R/M8,IMM8	8086	ROR R/M8,IMM8	Rotate 8 bits r/m16 right imm8 times.
ROR	R/M8,IMM8	8086	ROR R/M8,IMM8	Rotate 8 bits r/m16 right imm8 times.
ROR	R/M16,1	8086	ROR R/M16,1	Rotate 16 bits r/m16 right once.
ROR	R/M16,CL	8086	ROR R/M16,CL	Rotate 16 bits r/m16 right CL times.
ROR	R/M16,IMM8	8086	ROR R/M16,IMM8	Rotate 16 bits r/m16 right imm8 times.
ROR	R/M32,1	386	ROR R/M32,1	Rotate 32 bits r/m32 right once.
ROR	R/M64,1	X64	ROR R/M64,1	Rotate 64 bits r/m64 right once. Uses a 6 bit count.
ROR	R/M32,CL	386	ROR R/M32,CL	Rotate 32 bits r/m32 right CL times.
ROR	R/M64,CL	X64	ROR R/M64,CL	Rotate 64 bits r/m64 right CL times. Uses a 6 bit count.
ROR	R/M32,IMM8	386	ROR R/M32,IMM8	Rotate 32 bits r/m32 right imm8 times.
ROR	R/M64,IMM8	X64	ROR R/M64,IMM8	Rotate 64 bits r/m64 right imm8 times. Uses a 6 bit count.
;--------------------------------------------------------
GENERAL	RCPPS	Compute Reciprocals of Packed Single-Precision Floating-Point Values	RCPPS
RCPPS	XMM,XMM/M128	SSE	RCPPS XMM1,XMM2/M128	Computes the approximate reciprocals of the packed SP FP values in xmm2/m128 and stores the results in xmm1.
GENERAL	VRCPPS	Compute Reciprocals of Packed Single-Precision Floating-Point Values	RCPPS
VRCPPS	XMM,XMM/M128	AVX	VRCPPS XMM1,XMM2/M128	Computes the approximate reciprocals of packed SP values in xmm2/mem and stores the results in xmm1.
VRCPPS	YMM,YMM/M256	AVX	VRCPPS YMM1,YMM2/M256	Computes the approximate reciprocals of packed SP values in ymm2/mem and stores the results in ymm1.
;--------------------------------------------------------
GENERAL	RCPSS	Compute Reciprocal of Scalar Single-Precision Floating-Point Values	RCPSS
RCPSS	XMM,XMM/M32	SSE	RCPSS XMM1,XMM2/M32	Computes the approximate reciprocal of the scalar SP FP value in xmm2/m32 and stores the result in xmm1.
GENERAL	VRCPSS	Compute Reciprocal of Scalar Single-Precision Floating-Point Values	RCPSS
VRCPSS	XMM,XMM,XMM/M32	AVX	VRCPSS XMM1,XMM2,XMM3/M32	Computes the approximate reciprocal of the scalar SP FP value in xmm3/m32 and stores the result in xmm1. Also, upper single precision FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	RDFSBASE	Read FS/GS Segment Base	RDFSBASE_RDGSBASE
RDFSBASE	R32	FSGSBASE	RDFSBASE R32	Load the 32-bit destination register with the FS base address.
RDFSBASE	R64	FSGSBASE	RDFSBASE R64	Load the 64-bit destination register with the FS base address.
GENERAL	RDGSBASE	Read FS/GS Segment Base	RDFSBASE_RDGSBASE
RDGSBASE	R32	FSGSBASE	RDGSBASE R32	Load the 32-bit destination register with the GS base address.
RDGSBASE	R64		RDGSBASE R64	FSGSBASE
;--------------------------------------------------------
GENERAL	RDMSR	Read from Model Specific Register	RDMSR
RDMSR		8086	RDMSR	Read MSR specified by ECX into EDX:EAX.
;--------------------------------------------------------
GENERAL	RDPID	Read Processor ID	RDPID
RDPID	R32	RDPID	RDPID R32	Read IA32_TSC_AUX into r32.
RDPID	R64	RDPID	RDPID R64	Read IA32_TSC_AUX into r64.
;--------------------------------------------------------
GENERAL	RDPKRU	Read Protection Key Rights for User Pages	RDPKRU
RDPKRU		8086	RDPKRU	Reads PKRU into EAX.
;--------------------------------------------------------
GENERAL	RDPMC	Read Performance-Monitoring Counters	RDPMC
RDPMC		8086	RDPMC	Read performance-monitoring counter specified by ECX into EDX:EAX.
;--------------------------------------------------------
GENERAL	RDRAND	Read Random Number	RDRAND
RDRAND	R16	RDRAND	RDRAND R16	Read a 16-bit random number and store in the destination register.
RDRAND	R32	RDRAND	RDRAND R32	Read a 32-bit random number and store in the destination register.
RDRAND	R64	RDRAND	RDRAND R64	Read a 64-bit random number and store in the destination register.
;--------------------------------------------------------
GENERAL	RDSEED	Read Random SEED	RDSEED
RDSEED	R16	RDSEED	RDSEED R16	Read a 16-bit NIST SP800-90B & C compliant random value and store in the destination register.
RDSEED	R32	RDSEED	RDSEED R32	Read a 32-bit NIST SP800-90B & C compliant random value and store in the destination register.
RDSEED	R64	RDSEED	RDSEED R64	Read a 64-bit NIST SP800-90B & C compliant random value and store in the destination register.
;--------------------------------------------------------
GENERAL	RDTSC	Read Time-Stamp Counter	RDTSC
RDTSC		8086	RDTSC	Read time-stamp counter into EDX:EAX.
;--------------------------------------------------------
GENERAL	RDTSCP	Read Time-Stamp Counter and Processor ID	RDTSCP
RDTSCP		8086	RDTSCP	Read 64-bit time-stamp counter and IA32_TSC_AUX value into EDX:EAX and ECX.
;--------------------------------------------------------
GENERAL	REP_INS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REP_INS	M8,DX	8086	REP_INS M8,DX	Input (E)CX bytes from port DX into ES:[(E)DI].
REP_INS	M8,DX	8086	REP_INS M8,DX	Input RCX bytes from port DX into [RDI].
REP_INS	M16,DX	8086	REP_INS M16,DX	Input (E)CX words from port DX into ES:[(E)DI.]
REP_INS	M32,DX	386	REP_INS M32,DX	Input (E)CX doublewords from port DX into ES:[(E)DI].
REP_INS	R/M32,DX	386	REP_INS R/M32,DX	Input RCX default size from port DX into [RDI].
GENERAL	REP_MOVS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REP_MOVS	M8,M8	8086	REP_MOVS M8,M8	Move (E)CX bytes from DS:[(E)SI] to ES:[(E)DI].
REP_MOVS	M8,M8	8086	REP_MOVS M8,M8	Move RCX bytes from [RSI] to [RDI].
REP_MOVS	M16,M16	8086	REP_MOVS M16,M16	Move (E)CX words from DS:[(E)SI] to ES:[(E)DI].
REP_MOVS	M32,M32	386	REP_MOVS M32,M32	Move (E)CX doublewords from DS:[(E)SI] to ES:[(E)DI].
REP_MOVS	M64,M64	X64	REP_MOVS M64,M64	Move RCX quadwords from [RSI] to [RDI].
GENERAL	REP_OUTS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REP_OUTS	DX,R/M8	8086	REP_OUTS DX,R/M8	Output (E)CX bytes from DS:[(E)SI] to port DX.
REP_OUTS	DX,R/M8	8086	REP_OUTS DX,R/M8	Output RCX bytes from [RSI] to port DX.
REP_OUTS	DX,R/M16	8086	REP_OUTS DX,R/M16	Output (E)CX words from DS:[(E)SI] to port DX.
REP_OUTS	DX,R/M32	386	REP_OUTS DX,R/M32	Output (E)CX doublewords from DS:[(E)SI] to port DX.
REP_OUTS	DX,R/M32	386	REP_OUTS DX,R/M32	Output RCX default size from [RSI] to port DX.
GENERAL	REP_LODS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REP_LODS	AL	8086	REP_LODS AL	Load (E)CX bytes from DS:[(E)SI] to AL.
REP_LODS	AL	8086	REP_LODS AL	Load RCX bytes from [RSI] to AL.
REP_LODS	AX	8086	REP_LODS AX	Load (E)CX words from DS:[(E)SI] to AX.
REP_LODS	EAX	8086	REP_LODS EAX	Load (E)CX doublewords from DS:[(E)SI] to EAX.
REP_LODS	RAX	8086	REP_LODS RAX	Load RCX quadwords from [RSI] to RAX.
GENERAL	REP_STOS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REP_STOS	M8	8086	REP_STOS M8	Fill (E)CX bytes at ES:[(E)DI] with AL.
REP_STOS	M8	8086	REP_STOS M8	Fill RCX bytes at [RDI] with AL.
REP_STOS	M16	8086	REP_STOS M16	Fill (E)CX words at ES:[(E)DI] with AX.
REP_STOS	M32	386	REP_STOS M32	Fill (E)CX doublewords at ES:[(E)DI] with EAX.
REP_STOS	M64	X64	REP_STOS M64	Fill RCX quadwords at [RDI] with RAX.
GENERAL	REPE_CMPS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REPE_CMPS	M8,M8	8086	REPE_CMPS M8,M8	Find nonmatching bytes in ES:[(E)DI] and DS:[(E)SI].
REPE_CMPS	M8,M8	8086	REPE_CMPS M8,M8	Find non-matching bytes in [RDI] and [RSI].
REPE_CMPS	M16,M16	8086	REPE_CMPS M16,M16	Find nonmatching words in ES:[(E)DI] and DS:[(E)SI].
REPE_CMPS	M32,M32	386	REPE_CMPS M32,M32	Find nonmatching doublewords in ES:[(E)DI] and DS:[(E)SI].
REPE_CMPS	M64,M64	X64	REPE_CMPS M64,M64	Find non-matching quadwords in [RDI] and [RSI].
GENERAL	REPE_SCAS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REPE_SCAS	M8	8086	REPE_SCAS M8	Find non-AL byte starting at ES:[(E)DI].
REPE_SCAS	M8	8086	REPE_SCAS M8	Find non-AL byte starting at [RDI].
REPE_SCAS	M16	8086	REPE_SCAS M16	Find non-AX word starting at ES:[(E)DI].
REPE_SCAS	M32	386	REPE_SCAS M32	Find non-EAX doubleword starting at ES:[(E)DI].
REPE_SCAS	M64	X64	REPE_SCAS M64	Find non-RAX quadword starting at [RDI].
GENERAL	REPNE_CMPS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REPNE_CMPS	M8,M8	8086	REPNE_CMPS M8,M8	Find matching bytes in ES:[(E)DI] and DS:[(E)SI].
REPNE_CMPS	M8,M8	8086	REPNE_CMPS M8,M8	Find matching bytes in [RDI] and [RSI].
REPNE_CMPS	M16,M16	8086	REPNE_CMPS M16,M16	Find matching words in ES:[(E)DI] and DS:[(E)SI].
REPNE_CMPS	M32,M32	386	REPNE_CMPS M32,M32	Find matching doublewords in ES:[(E)DI] and DS:[(E)SI].
REPNE_CMPS	M64,M64	X64	REPNE_CMPS M64,M64	Find matching doublewords in [RDI] and [RSI].
GENERAL	REPNE_SCAS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ
REPNE_SCAS	M8	8086	REPNE_SCAS M8	Find AL, starting at ES:[(E)DI].
REPNE_SCAS	M8	8086	REPNE_SCAS M8	Find AL, starting at [RDI].
REPNE_SCAS	M16	8086	REPNE_SCAS M16	Find AX, starting at ES:[(E)DI].
REPNE_SCAS	M32	386	REPNE_SCAS M32	Find EAX, starting at ES:[(E)DI].
REPNE_SCAS	M64	X64	REPNE_SCAS M64	Find RAX, starting at [RDI].
;--------------------------------------------------------
GENERAL	RET	Return from Procedure	RET
RET		8086	RET	Near return to calling procedure.
RET		8086	RET	Far return to calling procedure.
RET	IMM16	8086	RET IMM16	Near return to calling procedure and pop imm16 bytes from stack.
RET	IMM16	8086	RET IMM16	Far return to calling procedure and pop imm16 bytes from stack.
;--------------------------------------------------------
GENERAL	RORX	Rotate Right Logical Without Affecting Flags	RORX
RORX	R32,R/M32,IMM8	BMI2	RORX R32,R/M32,IMM8	Rotate 32-bit r/m32 right imm8 times without affecting arithmetic flags.
RORX	R64,R/M64,IMM8	BMI2	RORX R64,R/M64,IMM8	Rotate 64-bit r/m64 right imm8 times without affecting arithmetic flags.
;--------------------------------------------------------
GENERAL	ROUNDPD	Round Packed Double Precision Floating-Point Values	ROUNDPD
ROUNDPD	XMM,XMM/M128,IMM8	SSE4_1	ROUNDPD XMM1,XMM2/M128,IMM8	Round packed double precision FP values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
GENERAL	VROUNDPD	Round Packed Double Precision Floating-Point Values	ROUNDPD
VROUNDPD	XMM,XMM/M128,IMM8	AVX	VROUNDPD XMM1,XMM2/M128,IMM8	Round packed DP FP values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VROUNDPD	YMM,YMM/M256,IMM8	AVX	VROUNDPD YMM1,YMM2/M256,IMM8	Round packed DP FP values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
;--------------------------------------------------------
GENERAL	ROUNDPS	Round Packed Single Precision Floating-Point Values	ROUNDPS
ROUNDPS	XMM,XMM/M128,IMM8	SSE4_1	ROUNDPS XMM1,XMM2/M128,IMM8	Round packed single precision FP values in xmm2/m128 and place the result in xmm1.  The rounding mode is determined by imm8.
GENERAL	VROUNDPS	Round Packed Single Precision Floating-Point Values	ROUNDPS
VROUNDPS	XMM,XMM/M128,IMM8	AVX	VROUNDPS XMM1,XMM2/M128,IMM8	Round packed SP FP values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VROUNDPS	YMM,YMM/M256,IMM8	AVX	VROUNDPS YMM1,YMM2/M256,IMM8	Round packed SP FP values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
;--------------------------------------------------------
GENERAL	ROUNDSD	Round Scalar Double Precision Floating-Point Values	ROUNDSD
ROUNDSD	XMM,XMM/M64,IMM8	SSE4_1	ROUNDSD XMM1,XMM2/M64,IMM8	Round the low packed double precision FP value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8.
GENERAL	VROUNDSD	Round Scalar Double Precision Floating-Point Values	ROUNDSD
VROUNDSD	XMM,XMM,XMM/M64,IMM8	AVX	VROUNDSD XMM1,XMM2,XMM3/M64,IMM8	Round the low packed double precision FP value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	ROUNDSS	Round Scalar Single Precision Floating-Point Values	ROUNDSS
ROUNDSS	XMM,XMM/M32,IMM8	SSE4_1	ROUNDSS XMM1,XMM2/M32,IMM8	Round the low packed single precision FP value in xmm2/m32 and place the result in xmm1.  The rounding mode is determined by imm8.
GENERAL	VROUNDSS	Round Scalar Single Precision Floating-Point Values	ROUNDSS
VROUNDSS	XMM,XMM,XMM/M32,IMM8	AVX	VROUNDSS XMM1,XMM2,XMM3/M32,IMM8	Round the low packed single precision FP value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	RSM	Resume from System Management Mode	RSM
RSM		8086	RSM	Resume operation of interrupted program.
;--------------------------------------------------------
GENERAL	RSQRTPS	Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values	RSQRTPS
RSQRTPS	XMM,XMM/M128	SSE	RSQRTPS XMM1,XMM2/M128	Computes the approximate reciprocals of the square roots of the packed SP FP values in xmm2/m128 and stores the results in xmm1.
GENERAL	VRSQRTPS	Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values	RSQRTPS
VRSQRTPS	XMM,XMM/M128	AVX	VRSQRTPS XMM1,XMM2/M128	Computes the approximate reciprocals of the square roots of packed SP values in xmm2/mem and stores the results in xmm1.
VRSQRTPS	YMM,YMM/M256	AVX	VRSQRTPS YMM1,YMM2/M256	Computes the approximate reciprocals of the square roots of packed SP values in ymm2/mem and stores the results in ymm1.
;--------------------------------------------------------
GENERAL	RSQRTSS	Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value	RSQRTSS
RSQRTSS	XMM,XMM/M32	SSE	RSQRTSS XMM1,XMM2/M32	Computes the approximate reciprocal of the square root of the low SP FP value in xmm2/m32 and stores the results in xmm1.
GENERAL	VRSQRTSS	Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value	RSQRTSS
VRSQRTSS	XMM,XMM,XMM/M32	AVX	VRSQRTSS XMM1,XMM2,XMM3/M32	Computes the approximate reciprocal of the square root of the low single precision FP value in xmm3/m32 and stores the results in xmm1. Also, upper single precision FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	SAHF	Store AH into Flags	SAHF
SAHF		8086	SAHF	Loads SF, ZF, AF, PF, and CF from AH into EFLAGS register.
;--------------------------------------------------------
GENERAL	SAL	Shift	SAL_SAR_SHL_SHR
SAL	R/M8,1	8086	SAL R/M8,1	Multiply r/m8 by 2, once.
SAL	R/M8,1	8086	SAL R/M8,1	Multiply r/m8 by 2, once.
SAL	R/M8,CL	8086	SAL R/M8,CL	Multiply r/m8 by 2, CL times.
SAL	R/M8,CL	8086	SAL R/M8,CL	Multiply r/m8 by 2, CL times.
SAL	R/M8,IMM8	8086	SAL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SAL	R/M8,IMM8	8086	SAL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SAL	R/M16,1	8086	SAL R/M16,1	Multiply r/m16 by 2, once.
SAL	R/M16,CL	8086	SAL R/M16,CL	Multiply r/m16 by 2, CL times.
SAL	R/M16,IMM8	8086	SAL R/M16,IMM8	Multiply r/m16 by 2, imm8 times.
SAL	R/M32,1	386	SAL R/M32,1	Multiply r/m32 by 2, once.
SAL	R/M64,1	X64	SAL R/M64,1	Multiply r/m64 by 2, once.
SAL	R/M32,CL	386	SAL R/M32,CL	Multiply r/m32 by 2, CL times.
SAL	R/M64,CL	X64	SAL R/M64,CL	Multiply r/m64 by 2, CL times.
SAL	R/M32,IMM8	386	SAL R/M32,IMM8	Multiply r/m32 by 2, imm8 times.
SAL	R/M64,IMM8	X64	SAL R/M64,IMM8	Multiply r/m64 by 2, imm8 times.
GENERAL	SAR	Shift	SAL_SAR_SHL_SHR
SAR	R/M8,1	8086	SAR R/M8,1	Signed divide* r/m8 by 2, once.
SAR	R/M8,1	8086	SAR R/M8,1	Signed divide* r/m8 by 2, once.
SAR	R/M8,CL	8086	SAR R/M8,CL	Signed divide* r/m8 by 2, CL times.
SAR	R/M8,CL	8086	SAR R/M8,CL	Signed divide* r/m8 by 2, CL times.
SAR	R/M8,IMM8	8086	SAR R/M8,IMM8	Signed divide* r/m8 by 2, imm8 time.
SAR	R/M8,IMM8	8086	SAR R/M8,IMM8	Signed divide* r/m8 by 2, imm8 times.
SAR	R/M16,1	8086	SAR R/M16,1	Signed divide* r/m16 by 2, once.
SAR	R/M16,CL	8086	SAR R/M16,CL	Signed divide* r/m16 by 2, CL times.
SAR	R/M16,IMM8	8086	SAR R/M16,IMM8	Signed divide* r/m16 by 2, imm8 times.
SAR	R/M32,1	386	SAR R/M32,1	Signed divide* r/m32 by 2, once.
SAR	R/M64,1	X64	SAR R/M64,1	Signed divide* r/m64 by 2, once.
SAR	R/M32,CL	386	SAR R/M32,CL	Signed divide* r/m32 by 2, CL times.
SAR	R/M64,CL	X64	SAR R/M64,CL	Signed divide* r/m64 by 2, CL times.
SAR	R/M32,IMM8	386	SAR R/M32,IMM8	Signed divide* r/m32 by 2, imm8 times.
SAR	R/M64,IMM8	X64	SAR R/M64,IMM8	Signed divide* r/m64 by 2, imm8 times
GENERAL	SHL	Shift	SAL_SAR_SHL_SHR
SHL	R/M8,1	8086	SHL R/M8,1	Multiply r/m8 by 2, once.
SHL	R/M8,1	8086	SHL R/M8,1	Multiply r/m8 by 2, once.
SHL	R/M8,CL	8086	SHL R/M8,CL	Multiply r/m8 by 2, CL times.
SHL	R/M8,CL	8086	SHL R/M8,CL	Multiply r/m8 by 2, CL times.
SHL	R/M8,IMM8	8086	SHL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SHL	R/M8,IMM8	8086	SHL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SHL	R/M16,1	8086	SHL R/M16,1	Multiply r/m16 by 2, once.
SHL	R/M16,CL	8086	SHL R/M16,CL	Multiply r/m16 by 2, CL times.
SHL	R/M16,IMM8	8086	SHL R/M16,IMM8	Multiply r/m16 by 2, imm8 times.
SHL	R/M32,1	386	SHL R/M32,1	Multiply r/m32 by 2, once.
SHL	R/M64,1	X64	SHL R/M64,1	Multiply r/m64 by 2, once.
SHL	R/M32,CL	386	SHL R/M32,CL	Multiply r/m32 by 2, CL times.
SHL	R/M64,CL	X64	SHL R/M64,CL	Multiply r/m64 by 2, CL times.
SHL	R/M32,IMM8	386	SHL R/M32,IMM8	Multiply r/m32 by 2, imm8 times.
SHL	R/M64,IMM8	X64	SHL R/M64,IMM8	Multiply r/m64 by 2, imm8 times.
GENERAL	SHR	Shift	SAL_SAR_SHL_SHR
SHR	R/M8,1	8086	SHR R/M8,1	Unsigned divide r/m8 by 2, once.
SHR	R/M8,1	8086	SHR R/M8,1	Unsigned divide r/m8 by 2, once.
SHR	R/M8,CL	8086	SHR R/M8,CL	Unsigned divide r/m8 by 2, CL times.
SHR	R/M8,CL	8086	SHR R/M8,CL	Unsigned divide r/m8 by 2, CL times.
SHR	R/M8,IMM8	8086	SHR R/M8,IMM8	Unsigned divide r/m8 by 2, imm8 times.
SHR	R/M8,IMM8	8086	SHR R/M8,IMM8	Unsigned divide r/m8 by 2, imm8 times.
SHR	R/M16,1	8086	SHR R/M16,1	Unsigned divide r/m16 by 2, once.
SHR	R/M16,CL	8086	SHR R/M16,CL	Unsigned divide r/m16 by 2, CL times
SHR	R/M16,IMM8	8086	SHR R/M16,IMM8	Unsigned divide r/m16 by 2, imm8 times.
SHR	R/M32,1	386	SHR R/M32,1	Unsigned divide r/m32 by 2, once.
SHR	R/M64,1	X64	SHR R/M64,1	Unsigned divide r/m64 by 2, once.
SHR	R/M32,CL	386	SHR R/M32,CL	Unsigned divide r/m32 by 2, CL times.
SHR	R/M64,CL	X64	SHR R/M64,CL	Unsigned divide r/m64 by 2, CL times.
SHR	R/M32,IMM8	386	SHR R/M32,IMM8	Unsigned divide r/m32 by 2, imm8 times.
SHR	R/M64,IMM8	X64	SHR R/M64,IMM8	Unsigned divide r/m64 by 2, imm8 times.
;--------------------------------------------------------
GENERAL	SARX	Shift Without Affecting Flags	SARX_SHLX_SHRX
SARX	R32,R/M32,R32	BMI2	SARX R32A,R/M32,R32B	Shift r/m32 arithmetically right with count specified in r32b.
SARX	R64,R/M64,R64	BMI2	SARX R64A,R/M64,R64B	Shift r/m64 arithmetically right with count specified in r64b.
GENERAL	SHLX	Shift Without Affecting Flags	SARX_SHLX_SHRX
SHLX	R32,R/M32,R32	BMI2	SHLX R32A,R/M32,R32B	Shift r/m32 logically left with count specified in r32b.
SHLX	R64,R/M64,R64	BMI2	SHLX R64A,R/M64,R64B	Shift r/m64 logically left with count specified in r64b.
GENERAL	SHRX	Shift Without Affecting Flags	SARX_SHLX_SHRX
SHRX	R32,R/M32,R32	BMI2	SHRX R32A,R/M32,R32B	Shift r/m32 logically right with count specified in r32b.
SHRX	R64,R/M64,R64	BMI2	SHRX R64A,R/M64,R64B	Shift r/m64 logically right with count specified in r64b.
;--------------------------------------------------------
GENERAL	SBB	Integer Subtraction with Borrow	SBB
SBB	AL,IMM8	8086	SBB AL,IMM8	Subtract with borrow imm8 from AL.
SBB	AX,IMM16	8086	SBB AX,IMM16	Subtract with borrow imm16 from AX.
SBB	EAX,IMM32	386	SBB EAX,IMM32	Subtract with borrow imm32 from EAX.
SBB	RAX,IMM32	386	SBB RAX,IMM32	Subtract with borrow sign-extended imm.32 to 64-bits from RAX.
SBB	R/M8,IMM8	8086	SBB R/M8,IMM8	Subtract with borrow imm8 from r/m8.
SBB	R/M8,IMM8	8086	SBB R/M8,IMM8	Subtract with borrow imm8 from r/m8.
SBB	R/M16,IMM16	8086	SBB R/M16,IMM16	Subtract with borrow imm16 from r/m16.
SBB	R/M32,IMM32	386	SBB R/M32,IMM32	Subtract with borrow imm32 from r/m32.
SBB	R/M64,IMM32	X64	SBB R/M64,IMM32	Subtract with borrow sign-extended imm32 to 64-bits from r/m64.
SBB	R/M16,IMM8	8086	SBB R/M16,IMM8	Subtract with borrow sign-extended imm8 from r/m16.
SBB	R/M32,IMM8	386	SBB R/M32,IMM8	Subtract with borrow sign-extended imm8 from r/m32.
SBB	R/M64,IMM8	X64	SBB R/M64,IMM8	Subtract with borrow sign-extended imm8 from r/m64.
SBB	R/M8,R8	8086	SBB R/M8,R8	Subtract with borrow r8 from r/m8.
SBB	R/M8,R8	8086	SBB R/M8,R8	Subtract with borrow r8 from r/m8.
SBB	R/M16,R16	8086	SBB R/M16,R16	Subtract with borrow r16 from r/m16.
SBB	R/M32,R32	386	SBB R/M32,R32	Subtract with borrow r32 from r/m32.
SBB	R/M64,R64	X64	SBB R/M64,R64	Subtract with borrow r64 from r/m64.
SBB	R8,R/M8	8086	SBB R8,R/M8	Subtract with borrow r/m8 from r8.
SBB	R8,R/M8	8086	SBB R8,R/M8	Subtract with borrow r/m8 from r8.
SBB	R16,R/M16	8086	SBB R16,R/M16	Subtract with borrow r/m16 from r16.
SBB	R32,R/M32	386	SBB R32,R/M32	Subtract with borrow r/m32 from r32.
SBB	R64,R/M64	X64	SBB R64,R/M64	Subtract with borrow r/m64 from r64.
;--------------------------------------------------------
GENERAL	SCAS	Scan String	SCAS_SCASB_SCASW_SCASD
SCAS	M8	8086	SCAS M8	Compare AL with byte at ES:(E)DI or RDI, then set status flags.*
SCAS	M16	8086	SCAS M16	Compare AX with word at ES:(E)DI or RDI, then set status flags.*
SCAS	M32	386	SCAS M32	Compare EAX with doubleword at ES(E)DI or RDI then set status flags.*
SCAS	M64	X64	SCAS M64	Compare RAX with quadword at RDI or EDI then set status flags.
GENERAL	SCASB	Scan String	SCAS_SCASB_SCASW_SCASD
SCASB		8086	SCASB	Compare AL with byte at ES:(E)DI or RDI then set status flags.*
GENERAL	SCASW	Scan String	SCAS_SCASB_SCASW_SCASD
SCASW		8086	SCASW	Compare AX with word at ES:(E)DI or RDI then set status flags.*
GENERAL	SCASD	Scan String	SCAS_SCASB_SCASW_SCASD
SCASD		8086	SCASD	Compare EAX with doubleword at ES:(E)DI or RDI then set status flags.*
GENERAL	SCASQ	Scan String	SCAS_SCASB_SCASW_SCASD
SCASQ		8086	SCASQ	Compare RAX with quadword at RDI or EDI then set status flags.
;--------------------------------------------------------
GENERAL	SETA	Set Byte on Condition	SETcc
SETA	R/M8	8086	SETA R/M8	Set byte if above (CF=0 and ZF=0).
SETA	R/M8	8086	SETA R/M8	Set byte if above (CF=0 and ZF=0).
GENERAL	SETAE	Set Byte on Condition	SETcc
SETAE	R/M8	8086	SETAE R/M8	Set byte if above or equal (CF=0).
SETAE	R/M8	8086	SETAE R/M8	Set byte if above or equal (CF=0).
GENERAL	SETB	Set Byte on Condition	SETcc
SETB	R/M8	8086	SETB R/M8	Set byte if below (CF=1).
SETB	R/M8	8086	SETB R/M8	Set byte if below (CF=1).
GENERAL	SETBE	Set Byte on Condition	SETcc
SETBE	R/M8	8086	SETBE R/M8	Set byte if below or equal (CF=1 or ZF=1).
SETBE	R/M8	8086	SETBE R/M8	Set byte if below or equal (CF=1 or ZF=1).
GENERAL	SETC	Set Byte on Condition	SETcc
SETC	R/M8	8086	SETC R/M8	Set byte if carry (CF=1).
SETC	R/M8	8086	SETC R/M8	Set byte if carry (CF=1).
GENERAL	SETE	Set Byte on Condition	SETcc
SETE	R/M8	8086	SETE R/M8	Set byte if equal (ZF=1).
SETE	R/M8	8086	SETE R/M8	Set byte if equal (ZF=1).
GENERAL	SETG	Set Byte on Condition	SETcc
SETG	R/M8	8086	SETG R/M8	Set byte if greater (ZF=0 and SF=OF).
SETG	R/M8	8086	SETG R/M8	Set byte if greater (ZF=0 and SF=OF).
GENERAL	SETGE	Set Byte on Condition	SETcc
SETGE	R/M8	8086	SETGE R/M8	Set byte if greater or equal (SF=OF).
SETGE	R/M8	8086	SETGE R/M8	Set byte if greater or equal (SF=OF).
GENERAL	SETL	Set Byte on Condition	SETcc
SETL	R/M8	8086	SETL R/M8	Set byte if less (SF≠ OF).
SETL	R/M8	8086	SETL R/M8	Set byte if less (SF≠ OF).
GENERAL	SETLE	Set Byte on Condition	SETcc
SETLE	R/M8	8086	SETLE R/M8	Set byte if less or equal (ZF=1 or SF≠ OF).
SETLE	R/M8	8086	SETLE R/M8	Set byte if less or equal (ZF=1 or SF≠ OF).
GENERAL	SETNA	Set Byte on Condition	SETcc
SETNA	R/M8	8086	SETNA R/M8	Set byte if not above (CF=1 or ZF=1).
SETNA	R/M8	8086	SETNA R/M8	Set byte if not above (CF=1 or ZF=1).
GENERAL	SETNAE	Set Byte on Condition	SETcc
SETNAE	R/M8	8086	SETNAE R/M8	Set byte if not above or equal (CF=1).
SETNAE	R/M8	8086	SETNAE R/M8	Set byte if not above or equal (CF=1).
GENERAL	SETNB	Set Byte on Condition	SETcc
SETNB	R/M8	8086	SETNB R/M8	Set byte if not below (CF=0).
SETNB	R/M8	8086	SETNB R/M8	Set byte if not below (CF=0).
GENERAL	SETNBE	Set Byte on Condition	SETcc
SETNBE	R/M8	8086	SETNBE R/M8	Set byte if not below or equal (CF=0 and ZF=0).
SETNBE	R/M8	8086	SETNBE R/M8	Set byte if not below or equal (CF=0 and ZF=0).
GENERAL	SETNC	Set Byte on Condition	SETcc
SETNC	R/M8	8086	SETNC R/M8	Set byte if not carry (CF=0).
SETNC	R/M8	8086	SETNC R/M8	Set byte if not carry (CF=0).
GENERAL	SETNE	Set Byte on Condition	SETcc
SETNE	R/M8	8086	SETNE R/M8	Set byte if not equal (ZF=0).
SETNE	R/M8	8086	SETNE R/M8	Set byte if not equal (ZF=0).
GENERAL	SETNG	Set Byte on Condition	SETcc
SETNG	R/M8	8086	SETNG R/M8	Set byte if not greater (ZF=1 or SF≠ OF)
SETNG	R/M8	8086	SETNG R/M8	Set byte if not greater (ZF=1 or SF≠ OF).
GENERAL	SETNGE	Set Byte on Condition	SETcc
SETNGE	R/M8	8086	SETNGE R/M8	Set byte if not greater or equal (SF≠ OF).
SETNGE	R/M8	8086	SETNGE R/M8	Set byte if not greater or equal (SF≠ OF).
GENERAL	SETNL	Set Byte on Condition	SETcc
SETNL	R/M8	8086	SETNL R/M8	Set byte if not less (SF=OF).
SETNL	R/M8	8086	SETNL R/M8	Set byte if not less (SF=OF).
GENERAL	SETNLE	Set Byte on Condition	SETcc
SETNLE	R/M8	8086	SETNLE R/M8	Set byte if not less or equal (ZF=0 and SF=OF).
SETNLE	R/M8	8086	SETNLE R/M8	Set byte if not less or equal (ZF=0 and SF=OF).
GENERAL	SETNO	Set Byte on Condition	SETcc
SETNO	R/M8	8086	SETNO R/M8	Set byte if not overflow (OF=0).
SETNO	R/M8	8086	SETNO R/M8	Set byte if not overflow (OF=0).
GENERAL	SETNP	Set Byte on Condition	SETcc
SETNP	R/M8	8086	SETNP R/M8	Set byte if not parity (PF=0).
SETNP	R/M8	8086	SETNP R/M8	Set byte if not parity (PF=0).
GENERAL	SETNS	Set Byte on Condition	SETcc
SETNS	R/M8	8086	SETNS R/M8	Set byte if not sign (SF=0).
SETNS	R/M8	8086	SETNS R/M8	Set byte if not sign (SF=0).
GENERAL	SETNZ	Set Byte on Condition	SETcc
SETNZ	R/M8	8086	SETNZ R/M8	Set byte if not zero (ZF=0).
SETNZ	R/M8	8086	SETNZ R/M8	Set byte if not zero (ZF=0).
GENERAL	SETO	Set Byte on Condition	SETcc
SETO	R/M8	8086	SETO R/M8	Set byte if overflow (OF=1)
SETO	R/M8	8086	SETO R/M8	Set byte if overflow (OF=1).
GENERAL	SETP	Set Byte on Condition	SETcc
SETP	R/M8	8086	SETP R/M8	Set byte if parity (PF=1).
SETP	R/M8	8086	SETP R/M8	Set byte if parity (PF=1).
GENERAL	SETPE	Set Byte on Condition	SETcc
SETPE	R/M8	8086	SETPE R/M8	Set byte if parity even (PF=1).
SETPE	R/M8	8086	SETPE R/M8	Set byte if parity even (PF=1).
GENERAL	SETPO	Set Byte on Condition	SETcc
SETPO	R/M8	8086	SETPO R/M8	Set byte if parity odd (PF=0).
SETPO	R/M8	8086	SETPO R/M8	Set byte if parity odd (PF=0).
GENERAL	SETS	Set Byte on Condition	SETcc
SETS	R/M8	8086	SETS R/M8	Set byte if sign (SF=1).
SETS	R/M8	8086	SETS R/M8	Set byte if sign (SF=1).
GENERAL	SETZ	Set Byte on Condition	SETcc
SETZ	R/M8	8086	SETZ R/M8	Set byte if zero (ZF=1).
SETZ	R/M8	8086	SETZ R/M8	Set byte if zero (ZF=1).
;--------------------------------------------------------
GENERAL	SFENCE	Store Fence	SFENCE
SFENCE		8086	SFENCE	Serializes store operations.
;--------------------------------------------------------
GENERAL	SGDT	Store Global Descriptor Table Register	SGDT
SGDT	M	8086	SGDT M	Store GDTR to m.
;--------------------------------------------------------
GENERAL	SHA1MSG1	Perform an Intermediate Calculation for the Next Four SHA1 Message Dwords	SHA1MSG1
SHA1MSG1	XMM,XMM/M128	SHA	SHA1MSG1 XMM1,XMM2/M128	Performs an intermediate calculation for the next four SHA1 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
;--------------------------------------------------------
GENERAL	SHA1MSG2	Perform a Final Calculation for the Next Four SHA1 Message Dwords	SHA1MSG2
SHA1MSG2	XMM,XMM/M128	SHA	SHA1MSG2 XMM1,XMM2/M128	Performs the final calculation for the next four SHA1 message dwords using intermediate results from xmm1 and the previous message dwords from xmm2/m128, storing the result in xmm1.
;--------------------------------------------------------
GENERAL	SHA1NEXTE	Calculate SHA1 State Variable E after Four Rounds	SHA1NEXTE
SHA1NEXTE	XMM,XMM/M128	SHA	SHA1NEXTE XMM1,XMM2/M128	Calculates SHA1 state variable E after four rounds of operation from the current SHA1 state variable A in xmm1. The calculated value of the SHA1 state variable E is added to the scheduled dwords in xmm2/m128, and stored with some of the scheduled dwords in xmm1.
;--------------------------------------------------------
GENERAL	SHA1RNDS4	Perform Four Rounds of SHA1 Operation	SHA1RNDS4
SHA1RNDS4	XMM,XMM/M128,IMM8	SHA	SHA1RNDS4 XMM1,XMM2/M128,IMM8	Performs four rounds of SHA1 operation operating on SHA1 state (A,B,C,D) from xmm1, with a pre-computed sum of the next 4 round message dwords and state variable E from xmm2/m128. The immediate byte controls logic functions and round constants.
;--------------------------------------------------------
GENERAL	SHA256MSG1	Perform an Intermediate Calculation for the Next Four SHA256 Message Dwords	SHA256MSG1
SHA256MSG1	XMM,XMM/M128	SHA	SHA256MSG1 XMM1,XMM2/M128	Performs an intermediate calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
;--------------------------------------------------------
GENERAL	SHA256MSG2	Perform a Final Calculation for the Next Four SHA256 Message Dwords	SHA256MSG2
SHA256MSG2	XMM,XMM/M128	SHA	SHA256MSG2 XMM1,XMM2/M128	Performs the final calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
;--------------------------------------------------------
GENERAL	SHA256RNDS2	Perform Two Rounds of SHA256 Operation	SHA256RNDS2
SHA256RNDS2	XMM,XMM/M128,XMM_ZERO	SHA	SHA256RNDS2 XMM1,XMM2/M128,<XMM0>	Perform 2 rounds of SHA256 operation using an initial SHA256 state (C,D,G,H) from xmm1, an initial SHA256 state (A,B,E,F) from xmm2/m128, and a pre-computed sum of the next 2 round mes- sage dwords and the corresponding round constants from the implicit operand XMM0, storing the updated SHA256 state (A,B,E,F) result in xmm1.
;--------------------------------------------------------
GENERAL	SHLD	Double Precision Shift Left	SHLD
SHLD	R/M16,R16,IMM8	8086	SHLD R/M16,R16,IMM8	Shift r/m16 to left imm8 places while shifting bits from r16 in from the right.
SHLD	R/M16,R16,CL	8086	SHLD R/M16,R16,CL	Shift r/m16 to left CL places while shifting bits from r16 in from the right.
SHLD	R/M32,R32,IMM8	386	SHLD R/M32,R32,IMM8	Shift r/m32 to left imm8 places while shifting bits from r32 in from the right.
SHLD	R/M64,R64,IMM8	X64	SHLD R/M64,R64,IMM8	Shift r/m64 to left imm8 places while shifting bits from r64 in from the right.
SHLD	R/M32,R32,CL	386	SHLD R/M32,R32,CL	Shift r/m32 to left CL places while shifting bits from r32 in from the right.
SHLD	R/M64,R64,CL	X64	SHLD R/M64,R64,CL	Shift r/m64 to left CL places while shifting bits from r64 in from the right.
;--------------------------------------------------------
GENERAL	SHRD	Double Precision Shift Right	SHRD
SHRD	R/M16,R16,IMM8	8086	SHRD R/M16,R16,IMM8	Shift r/m16 to right imm8 places while shifting bits from r16 in from the left.
SHRD	R/M16,R16,CL	8086	SHRD R/M16,R16,CL	Shift r/m16 to right CL places while shifting bits from r16 in from the left.
SHRD	R/M32,R32,IMM8	386	SHRD R/M32,R32,IMM8	Shift r/m32 to right imm8 places while shifting bits from r32 in from the left.
SHRD	R/M64,R64,IMM8	X64	SHRD R/M64,R64,IMM8	Shift r/m64 to right imm8 places while shifting bits from r64 in from the left.
SHRD	R/M32,R32,CL	386	SHRD R/M32,R32,CL	Shift r/m32 to right CL places while shifting bits from r32 in from the left.
SHRD	R/M64,R64,CL	X64	SHRD R/M64,R64,CL	Shift r/m64 to right CL places while shifting bits from r64 in from the left.
;--------------------------------------------------------
GENERAL	SHUFPD	Packed Interleave Shuffle of Pairs of Double-Precision Floating-Point Values	SHUFPD
SHUFPD	XMM,XMM/M128,IMM8	SSE2	SHUFPD XMM1,XMM2/M128,IMM8	Shuffle two pairs of DP FP values from xmm1 and xmm2/m128 using imm8 to select from each pair, interleaved result is stored in xmm1.
GENERAL	VSHUFPD	Packed Interleave Shuffle of Pairs of Double-Precision Floating-Point Values	SHUFPD
VSHUFPD	XMM,XMM,XMM/M128,IMM8	AVX	VSHUFPD XMM1,XMM2,XMM3/M128,IMM8	Shuffle two pairs of DP FP values from xmm2 and xmm3/m128 using imm8 to select from each pair, interleaved result is stored in xmm1.
VSHUFPD	YMM,YMM,YMM/M256,IMM8	AVX	VSHUFPD YMM1,YMM2,YMM3/M256,IMM8	Shuffle four pairs of DP FP values from ymm2 and ymm3/m256 using imm8 to select from each pair, interleaved result is stored in xmm1.
VSHUFPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VSHUFPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Shuffle two paris of DP FP values from xmm2 and xmm3/m128/m64bcst using imm8 to select from each pair. store interleaved results in xmm1 subject to writemask k1.
VSHUFPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VSHUFPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle four paris of DP FP values from ymm2 and ymm3/m256/m64bcst using imm8 to select from each pair. store interleaved results in ymm1 subject to writemask k1.
VSHUFPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VSHUFPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle eight paris of DP FP values from zmm2 and zmm3/m512/m64bcst using imm8 to select from each pair. store interleaved results in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	SHUFPS	Packed Interleave Shuffle of Quadruplets of Single-Precision Floating-Point Values	SHUFPS
SHUFPS	XMM,XMM/M128,IMM8	SSE	SHUFPS XMM1,XMM3/M128,IMM8	Select from quadruplet of SP FP values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1.
GENERAL	VSHUFPS	Packed Interleave Shuffle of Quadruplets of Single-Precision Floating-Point Values	SHUFPS
VSHUFPS	XMM,XMM,XMM/M128,IMM8	AVX	VSHUFPS XMM1,XMM2,XMM3/M128,IMM8	Select from quadruplet of SP FP values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1.
VSHUFPS	YMM,YMM,YMM/M256,IMM8	AVX	VSHUFPS YMM1,YMM2,YMM3/M256,IMM8	Select from quadruplet of SP FP values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1.
VSHUFPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VSHUFPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Select from quadruplet of SP FP values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1, subject to writemask k1.
VSHUFPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VSHUFPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Select from quadruplet of SP FP values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1, subject to writemask k1.
VSHUFPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VSHUFPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Select from quadruplet of SP FP values in zmm2 and zmm3/m512 using imm8, interleaved result pairs are stored in zmm1, subject to writemask k1.
;--------------------------------------------------------
GENERAL	SIDT	Store Interrupt Descriptor Table Register	SIDT
SIDT	M	8086	SIDT M	Store IDTR to m.
;--------------------------------------------------------
GENERAL	SLDT	Store Local Descriptor Table Register	SLDT
SLDT	R/M16	8086	SLDT R/M16	Stores segment selector from LDTR in r/m16.
SLDT	R64/M16	X64	SLDT R64/M16	Stores segment selector from LDTR in r64/m16.
;--------------------------------------------------------
GENERAL	SMSW	Store Machine Status Word	SMSW
SMSW	R/M16	8086	SMSW R/M16	Store machine status word to r/m16.
SMSW	R32/M16	386	SMSW R32/M16	Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined.
SMSW	R64/M16	X64	SMSW R64/M16	Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined.
;--------------------------------------------------------
GENERAL	SQRTPD	Square Root of Double-Precision Floating-Point Values	SQRTPD
SQRTPD	XMM,XMM/M128	SSE2	SQRTPD XMM1,XMM2/M128	Computes Square Roots of the packed DP FP values in xmm2/m128 and stores the result in xmm1.
GENERAL	VSQRTPD	Square Root of Double-Precision Floating-Point Values	SQRTPD
VSQRTPD	XMM,XMM/M128	AVX	VSQRTPD XMM1,XMM2/M128	Computes Square Roots of the packed DP FP values in xmm2/m128 and stores the result in xmm1.
VSQRTPD	YMM,YMM/M256	AVX	VSQRTPD YMM1,YMM2/M256	Computes Square Roots of the packed DP FP values in ymm2/m256 and stores the result in ymm1.
VSQRTPD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VSQRTPD XMM1{K1}{Z},XMM2/M128/M64BCST	Computes Square Roots of the packed DP FP values in xmm2/m128/m64bcst and stores the result in xmm1 subject to writemask k1.
VSQRTPD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VSQRTPD YMM1{K1}{Z},YMM2/M256/M64BCST	Computes Square Roots of the packed DP FP values in ymm2/m256/m64bcst and stores the result in ymm1 subject to writemask k1.
VSQRTPD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_F	VSQRTPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Computes Square Roots of the packed DP FP values in zmm2/m512/m64bcst and stores the result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	SQRTPS	Square Root of Single-Precision Floating-Point Values	SQRTPS
SQRTPS	XMM,XMM/M128	SSE	SQRTPS XMM1,XMM2/M128	Computes Square Roots of the packed SP FP values in xmm2/m128 and stores the result in xmm1.
GENERAL	VSQRTPS	Square Root of Single-Precision Floating-Point Values	SQRTPS
VSQRTPS	XMM,XMM/M128	AVX	VSQRTPS XMM1,XMM2/M128	Computes Square Roots of the packed SP FP values in xmm2/m128 and stores the result in xmm1.
VSQRTPS	YMM,YMM/M256	AVX	VSQRTPS YMM1,YMM2/M256	Computes Square Roots of the packed SP FP values in ymm2/m256 and stores the result in ymm1.
VSQRTPS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VSQRTPS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes Square Roots of the packed SP FP values in xmm2/m128/m32bcst and stores the result in xmm1 subject to writemask k1.
VSQRTPS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VSQRTPS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes Square Roots of the packed SP FP values in ymm2/m256/m32bcst and stores the result in ymm1 subject to writemask k1.
VSQRTPS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512_F	VSQRTPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Computes Square Roots of the packed SP FP values in zmm2/m512/m32bcst and stores the result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	SQRTSD	Compute Square Root of Scalar Double-Precision Floating-Point Value	SQRTSD
SQRTSD	XMM,XMM/M64	SSE2	SQRTSD XMM1,XMM2/M64	Computes square root of the low DP FP value in xmm2/m64 and stores the results in xmm1.
GENERAL	VSQRTSD	Compute Square Root of Scalar Double-Precision Floating-Point Value	SQRTSD
VSQRTSD	XMM,XMM,XMM/M64	AVX	VSQRTSD XMM1,XMM2,XMM3/M64	Computes square root of the low DP FP value in xmm3/m64 and stores the results in xmm1. Also, upper DP FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VSQRTSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VSQRTSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Computes square root of the low DP FP value in xmm3/m64 and stores the results in xmm1 under writemask k1. Also, upper DP FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	SQRTSS	Compute Square Root of Scalar Single-Precision Value	SQRTSS
SQRTSS	XMM,XMM/M32	SSE	SQRTSS XMM1,XMM2/M32	Computes square root of the low SP FP value in xmm2/m32 and stores the results in xmm1.
GENERAL	VSQRTSS	Compute Square Root of Scalar Single-Precision Value	SQRTSS
VSQRTSS	XMM,XMM,XMM/M32	AVX	VSQRTSS XMM1,XMM2,XMM3/M32	Computes square root of the low SP FP value in xmm3/m32 and stores the results in xmm1. Also, upper SP FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VSQRTSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VSQRTSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Computes square root of the low SP FP value in xmm3/m32 and stores the results in xmm1 under writemask k1. Also, upper SP FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	STAC	Set AC Flag in EFLAGS Register	STAC
STAC			STAC	Set the AC flag in the EFLAGS register.
;--------------------------------------------------------
GENERAL	STC	Set Carry Flag	STC
STC		8086	STC	Set CF flag.
;--------------------------------------------------------
GENERAL	STD	Set Direction Flag	STD
STD		8086	STD	Set DF flag.
;--------------------------------------------------------
GENERAL	STI	Set Interrupt Flag	STI
STI		8086	STI	Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction.
;--------------------------------------------------------
GENERAL	STMXCSR	Store MXCSR Register State	STMXCSR
STMXCSR	M32	SSE	STMXCSR M32	Store contents of MXCSR register to m32.
GENERAL	VSTMXCSR	Store MXCSR Register State	STMXCSR
VSTMXCSR	M32	AVX	VSTMXCSR M32	Store contents of MXCSR register to m32.
;--------------------------------------------------------
GENERAL	STOS	Store String	STOS_STOSB_STOSW_STOSD_STOSQ
STOS	M8	8086	STOS M8	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
STOS	M16	8086	STOS M16	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
STOS	M32	386	STOS M32	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
STOS	M64	X64	STOS M64	Store RAX at address RDI or EDI.
GENERAL	STOSB	Store String	STOS_STOSB_STOSW_STOSD_STOSQ
STOSB		8086	STOSB	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
GENERAL	STOSW	Store String	STOS_STOSB_STOSW_STOSD_STOSQ
STOSW		8086	STOSW	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
GENERAL	STOSD	Store String	STOS_STOSB_STOSW_STOSD_STOSQ
STOSD		8086	STOSD	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
GENERAL	STOSQ	Store String	STOS_STOSB_STOSW_STOSD_STOSQ
STOSQ		8086	STOSQ	Store RAX at address RDI or EDI.
;--------------------------------------------------------
GENERAL	STR	Store Task Register	STR
STR	R/M16	8086	STR R/M16	Stores segment selector from TR in r/m16.
;--------------------------------------------------------
GENERAL	SUB	Subtract	SUB
SUB	AL,IMM8	8086	SUB AL,IMM8	Subtract imm8 from AL.
SUB	AX,IMM16	8086	SUB AX,IMM16	Subtract imm16 from AX.
SUB	EAX,IMM32	386	SUB EAX,IMM32	Subtract imm32 from EAX.
SUB	RAX,IMM32	386	SUB RAX,IMM32	Subtract imm32 sign-extended to 64-bits from RAX.
SUB	R/M8,IMM8	8086	SUB R/M8,IMM8	Subtract imm8 from r/m8.
SUB	R/M8,IMM8	8086	SUB R/M8,IMM8	Subtract imm8 from r/m8.
SUB	R/M16,IMM16	8086	SUB R/M16,IMM16	Subtract imm16 from r/m16.
SUB	R/M32,IMM32	386	SUB R/M32,IMM32	Subtract imm32 from r/m32.
SUB	R/M64,IMM32	X64	SUB R/M64,IMM32	Subtract imm32 sign-extended to 64-bits from r/m64.
SUB	R/M16,IMM8	8086	SUB R/M16,IMM8	Subtract sign-extended imm8 from r/m16.
SUB	R/M32,IMM8	386	SUB R/M32,IMM8	Subtract sign-extended imm8 from r/m32.
SUB	R/M64,IMM8	X64	SUB R/M64,IMM8	Subtract sign-extended imm8 from r/m64.
SUB	R/M8,R8	8086	SUB R/M8,R8	Subtract r8 from r/m8.
SUB	R/M8,R8	8086	SUB R/M8,R8	Subtract r8 from r/m8.
SUB	R/M16,R16	8086	SUB R/M16,R16	Subtract r16 from r/m16.
SUB	R/M32,R32	386	SUB R/M32,R32	Subtract r32 from r/m32.
SUB	R/M64,R64	X64	SUB R/M64,R64	Subtract r64 from r/m64.
SUB	R8,R/M8	8086	SUB R8,R/M8	Subtract r/m8 from r8.
SUB	R8,R/M8	8086	SUB R8,R/M8	Subtract r/m8 from r8.
SUB	R16,R/M16	8086	SUB R16,R/M16	Subtract r/m16 from r16.
SUB	R32,R/M32	386	SUB R32,R/M32	Subtract r/m32 from r32.
SUB	R64,R/M64	X64	SUB R64,R/M64	Subtract r/m64 from r64.
;--------------------------------------------------------
GENERAL	SUBPD	Subtract Packed Double-Precision Floating-Point Values	SUBPD
SUBPD	XMM,XMM/M128	SSE2	SUBPD XMM1,XMM2/M128	Subtract packed DP FP values in xmm2/mem from xmm1 and store result in xmm1.
GENERAL	VSUBPD	Subtract Packed Double-Precision Floating-Point Values	SUBPD
VSUBPD	XMM,XMM,XMM/M128	AVX	VSUBPD XMM1,XMM2,XMM3/M128	Subtract packed DP FP values in xmm3/mem from xmm2 and store result in xmm1.
VSUBPD	YMM,YMM,YMM/M256	AVX	VSUBPD YMM1,YMM2,YMM3/M256	Subtract packed DP FP values in ymm3/mem from ymm2 and store result in ymm1.
VSUBPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VSUBPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Subtract packed DP FP values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1.
VSUBPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VSUBPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Subtract packed DP FP values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1.
VSUBPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VSUBPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Subtract packed DP FP values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	SUBPS	Subtract Packed Single-Precision Floating-Point Values	SUBPS
SUBPS	XMM,XMM/M128	SSE	SUBPS XMM1,XMM2/M128	Subtract packed SP FP values in xmm2/mem from xmm1 and store result in xmm1.
GENERAL	VSUBPS	Subtract Packed Single-Precision Floating-Point Values	SUBPS
VSUBPS	XMM,XMM,XMM/M128	AVX	VSUBPS XMM1,XMM2,XMM3/M128	Subtract packed SP FP values in xmm3/mem from xmm2 and stores result in xmm1.
VSUBPS	YMM,YMM,YMM/M256	AVX	VSUBPS YMM1,YMM2,YMM3/M256	Subtract packed SP FP values in ymm3/mem from ymm2 and stores result in ymm1.
VSUBPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VSUBPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Subtract packed SP FP values from xmm3/m128/m32bcst to xmm2 and stores result in xmm1 with writemask k1.
VSUBPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VSUBPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Subtract packed SP FP values from ymm3/m256/m32bcst to ymm2 and stores result in ymm1 with writemask k1.
VSUBPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VSUBPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Subtract packed SP FP values in zmm3/m512/m32bcst from zmm2 and stores result in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	SUBSD	Subtract Scalar Double-Precision Floating-Point Value	SUBSD
SUBSD	XMM,XMM/M64	SSE2	SUBSD XMM1,XMM2/M64	Subtract the low DP FP value in xmm2/m64 from xmm1 and store the result in xmm1.
GENERAL	VSUBSD	Subtract Scalar Double-Precision Floating-Point Value	SUBSD
VSUBSD	XMM,XMM,XMM/M64	AVX	VSUBSD XMM1,XMM2,XMM3/M64	Subtract the low DP FP value in xmm3/m64 from xmm2 and store the result in xmm1.
VSUBSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VSUBSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Subtract the low DP FP value in xmm3/m64 from xmm2 and store the result in xmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	SUBSS	Subtract Scalar Single-Precision Floating-Point Value	SUBSS
SUBSS	XMM,XMM/M32	SSE	SUBSS XMM1,XMM2/M32	Subtract the low SP FP value in xmm2/m32 from xmm1 and store the result in xmm1.
GENERAL	VSUBSS	Subtract Scalar Single-Precision Floating-Point Value	SUBSS
VSUBSS	XMM,XMM,XMM/M32	AVX	VSUBSS XMM1,XMM2,XMM3/M32	Subtract the low SP FP value in xmm3/m32 from xmm2 and store the result in xmm1.
VSUBSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VSUBSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Subtract the low SP FP value in xmm3/m32 from xmm2 and store the result in xmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	SWAPGS	Swap GS Base Register	SWAPGS
SWAPGS		8086	SWAPGS	Exchanges the current GS base register value with the value contained in MSR address C0000102H.
;--------------------------------------------------------
GENERAL	SYSCALL	Fast System Call	SYSCALL
SYSCALL		8086	SYSCALL	Fast call to privilege level 0 system procedures.
;--------------------------------------------------------
GENERAL	SYSENTER	Fast System Call	SYSENTER
SYSENTER		8086	SYSENTER	Fast call to privilege level 0 system procedures.
;--------------------------------------------------------
GENERAL	SYSEXIT	Fast Return from Fast System Call	SYSEXIT
SYSEXIT		8086	SYSEXIT	Fast return to privilege level 3 user code.
SYSEXIT		8086	SYSEXIT	Fast return to 64-bit mode privilege level 3 user code.
;--------------------------------------------------------
GENERAL	SYSRET	Return From Fast System Call	SYSRET
SYSRET		8086	SYSRET	Return to compatibility mode from fast system call
SYSRET		8086	SYSRET	Return to 64-bit mode from fast system call
;--------------------------------------------------------
GENERAL	TEST	Logical Compare	TEST
TEST	AL,IMM8	8086	TEST AL,IMM8	AND imm8 with AL; set SF, ZF, PF according to result.
TEST	AX,IMM16	8086	TEST AX,IMM16	AND imm16 with AX; set SF, ZF, PF according to result.
TEST	EAX,IMM32	386	TEST EAX,IMM32	AND imm32 with EAX; set SF, ZF, PF according to result.
TEST	RAX,IMM32	386	TEST RAX,IMM32	AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result.
TEST	R/M8,IMM8	8086	TEST R/M8,IMM8	AND imm8 with r/m8; set SF, ZF, PF according to result.
TEST	R/M8,IMM8	8086	TEST R/M8,IMM8	AND imm8 with r/m8; set SF, ZF, PF according to result.
TEST	R/M16,IMM16	8086	TEST R/M16,IMM16	AND imm16 with r/m16; set SF, ZF, PF according to result.
TEST	R/M32,IMM32	386	TEST R/M32,IMM32	AND imm32 with r/m32; set SF, ZF, PF according to result.
TEST	R/M64,IMM32	X64	TEST R/M64,IMM32	AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result.
TEST	R/M8,R8	8086	TEST R/M8,R8	AND r8 with r/m8; set SF, ZF, PF according to result.
TEST	R/M8,R8	8086	TEST R/M8,R8	AND r8 with r/m8; set SF, ZF, PF according to result.
TEST	R/M16,R16	8086	TEST R/M16,R16	AND r16 with r/m16; set SF, ZF, PF according to result.
TEST	R/M32,R32	386	TEST R/M32,R32	AND r32 with r/m32; set SF, ZF, PF according to result.
TEST	R/M64,R64	X64	TEST R/M64,R64	AND r64 with r/m64; set SF, ZF, PF according to result.
;--------------------------------------------------------
GENERAL	TPAUSE	Timed PAUSE	TPAUSE
TPAUSE	R32	WAITPKG	TPAUSE R32	Directs the processor to enter an implementation-dependent optimized state until the TSC reaches the value in EDX:EAX.
;--------------------------------------------------------
GENERAL	TZCNT	Count the Number of Trailing Zero Bits	TZCNT
TZCNT	R16,R/M16	BMI1	TZCNT R16,R/M16	Count the number of trailing zero bits in r/m16, return result in r16.
TZCNT	R32,R/M32	BMI1	TZCNT R32,R/M32	Count the number of trailing zero bits in r/m32, return result in r32.
TZCNT	R64,R/M64	BMI1	TZCNT R64,R/M64	Count the number of trailing zero bits in r/m64, return result in r64.
;--------------------------------------------------------
GENERAL	UCOMISD	Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS	UCOMISD
UCOMISD	XMM,XMM/M64	SSE2	UCOMISD XMM1,XMM2/M64	Compare low DP FP values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
GENERAL	VUCOMISD	Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS	UCOMISD
VUCOMISD	XMM,XMM/M64	AVX	VUCOMISD XMM1,XMM2/M64	Compare low DP FP values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VUCOMISD	XMM,XMM/M64{SAE}	AVX512_F	VUCOMISD XMM1,XMM2/M64{SAE}	Compare low DP FP values in xmm1 and xmm2/m64 and set the EFLAGS flags accordingly.
;--------------------------------------------------------
GENERAL	UCOMISS	Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS	UCOMISS
UCOMISS	XMM,XMM/M32	SSE	UCOMISS XMM1,XMM2/M32	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
GENERAL	VUCOMISS	Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS	UCOMISS
VUCOMISS	XMM,XMM/M32	AVX	VUCOMISS XMM1,XMM2/M32	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VUCOMISS	XMM,XMM/M32{SAE}	AVX512_F	VUCOMISS XMM1,XMM2/M32{SAE}	Compare low SP FP values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
;--------------------------------------------------------
GENERAL	UD01	Undefined Instruction	UD
UD01	R32,R/M32	386	UD01 R32,R/M32	Raise invalid opcode exception.
GENERAL	UD1	Undefined Instruction	UD
UD1	R32,R/M32	386	UD1 R32,R/M32	Raise invalid opcode exception.
GENERAL	UD2	Undefined Instruction	UD
UD2		8086	UD2	Raise invalid opcode exception.
;--------------------------------------------------------
GENERAL	UMONITOR	User Level Set Up Monitor Address	UMONITOR
UMONITOR	R16/R32/R64	WAITPKG	UMONITOR R16/R32/R64	Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write- back memory caching type. The address is contained in r16/r32/r64.
;--------------------------------------------------------
GENERAL	UMWAIT	User Level Monitor Wait	UMWAIT
UMWAIT	R32	WAITPKG	UMWAIT R32	A hint that allows the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.
;--------------------------------------------------------
GENERAL	UNPCKHPD	Unpack and Interleave High Packed Double-Precision Floating-Point Values	UNPCKHPD
UNPCKHPD	XMM,XMM/M128	SSE2	UNPCKHPD XMM1,XMM2/M128	Unpacks and Interleaves DP FP values from high quadwords of xmm1 and xmm2/m128.
GENERAL	VUNPCKHPD	Unpack and Interleave High Packed Double-Precision Floating-Point Values	UNPCKHPD
VUNPCKHPD	XMM,XMM,XMM/M128	AVX	VUNPCKHPD XMM1,XMM2,XMM3/M128	Unpacks and Interleaves DP FP values from high quadwords of xmm2 and xmm3/m128.
VUNPCKHPD	YMM,YMM,YMM/M256	AVX	VUNPCKHPD YMM1,YMM2,YMM3/M256	Unpacks and Interleaves DP FP values from high quadwords of ymm2 and ymm3/m256.
VUNPCKHPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VUNPCKHPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Unpacks and Interleaves double precision FP values from high quadwords of xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VUNPCKHPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VUNPCKHPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Unpacks and Interleaves double precision FP values from high quadwords of ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VUNPCKHPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VUNPCKHPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Unpacks and Interleaves DP FP values from high quadwords of zmm2 and zmm3/m512/m64bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	UNPCKHPS	Unpack and Interleave High Packed Single-Precision Floating-Point Values	UNPCKHPS
UNPCKHPS	XMM,XMM/M128	SSE	UNPCKHPS XMM1,XMM2/M128	Unpacks and Interleaves SP FP values from high quadwords of xmm1 and xmm2/m128.
GENERAL	VUNPCKHPS	Unpack and Interleave High Packed Single-Precision Floating-Point Values	UNPCKHPS
VUNPCKHPS	XMM,XMM,XMM/M128	AVX	VUNPCKHPS XMM1,XMM2,XMM3/M128	Unpacks and Interleaves SP FP values from high quadwords of xmm2 and xmm3/m128.
VUNPCKHPS	YMM,YMM,YMM/M256	AVX	VUNPCKHPS YMM1,YMM2,YMM3/M256	Unpacks and Interleaves SP FP values from high quadwords of ymm2 and ymm3/m256.
VUNPCKHPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VUNPCKHPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Unpacks and Interleaves SP FP values from high quadwords of xmm2 and xmm3/m128/m32bcst and write result to xmm1 subject to writemask k1.
VUNPCKHPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VUNPCKHPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Unpacks and Interleaves SP FP values from high quadwords of ymm2 and ymm3/m256/m32bcst and write result to ymm1 subject to writemask k1.
VUNPCKHPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VUNPCKHPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Unpacks and Interleaves SP FP values from high quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	UNPCKLPD	Unpack and Interleave Low Packed Double-Precision Floating-Point Values	UNPCKLPD
UNPCKLPD	XMM,XMM/M128	SSE2	UNPCKLPD XMM1,XMM2/M128	Unpacks and Interleaves DP FP values from low quadwords of xmm1 and xmm2/m128.
GENERAL	VUNPCKLPD	Unpack and Interleave Low Packed Double-Precision Floating-Point Values	UNPCKLPD
VUNPCKLPD	XMM,XMM,XMM/M128	AVX	VUNPCKLPD XMM1,XMM2,XMM3/M128	Unpacks and Interleaves DP FP values from low quadwords of xmm2 and xmm3/m128.
VUNPCKLPD	YMM,YMM,YMM/M256	AVX	VUNPCKLPD YMM1,YMM2,YMM3/M256	Unpacks and Interleaves DP FP values from low quadwords of ymm2 and ymm3/m256.
VUNPCKLPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VUNPCKLPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Unpacks and Interleaves double precision FP values from low quadwords of xmm2 and xmm3/m128/m64bcst subject to write mask k1.
VUNPCKLPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VUNPCKLPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Unpacks and Interleaves double precision FP values from low quadwords of ymm2 and ymm3/m256/m64bcst subject to write mask k1.
VUNPCKLPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VUNPCKLPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Unpacks and Interleaves DP FP values from low quadwords of zmm2 and zmm3/m512/m64bcst subject to write mask k1.
;--------------------------------------------------------
GENERAL	UNPCKLPS	Unpack and Interleave Low Packed Single-Precision Floating-Point Values	UNPCKLPS
UNPCKLPS	XMM,XMM/M128	SSE	UNPCKLPS XMM1,XMM2/M128	Unpacks and Interleaves SP FP values from low quadwords of xmm1 and xmm2/m128.
GENERAL	VUNPCKLPS	Unpack and Interleave Low Packed Single-Precision Floating-Point Values	UNPCKLPS
VUNPCKLPS	XMM,XMM,XMM/M128	AVX	VUNPCKLPS XMM1,XMM2,XMM3/M128	Unpacks and Interleaves SP FP values from low quadwords of xmm2 and xmm3/m128.
VUNPCKLPS	YMM,YMM,YMM/M256	AVX	VUNPCKLPS YMM1,YMM2,YMM3/M256	Unpacks and Interleaves SP FP values from low quadwords of ymm2 and ymm3/m256.
VUNPCKLPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VUNPCKLPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Unpacks and Interleaves SP FP values from low quadwords of xmm2 and xmm3/mem and write result to xmm1 subject to write mask k1.
VUNPCKLPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VUNPCKLPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Unpacks and Interleaves SP FP values from low quadwords of ymm2 and ymm3/mem and write result to ymm1 subject to write mask k1.
VUNPCKLPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VUNPCKLPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Unpacks and Interleaves SP FP values from low quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to write mask k1.
;--------------------------------------------------------
GENERAL	V4FMADDPS	Packed Single-Precision Floating-Point Fused Multiply-Add (4-iterations)	V4FMADDPS_V4FNMADDPS
V4FMADDPS	ZMM{K}{Z},ZMM,M128	AVX512_4FMAPS	V4FMADDPS ZMM1{K1}{Z},ZMM2+3,M128	Multiply packed SP FP values from source register block indicated by zmm2 by values from m128 and accumulate the result in zmm1.
GENERAL	V4FNMADDPS	Packed Single-Precision Floating-Point Fused Multiply-Add (4-iterations)	V4FMADDPS_V4FNMADDPS
V4FNMADDPS	ZMM{K}{Z},ZMM,M128	AVX512_4FMAPS	V4FNMADDPS ZMM1{K1}{Z},ZMM2+3,M128	Multiply and negate packed SP FP values from source register block indicated by zmm2 by values from m128 and accumulate the result in zmm1.
;--------------------------------------------------------
GENERAL	V4FMADDSS	Scalar Single-Precision Floating-Point Fused Multiply-Add (4-iterations)	V4FMADDSS_V4FNMADDSS
V4FMADDSS	XMM{K}{Z},XMM,M128	AVX512_4FMAPS	V4FMADDSS XMM1{K1}{Z},XMM2+3,M128	Multiply scalar SP FP values from source register block indicated by xmm2 by values from m128 and accumulate the result in xmm1.
GENERAL	V4FNMADDSS	Scalar Single-Precision Floating-Point Fused Multiply-Add (4-iterations)	V4FMADDSS_V4FNMADDSS
V4FNMADDSS	XMM{K}{Z},XMM,M128	AVX512_4FMAPS	V4FNMADDSS XMM1{K1}{Z},XMM2+3,M128	Multiply and negate scalar SP FP values from source register block indicated by xmm2 by values from m128 and accumulate the result in xmm1.
;--------------------------------------------------------
GENERAL	VAESDEC	Perform One Round of an AES Decryption Flow	VAESDEC
VAESDEC	YMM,YMM,YMM/M256	AVX512_VAES	VAESDEC YMM1,YMM2,YMM3/M256	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from ymm3/m256; store the result in ymm1.
VAESDEC	XMM,XMM,XMM/M128	AVX512_VL,AVX512_VAES	VAESDEC XMM1,XMM2,XMM3/M128	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
VAESDEC	YMM,YMM,YMM/M256	AVX512_VL,AVX512_VAES	VAESDEC YMM1,YMM2,YMM3/M256	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from ymm3/m256; store the result in ymm1.
VAESDEC	ZMM,ZMM,ZMM/M512	AVX512_F,AVX512_VAES	VAESDEC ZMM1,ZMM2,ZMM3/M512	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from zmm2 with a 128-bit round key from zmm3/m512; store the result in zmm1.
;--------------------------------------------------------
GENERAL	VAESDECLAST	Perform Last Round of an AES Decryption Flow	VAESDECLAST
VAESDECLAST	YMM,YMM,YMM/M256	AVX512_VAES	VAESDECLAST YMM1,YMM2,YMM3/M256	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from ymm3/m256; store the result in ymm1.
VAESDECLAST	XMM,XMM,XMM/M128	AVX512_VL,AVX512_VAES	VAESDECLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, oper- ating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
VAESDECLAST	YMM,YMM,YMM/M256	AVX512_VL,AVX512_VAES	VAESDECLAST YMM1,YMM2,YMM3/M256	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from ymm3/m256; store the result in ymm1.
VAESDECLAST	ZMM,ZMM,ZMM/M512	AVX512_F,AVX512_VAES	VAESDECLAST ZMM1,ZMM2,ZMM3/M512	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from zmm2 with a 128-bit round key from zmm3/m512; store the result in zmm1.
;--------------------------------------------------------
GENERAL	VAESENC	Perform One Round of an AES Encryption Flow	VAESENC
VAESENC	YMM,YMM,YMM/M256	AVX512_VAES	VAESENC YMM1,YMM2,YMM3/M256	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from the ymm3/m256; store the result in ymm1.
VAESENC	XMM,XMM,XMM/M128	AVX512_VL,AVX512_VAES	VAESENC XMM1,XMM2,XMM3/M128	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1.
VAESENC	YMM,YMM,YMM/M256	AVX512_VL,AVX512_VAES	VAESENC YMM1,YMM2,YMM3/M256	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from ymm2 with a 128-bit round key from the ymm3/m256; store the result in ymm1.
VAESENC	ZMM,ZMM,ZMM/M512	AVX512_F,AVX512_VAES	VAESENC ZMM1,ZMM2,ZMM3/M512	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from zmm2 with a 128-bit round key from the zmm3/m512; store the result in zmm1.
;--------------------------------------------------------
GENERAL	VAESENCLAST	Perform Last Round of an AES Encryption Flow	VAESENCLAST
VAESENCLAST	YMM,YMM,YMM/M256	AVX512_VAES	VAESENCLAST YMM1,YMM2,YMM3/M256	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from ymm2 with a 128 bit round key from ymm3/m256; store the result in ymm1.
VAESENCLAST	XMM,XMM,XMM/M128	AVX512_VL,AVX512_VAES	VAESENCLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1.
VAESENCLAST	YMM,YMM,YMM/M256	AVX512_VL,AVX512_VAES	VAESENCLAST YMM1,YMM2,YMM3/M256	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from ymm2 with a 128 bit round key from ymm3/m256; store the result in ymm1.
VAESENCLAST	ZMM,ZMM,ZMM/M512	AVX512_F,AVX512_VAES	VAESENCLAST ZMM1,ZMM2,ZMM3/M512	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from zmm2 with a 128 bit round key from zmm3/m512; store the result in zmm1.
;--------------------------------------------------------
GENERAL	VALIGND	Align Doubleword/Quadword Vectors	VALIGND_VALIGNQ
VALIGND	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VALIGND XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Shift right and merge vectors xmm2 and xmm3/m128/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask.
VALIGND	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VALIGND YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shift right and merge vectors ymm2 and ymm3/m256/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask.
VALIGND	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VALIGND ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shift right and merge vectors zmm2 and zmm3/m512/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask.
GENERAL	VALIGNQ	Align Doubleword/Quadword Vectors	VALIGND_VALIGNQ
VALIGNQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VALIGNQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Shift right and merge vectors xmm2 and xmm3/m128/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask.
VALIGNQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VALIGNQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shift right and merge vectors ymm2 and ymm3/m256/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask.
VALIGNQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VALIGNQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shift right and merge vectors zmm2 and zmm3/m512/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask.
;--------------------------------------------------------
GENERAL	VBLENDMPD	Blend Float64/Float32 Vectors Using an OpMask Control	VBLENDMPD_VBLENDMPS
VBLENDMPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VBLENDMPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Blend DP vector xmm2 and DP vector xmm3/m128/m64bcst and store the result in xmm1, under control mask.
VBLENDMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VBLENDMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Blend DP vector ymm2 and DP vector ymm3/m256/m64bcst and store the result in ymm1, under control mask.
VBLENDMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VBLENDMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Blend DP vector zmm2 and DP vector zmm3/m512/m64bcst and store the result in zmm1, under control mask.
GENERAL	VBLENDMPS	Blend Float64/Float32 Vectors Using an OpMask Control	VBLENDMPD_VBLENDMPS
VBLENDMPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VBLENDMPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Blend SP vector xmm2 and SP vector xmm3/m128/m32bcst and store the result in xmm1, under control mask.
VBLENDMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VBLENDMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Blend SP vector ymm2 and SP vector ymm3/m256/m32bcst and store the result in ymm1, under control mask.
VBLENDMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VBLENDMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Blend SP vector zmm2 and SP vector zmm3/m512/m32bcst using k1 as select control and store the result in zmm1.
;--------------------------------------------------------
GENERAL	VBROADCASTSS	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTSS	XMM,M32	AVX	VBROADCASTSS XMM1,M32	Broadcast SP FP element in mem to four locations in xmm1.
VBROADCASTSS	YMM,M32	AVX	VBROADCASTSS YMM1,M32	Broadcast SP FP element in mem to eight locations in ymm1.
VBROADCASTSS	XMM,XMM	AVX2	VBROADCASTSS XMM1,XMM2	Broadcast the low SP FP element in the source operand to four locations in xmm1.
VBROADCASTSS	YMM,XMM	AVX2	VBROADCASTSS YMM1,XMM2	Broadcast low SP FP element in the source operand to eight locations in ymm1.
VBROADCASTSS	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VBROADCASTSS XMM1{K1}{Z},XMM2/M32	Broadcast low SP FP element in xmm2/m32 to all locations in xmm1 using writemask k1.
VBROADCASTSS	YMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VBROADCASTSS YMM1{K1}{Z},XMM2/M32	Broadcast low SP FP element in xmm2/m32 to all locations in ymm1 using writemask k1.
VBROADCASTSS	ZMM{K}{Z},XMM/M32	AVX512_F	VBROADCASTSS ZMM1{K1}{Z},XMM2/M32	Broadcast low SP FP element in xmm2/m32 to all locations in zmm1 using writemask k1.
GENERAL	VBROADCASTSD	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTSD	YMM,M64	AVX	VBROADCASTSD YMM1,M64	Broadcast DP FP element in mem to four locations in ymm1.
VBROADCASTSD	YMM,XMM	AVX2	VBROADCASTSD YMM1,XMM2	Broadcast low DP FP element in the source operand to four locations in ymm1.
VBROADCASTSD	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VBROADCASTSD YMM1{K1}{Z},XMM2/M64	Broadcast low DP FP element in xmm2/m64 to four locations in ymm1 using writemask k1.
VBROADCASTSD	ZMM{K}{Z},XMM/M64	AVX512_F	VBROADCASTSD ZMM1{K1}{Z},XMM2/M64	Broadcast low DP FP element in xmm2/m64 to eight locations in zmm1 using writemask k1.
GENERAL	VBROADCASTF128	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF128	YMM,M128	AVX	VBROADCASTF128 YMM1,M128	Broadcast 128 bits of FP data in mem to low and high 128-bits in ymm1.
GENERAL	VBROADCASTF32X2	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF32X2	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_DQ	VBROADCASTF32X2 YMM1{K1}{Z},XMM2/M64	Broadcast two SP FP elements in xmm2/m64 to locations in ymm1 using writemask k1.
VBROADCASTF32X2	ZMM{K}{Z},XMM/M64	AVX512_DQ	VBROADCASTF32X2 ZMM1{K1}{Z},XMM2/M64	Broadcast two SP FP elements in xmm2/m64 to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTF32X4	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF32X4	YMM{K}{Z},M128	AVX512_VL,AVX512_F	VBROADCASTF32X4 YMM1{K1}{Z},M128	Broadcast 128 bits of 4 SP FP data in mem to locations in ymm1 using writemask k1.
VBROADCASTF32X4	ZMM{K}{Z},M128	AVX512_F	VBROADCASTF32X4 ZMM1{K1}{Z},M128	Broadcast 128 bits of 4 SP FP data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTF64X2	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF64X2	YMM{K}{Z},M128	AVX512_VL,AVX512_DQ	VBROADCASTF64X2 YMM1{K1}{Z},M128	Broadcast 128 bits of 2 DP FP data in mem to locations in ymm1 using writemask k1.
VBROADCASTF64X2	ZMM{K}{Z},M128	AVX512_DQ	VBROADCASTF64X2 ZMM1{K1}{Z},M128	Broadcast 128 bits of 2 DP FP data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTF32X8	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF32X8	ZMM{K}{Z},M256	AVX512_DQ	VBROADCASTF32X8 ZMM1{K1}{Z},M256	Broadcast 256 bits of 8 SP FP data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTF64X4	Load with Broadcast Floating-Point Data	VBROADCAST
VBROADCASTF64X4	ZMM{K}{Z},M256	AVX512_F	VBROADCASTF64X4 ZMM1{K1}{Z},M256	Broadcast 256 bits of 4 DP FP data in mem to locations in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VCOMPRESSPD	Store Sparse Packed Double-Precision Floating-Point Values into Dense Memory	VCOMPRESSPD
VCOMPRESSPD	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VCOMPRESSPD XMM1/M128{K1}{Z},XMM2	Compress packed DP FP values from xmm2 to xmm1/m128 using writemask k1.
VCOMPRESSPD	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VCOMPRESSPD YMM1/M256{K1}{Z},YMM2	Compress packed DP FP values from ymm2 to ymm1/m256 using writemask k1.
VCOMPRESSPD	ZMM/M512{K}{Z},ZMM	AVX512_F	VCOMPRESSPD ZMM1/M512{K1}{Z},ZMM2	Compress packed DP FP values from zmm2 using control mask k1 to zmm1/m512.
;--------------------------------------------------------
GENERAL	VCOMPRESSPS	Store Sparse Packed Single-Precision Floating-Point Values into Dense Memory	VCOMPRESSPS
VCOMPRESSPS	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VCOMPRESSPS XMM1/M128{K1}{Z},XMM2	Compress packed SP FP values from xmm2 to xmm1/m128 using writemask k1.
VCOMPRESSPS	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VCOMPRESSPS YMM1/M256{K1}{Z},YMM2	Compress packed SP FP values from ymm2 to ymm1/m256 using writemask k1.
VCOMPRESSPS	ZMM/M512{K}{Z},ZMM	AVX512_F	VCOMPRESSPS ZMM1/M512{K1}{Z},ZMM2	Compress packed SP FP values from zmm2 using control mask k1 to zmm1/m512.
;--------------------------------------------------------
GENERAL	VCVTPD2QQ	Convert Packed Double-Precision Floating-Point Values to Packed Quadword Integers	VCVTPD2QQ
VCVTPD2QQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTPD2QQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values from xmm2/m128/m64bcst to two packed quadword integers in xmm1 with writemask k1.
VCVTPD2QQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTPD2QQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 with writemask k1.
VCVTPD2QQ	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTPD2QQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed DP FP values from zmm2/m512/m64bcst to eight packed quadword integers in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTPD2UDQ	Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers	VCVTPD2UDQ
VCVTPD2UDQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VCVTPD2UDQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 subject to writemask k1.
VCVTPD2UDQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VCVTPD2UDQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 subject to writemask k1.
VCVTPD2UDQ	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_F	VCVTPD2UDQ YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed DP FP values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTPD2UQQ	Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers	VCVTPD2UQQ
VCVTPD2UQQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTPD2UQQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values from xmm2/mem to two packed unsigned quadword integers in xmm1 with writemask k1.
VCVTPD2UQQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTPD2UQQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert fourth packed DP FP values from ymm2/mem to four packed unsigned quadword integers in ymm1 with writemask k1.
VCVTPD2UQQ	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTPD2UQQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed DP FP values from zmm2/mem to eight packed unsigned quadword integers in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTPH2PS	Convert 16-bit FP values to Single-Precision FP values	VCVTPH2PS
VCVTPH2PS	XMM,XMM/M64	F16C	VCVTPH2PS XMM1,XMM2/M64	Convert four packed half precision (16-bit) FP values in xmm2/m64 to packed SP FP value in xmm1.
VCVTPH2PS	YMM,XMM/M128	F16C	VCVTPH2PS YMM1,XMM2/M128	Convert eight packed half precision (16-bit) FP values in xmm2/m128 to packed SP FP value in ymm1.
VCVTPH2PS	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VCVTPH2PS XMM1{K1}{Z},XMM2/M64	Convert four packed half precision (16-bit) FP values in xmm2/m64 to packed SP FP values in xmm1.
VCVTPH2PS	YMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VCVTPH2PS YMM1{K1}{Z},XMM2/M128	Convert eight packed half precision (16-bit) FP values in xmm2/m128 to packed SP FP values in ymm1.
VCVTPH2PS	ZMM{K}{Z},YMM/M256{SAE}	AVX512_F	VCVTPH2PS ZMM1{K1}{Z},YMM2/M256{SAE}	Convert sixteen packed half precision (16-bit) FP values in ymm2/m256 to packed SP FP values in zmm1.
;--------------------------------------------------------
GENERAL	VCVTPS2PH	Convert Single-Precision FP value to 16-bit FP value	VCVTPS2PH
VCVTPS2PH	XMM/M64,XMM,IMM8	F16C	VCVTPS2PH XMM1/M64,XMM2,IMM8	Convert four packed SP FP values in xmm2 to packed half-precision (16-bit) FP values in xmm1/m64. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M128,YMM,IMM8	F16C	VCVTPS2PH XMM1/M128,YMM2,IMM8	Convert eight packed SP FP values in ymm2 to packed half-precision (16-bit) FP values in xmm1/m128. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M64{K}{Z},XMM,IMM8	AVX512_VL,AVX512_F	VCVTPS2PH XMM1/M64{K1}{Z},XMM2,IMM8	Convert four packed SP FP values in xmm2 to packed half-precision (16-bit) FP values in xmm1/m64. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M128{K}{Z},YMM,IMM8	AVX512_VL,AVX512_F	VCVTPS2PH XMM1/M128{K1}{Z},YMM2,IMM8	Convert eight packed SP FP values in ymm2 to packed half-precision (16-bit) FP values in xmm1/m128. Imm8 provides rounding controls.
VCVTPS2PH	YMM/M256{K}{Z},ZMM{SAE},IMM8	AVX512_F	VCVTPS2PH YMM1/M256{K1}{Z},ZMM2{SAE},IMM8	Convert sixteen packed SP FP values in zmm2 to packed half-precision (16-bit) FP values in ymm1/m256. Imm8 provides rounding controls.
;--------------------------------------------------------
GENERAL	VCVTPS2QQ	Convert Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values	VCVTPS2QQ
VCVTPS2QQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_DQ	VCVTPS2QQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision FP values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 subject to writemask k1.
VCVTPS2QQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VCVTPS2QQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 subject to writemask k1.
VCVTPS2QQ	ZMM{K}{Z},YMM/M256/M32BCST{ER}	AVX512_DQ	VCVTPS2QQ ZMM1{K1}{Z},YMM2/M256/M32BCST{ER}	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTPS2UDQ	Convert Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values	VCVTPS2UDQ
VCVTPS2UDQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTPS2UDQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 subject to writemask k1.
VCVTPS2UDQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTPS2UDQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 subject to writemask k1.
VCVTPS2UDQ	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512_F	VCVTPS2UDQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed SP FP values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTPS2UQQ	Convert Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values	VCVTPS2UQQ
VCVTPS2UQQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_DQ	VCVTPS2UQQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision FP values from zmm2/m64/m32bcst to two packed unsigned quadword values in zmm1 subject to writemask k1.
VCVTPS2UQQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VCVTPS2UQQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 subject to writemask k1.
VCVTPS2UQQ	ZMM{K}{Z},YMM/M256/M32BCST{ER}	AVX512_DQ	VCVTPS2UQQ ZMM1{K1}{Z},YMM2/M256/M32BCST{ER}	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTQQ2PD	Convert Packed Quadword Integers to Packed Double-Precision Floating-Point Values	VCVTQQ2PD
VCVTQQ2PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTQQ2PD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed quadword integers from xmm2/m128/m64bcst to packed DP FP values in xmm1 with writemask k1.
VCVTQQ2PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTQQ2PD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed quadword integers from ymm2/m256/m64bcst to packed DP FP values in ymm1 with writemask k1.
VCVTQQ2PD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTQQ2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed quadword integers from zmm2/m512/m64bcst to eight packed DP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTQQ2PS	Convert Packed Quadword Integers to Packed Single-Precision Floating-Point Values	VCVTQQ2PS
VCVTQQ2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTQQ2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed quadword integers from xmm2/mem to packed SP FP values in xmm1 with writemask k1.
VCVTQQ2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTQQ2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed quadword integers from ymm2/mem to packed SP FP values in xmm1 with writemask k1.
VCVTQQ2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTQQ2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed quadword integers from zmm2/mem to eight packed SP FP values in ymm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTSD2USI	Convert Scalar Double-Precision Floating-Point Value to Unsigned Doubleword Integer	VCVTSD2USI
VCVTSD2USI	R32,XMM/M64{ER}	AVX512_F	VCVTSD2USI R32,XMM1/M64{ER}	Convert one DP FP value from xmm1/m64 to one unsigned doubleword integer r32.
VCVTSD2USI	R64,XMM/M64{ER}	AVX512_F	VCVTSD2USI R64,XMM1/M64{ER}	Convert one DP FP value from xmm1/m64 to one unsigned quadword integer zero- extended into r64.
;--------------------------------------------------------
GENERAL	VCVTSS2USI	Convert Scalar Single-Precision Floating-Point Value to Unsigned Doubleword Integer	VCVTSS2USI
VCVTSS2USI	R32,XMM/M32{ER}	AVX512_F	VCVTSS2USI R32,XMM1/M32{ER}	Convert one SP FP value from xmm1/m32 to one unsigned doubleword integer in r32.
VCVTSS2USI	R64,XMM/M32{ER}	AVX512_F	VCVTSS2USI R64,XMM1/M32{ER}	Convert one SP FP value from xmm1/m32 to one unsigned quadword integer in r64.
;--------------------------------------------------------
GENERAL	VCVTTPD2QQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Quadword Integers	VCVTTPD2QQ
VCVTTPD2QQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTTPD2QQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values from zmm2/m128/m64bcst to two packed quadword integers in zmm1 using truncation with writemask k1.
VCVTTPD2QQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTTPD2QQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 using truncation with writemask k1.
VCVTTPD2QQ	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_DQ	VCVTTPD2QQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed DP FP values from zmm2/m512 to eight packed quadword integers in zmm1 using truncation with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTPD2UDQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers	VCVTTPD2UDQ
VCVTTPD2UDQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VCVTTPD2UDQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2UDQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VCVTTPD2UDQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2UDQ	YMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_F	VCVTTPD2UDQ YMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed DP FP values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTPD2UQQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers	VCVTTPD2UQQ
VCVTTPD2UQQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTTPD2UQQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed DP FP values from xmm2/m128/m64bcst to two packed unsigned quadword integers in xmm1 using truncation with writemask k1.
VCVTTPD2UQQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTTPD2UQQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed DP FP values from ymm2/m256/m64bcst to four packed unsigned quadword integers in ymm1 using truncation with writemask k1.
VCVTTPD2UQQ	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_DQ	VCVTTPD2UQQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed DP FP values from zmm2/mem to eight packed unsigned quadword integers in zmm1 using truncation with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTPS2QQ	Convert with Truncation Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values	VCVTTPS2QQ
VCVTTPS2QQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_DQ	VCVTTPS2QQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision FP values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2QQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VCVTTPS2QQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2QQ	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512_DQ	VCVTTPS2QQ ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTPS2UDQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values	VCVTTPS2UDQ
VCVTTPS2UDQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTTPS2UDQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2UDQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTTPS2UDQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2UDQ	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_F	VCVTTPS2UDQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert sixteen packed SP FP values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTPS2UQQ	Convert with Truncation Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values	VCVTTPS2UQQ
VCVTTPS2UQQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_DQ	VCVTTPS2UQQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision FP values from xmm2/m64/m32bcst to two packed unsigned quadword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2UQQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VCVTTPS2UQQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision FP values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2UQQ	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512_DQ	VCVTTPS2UQQ ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed single precision FP values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 using truncation subject to writemask k1.
;--------------------------------------------------------
GENERAL	VCVTTSD2USI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Unsigned Integer	VCVTTSD2USI
VCVTTSD2USI	R32,XMM/M64{SAE}	AVX512_F	VCVTTSD2USI R32,XMM1/M64{SAE}	Convert one DP FP value from xmm1/m64 to one unsigned doubleword integer r32 using truncation.
VCVTTSD2USI	R64,XMM/M64{SAE}	AVX512_F	VCVTTSD2USI R64,XMM1/M64{SAE}	Convert one DP FP value from xmm1/m64 to one unsigned quadword integer zero- extended into r64 using truncation.
;--------------------------------------------------------
GENERAL	VCVTTSS2USI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Unsigned Integer	VCVTTSS2USI
VCVTTSS2USI	R32,XMM/M32{SAE}	AVX512_F	VCVTTSS2USI R32,XMM1/M32{SAE}	Convert one SP FP value from xmm1/m32 to one unsigned doubleword integer in r32 using truncation.
VCVTTSS2USI	R64,XMM/M32{SAE}	AVX512_F	VCVTTSS2USI R64,XMM1/M32{SAE}	Convert one SP FP value from xmm1/m32 to one unsigned quadword integer in r64 using truncation.
;--------------------------------------------------------
GENERAL	VCVTUDQ2PD	Convert Packed Unsigned Doubleword Integers to Packed Double-Precision Floating-Point Values	VCVTUDQ2PD
VCVTUDQ2PD	XMM{K}{Z},XMM/M64/M32BCST	AVX512_VL,AVX512_F	VCVTUDQ2PD XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed unsigned doubleword integers from ymm2/m64/m32bcst to packed DP FP values in zmm1 with writemask k1.
VCVTUDQ2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTUDQ2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed DP FP values in zmm1 with writemask k1.
VCVTUDQ2PD	ZMM{K}{Z},YMM/M256/M32BCST	AVX512_F	VCVTUDQ2PD ZMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to eight packed DP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTUDQ2PS	Convert Packed Unsigned Doubleword Integers to Packed Single-Precision Floating-Point Values	VCVTUDQ2PS
VCVTUDQ2PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VCVTUDQ2PS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed SP FP values in xmm1 with writemask k1.
VCVTUDQ2PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VCVTUDQ2PS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to packed SP FP values in zmm1 with writemask k1.
VCVTUDQ2PS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512_F	VCVTUDQ2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed unsigned doubleword integers from zmm2/m512/m32bcst to sixteen packed SP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTUQQ2PD	Convert Packed Unsigned Quadword Integers to Packed Double-Precision Floating-Point Values	VCVTUQQ2PD
VCVTUQQ2PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTUQQ2PD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to two packed DP FP values in xmm1 with writemask k1.
VCVTUQQ2PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTUQQ2PD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed DP FP values in ymm1 with writemask k1.
VCVTUQQ2PD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTUQQ2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed DP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTUQQ2PS	Convert Packed Unsigned Quadword Integers to Packed Single-Precision Floating-Point Values	VCVTUQQ2PS
VCVTUQQ2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VCVTUQQ2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to packed SP FP values in zmm1 with writemask k1.
VCVTUQQ2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VCVTUQQ2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed SP FP values in xmm1 with writemask k1.
VCVTUQQ2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512_DQ	VCVTUQQ2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed SP FP values in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VCVTUSI2SD	Convert Unsigned Integer to Scalar Double-Precision Floating-Point Value	VCVTUSI2SD
VCVTUSI2SD	XMM,XMM,R/M32	AVX512_F	VCVTUSI2SD XMM1,XMM2,R/M32	Convert one unsigned doubleword integer from r/m32 to one DP FP value in xmm1.
VCVTUSI2SD	XMM,XMM,R/M64{ER}	AVX512_F	VCVTUSI2SD XMM1,XMM2,R/M64{ER}	Convert one unsigned quadword integer from r/m64 to one DP FP value in xmm1.
;--------------------------------------------------------
GENERAL	VCVTUSI2SS	Convert Unsigned Integer to Scalar Single-Precision Floating-Point Value	VCVTUSI2SS
VCVTUSI2SS	XMM,XMM,R/M32{ER}	AVX512_F	VCVTUSI2SS XMM1,XMM2,R/M32{ER}	Convert one signed doubleword integer from r/m32 to one SP FP value in xmm1.
VCVTUSI2SS	XMM,XMM,R/M64{ER}	AVX512_F	VCVTUSI2SS XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one SP FP value in xmm1.
;--------------------------------------------------------
GENERAL	VDBPSADBW	Double Block Packed Sum-Absolute-Differences (SAD) on Unsigned Bytes	VDBPSADBW
VDBPSADBW	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VDBPSADBW XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Compute packed SAD word results of unsigned bytes in dword block from xmm2 with unsigned bytes of dword blocks transformed from xmm3/m128 using the shuffle controls in imm8. Results are written to xmm1 under the writemask k1.
VDBPSADBW	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VDBPSADBW YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Compute packed SAD word results of unsigned bytes in dword block from ymm2 with unsigned bytes of dword blocks transformed from ymm3/m256 using the shuffle controls in imm8. Results are written to ymm1 under the writemask k1.
VDBPSADBW	ZMM{K}{Z},ZMM,ZMM/M512,IMM8	AVX512_BW	VDBPSADBW ZMM1{K1}{Z},ZMM2,ZMM3/M512,IMM8	Compute packed SAD word results of unsigned bytes in dword block from zmm2 with unsigned bytes of dword blocks transformed from zmm3/m512 using the shuffle controls in imm8. Results are written to zmm1 under the writemask k1.
;--------------------------------------------------------
GENERAL	VERR	Verify a Segment for Reading or Writing	VERR_VERW
VERR	R/M16	8086	VERR R/M16	Set ZF=1 if segment specified with r/m16 can be read.
GENERAL	VERW	Verify a Segment for Reading or Writing	VERR_VERW
VERW	R/M16	8086	VERW R/M16	Set ZF=1 if segment specified with r/m16 can be written.
;--------------------------------------------------------
GENERAL	VEXP2PD	Approximation to the Exponential 2^x of Packed Double-Precision Floating-Point  Values with Less Than 2^-23 Relative Error	VEXP2PD
VEXP2PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_ER	VEXP2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed DP FP values from zmm2/m512/m64bcst and stores the FP result in zmm1with writemask k1.
;--------------------------------------------------------
GENERAL	VEXP2PS	Approximation to the Exponential 2^x of Packed Single-Precision Floating-Point  Values with Less Than 2^-23 Relative Error	VEXP2PS
VEXP2PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_ER	VEXP2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed SP FP values from zmm2/m512/m32bcst and stores the FP result in zmm1with writemask k1.
;--------------------------------------------------------
GENERAL	VEXPANDPD	Load Sparse Packed Double-Precision Floating-Point Values from Dense Memory	VEXPANDPD
VEXPANDPD	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VEXPANDPD XMM1{K1}{Z},XMM2/M128	Expand packed DP FP values from xmm2/m128 to xmm1 using writemask k1.
VEXPANDPD	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VEXPANDPD YMM1{K1}{Z},YMM2/M256	Expand packed DP FP values from ymm2/m256 to ymm1 using writemask k1.
VEXPANDPD	ZMM{K}{Z},ZMM/M512	AVX512_F	VEXPANDPD ZMM1{K1}{Z},ZMM2/M512	Expand packed DP FP values from zmm2/m512 to zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VEXPANDPS	Load Sparse Packed Single-Precision Floating-Point Values from Dense Memory	VEXPANDPS
VEXPANDPS	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VEXPANDPS XMM1{K1}{Z},XMM2/M128	Expand packed SP FP values from xmm2/m128 to xmm1 using writemask k1.
VEXPANDPS	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VEXPANDPS YMM1{K1}{Z},YMM2/M256	Expand packed SP FP values from ymm2/m256 to ymm1 using writemask k1.
VEXPANDPS	ZMM{K}{Z},ZMM/M512	AVX512_F	VEXPANDPS ZMM1{K1}{Z},ZMM2/M512	Expand packed SP FP values from zmm2/m512 to zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VEXTRACTF128	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4
VEXTRACTF128	XMM/M128,YMM,IMM8	AVX	VEXTRACTF128 XMM1/M128,YMM2,IMM8	Extract 128 bits of packed FP values from ymm2 and store results in xmm1/m128.
GENERAL	VEXTRACTF32X4	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4
VEXTRACTF32X4	XMM/M128{K}{Z},YMM,IMM8	AVX512_VL,AVX512_F	VEXTRACTF32X4 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of packed SP FP values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF32X4	XMM/M128{K}{Z},ZMM,IMM8	AVX512_F	VEXTRACTF32X4 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of packed SP FP values from zmm2 and store results in xmm1/m128 subject to writemask k1.
GENERAL	VEXTRACTF64X2	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4
VEXTRACTF64X2	XMM/M128{K}{Z},YMM,IMM8	AVX512_VL,AVX512_DQ	VEXTRACTF64X2 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of packed DP FP values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF64X2	XMM/M128{K}{Z},ZMM,IMM8	AVX512_DQ	VEXTRACTF64X2 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of packed DP FP values from zmm2 and store results in xmm1/m128 subject to writemask k1.
GENERAL	VEXTRACTF32X8	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4
VEXTRACTF32X8	YMM/M256{K}{Z},ZMM,IMM8	AVX512_DQ	VEXTRACTF32X8 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of packed SP FP values from zmm2 and store results in ymm1/m256 subject to writemask k1.
GENERAL	VEXTRACTF64X4	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4
VEXTRACTF64X4	YMM/M256{K}{Z},ZMM,IMM8	AVX512_F	VEXTRACTF64X4 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of packed DP FP values from zmm2 and store results in ymm1/m256 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VEXTRACTI128	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4
VEXTRACTI128	XMM/M128,YMM,IMM8	AVX2	VEXTRACTI128 XMM1/M128,YMM2,IMM8	Extract 128 bits of integer data from ymm2 and store results in xmm1/m128.
GENERAL	VEXTRACTI32X4	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4
VEXTRACTI32X4	XMM/M128{K}{Z},YMM,IMM8	AVX512_VL,AVX512_F	VEXTRACTI32X4 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of double-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI32X4	XMM/M128{K}{Z},ZMM,IMM8	AVX512_F	VEXTRACTI32X4 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of double-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1.
GENERAL	VEXTRACTI64X2	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4
VEXTRACTI64X2	XMM/M128{K}{Z},YMM,IMM8	AVX512_VL,AVX512_DQ	VEXTRACTI64X2 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of quad-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI64X2	XMM/M128{K}{Z},ZMM,IMM8	AVX512_DQ	VEXTRACTI64X2 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of quad-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1.
GENERAL	VEXTRACTI32X8	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4
VEXTRACTI32X8	YMM/M256{K}{Z},ZMM,IMM8	AVX512_DQ	VEXTRACTI32X8 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of double-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1.
GENERAL	VEXTRACTI64X4	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4
VEXTRACTI64X4	YMM/M256{K}{Z},ZMM,IMM8	AVX512_F	VEXTRACTI64X4 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of quad-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFIXUPIMMPD	Fix Up Special Packed Float64 Values	VFIXUPIMMPD
VFIXUPIMMPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VFIXUPIMMPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Fix up special numbers in float64 vector xmm1, float64 vector xmm2 and int64 vector xmm3/m128/m64bcst and store the result in xmm1, under writemask.
VFIXUPIMMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VFIXUPIMMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Fix up special numbers in float64 vector ymm1, float64 vector ymm2 and int64 vector ymm3/m256/m64bcst and store the result in ymm1, under writemask.
VFIXUPIMMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512_F	VFIXUPIMMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Fix up elements of float64 vector in zmm2 using int64 vector table in zmm3/m512/m64bcst, combine with preserved elements from zmm1, and store the result in zmm1.
;--------------------------------------------------------
GENERAL	VFIXUPIMMPS	Fix Up Special Packed Float32 Values	VFIXUPIMMPS
VFIXUPIMMPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VFIXUPIMMPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Fix up special numbers in float32 vector xmm1, float32 vector xmm2 and int32 vector xmm3/m128/m32bcst and store the result in xmm1, under writemask.
VFIXUPIMMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VFIXUPIMMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Fix up special numbers in float32 vector ymm1, float32 vector ymm2 and int32 vector ymm3/m256/m32bcst and store the result in ymm1, under writemask.
VFIXUPIMMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512_F	VFIXUPIMMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Fix up elements of float32 vector in zmm2 using int32 vector table in zmm3/m512/m32bcst, combine with preserved elements from zmm1, and store the result in zmm1.
;--------------------------------------------------------
GENERAL	VFIXUPIMMSD	Fix Up Special Scalar Float64 Value	VFIXUPIMMSD
VFIXUPIMMSD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512_F	VFIXUPIMMSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Fix up a float64 number in the low quadword element of xmm2 using scalar int32 table in xmm3/m64 and store the result in xmm1.
;--------------------------------------------------------
GENERAL	VFIXUPIMMSS	Fix Up Special Scalar Float32 Value	VFIXUPIMMSS
VFIXUPIMMSS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512_F	VFIXUPIMMSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Fix up a float32 number in the low doubleword element in xmm2 using scalar int32 table in xmm3/m32 and store the result in xmm1.
;--------------------------------------------------------
GENERAL	VFMADD132PD	Fused Multiply-Add of Packed Double- Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD
VFMADD132PD	XMM,XMM,XMM/M128	FMA	VFMADD132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1.
VFMADD132PD	YMM,YMM,YMM/M256	FMA	VFMADD132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1.
VFMADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADD132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, add to xmm2 and put result in xmm1.
VFMADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, add to ymm2 and put result in ymm1.
VFMADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, add to zmm2 and put result in zmm1.
GENERAL	VFMADD213PD	Fused Multiply-Add of Packed Double- Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD
VFMADD213PD	XMM,XMM,XMM/M128	FMA	VFMADD213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1.
VFMADD213PD	YMM,YMM,YMM/M256	FMA	VFMADD213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1.
VFMADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, add to xmm3/m128/m64bcst and put result in xmm1.
VFMADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, add to ymm3/m256/m64bcst and put result in ymm1.
VFMADD213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADD213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm2, add to zmm3/m512/m64bcst and put result in zmm1.
GENERAL	VFMADD231PD	Fused Multiply-Add of Packed Double- Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD
VFMADD231PD	XMM,XMM,XMM/M128	FMA	VFMADD231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1.
VFMADD231PD	YMM,YMM,YMM/M256	FMA	VFMADD231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1.
VFMADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, add to xmm1 and put result in xmm1.
VFMADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, add to ymm1 and put result in ymm1.
VFMADD231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADD231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, add to zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFMADD132PS	Fused Multiply-Add of Packed Single- Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS
VFMADD132PS	XMM,XMM,XMM/M128	FMA	VFMADD132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1.
VFMADD132PS	YMM,YMM,YMM/M256	FMA	VFMADD132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1.
VFMADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, add to xmm2 and put result in xmm1.
VFMADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, add to ymm2 and put result in ymm1.
VFMADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, add to zmm2 and put result in zmm1.
GENERAL	VFMADD213PS	Fused Multiply-Add of Packed Single- Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS
VFMADD213PS	XMM,XMM,XMM/M128	FMA	VFMADD213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1.
VFMADD213PS	YMM,YMM,YMM/M256	FMA	VFMADD213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1.
VFMADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, add to xmm3/m128/m32bcst and put result in xmm1.
VFMADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, add to ymm3/m256/m32bcst and put result in ymm1.
VFMADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, add to zmm3/m512/m32bcst and put result in zmm1.
GENERAL	VFMADD231PS	Fused Multiply-Add of Packed Single- Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS
VFMADD231PS	XMM,XMM,XMM/M128	FMA	VFMADD231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1.
VFMADD231PS	YMM,YMM,YMM/M256	FMA	VFMADD231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1.
VFMADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, add to xmm1 and put result in xmm1.
VFMADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, add to ymm1 and put result in ymm1.
VFMADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, add to zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFMADD132SD	Fused Multiply-Add of Scalar Double- Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD
VFMADD132SD	XMM,XMM,XMM/M64	FMA	VFMADD132SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1.
VFMADD132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMADD132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1.
GENERAL	VFMADD213SD	Fused Multiply-Add of Scalar Double- Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD
VFMADD213SD	XMM,XMM,XMM/M64	FMA	VFMADD213SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1.
VFMADD213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMADD213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1.
GENERAL	VFMADD231SD	Fused Multiply-Add of Scalar Double- Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD
VFMADD231SD	XMM,XMM,XMM/M64	FMA	VFMADD231SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1.
VFMADD231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMADD231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFMADD132SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS
VFMADD132SS	XMM,XMM,XMM/M32	FMA	VFMADD132SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1.
VFMADD132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMADD132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1.
GENERAL	VFMADD213SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS
VFMADD213SS	XMM,XMM,XMM/M32	FMA	VFMADD213SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1.
VFMADD213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMADD213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1.
GENERAL	VFMADD231SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS
VFMADD231SS	XMM,XMM,XMM/M32	FMA	VFMADD231SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1.
VFMADD231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMADD231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFMADDSUB132PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD
VFMADDSUB132PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, add/subtract elements in xmm2 and put result in xmm1.
VFMADDSUB132PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, add/subtract elements in ymm2 and put result in ymm1.
VFMADDSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, add/subtract elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMADDSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMADDSUB132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADDSUB132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1.
GENERAL	VFMADDSUB213PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD
VFMADDSUB213PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, add/subtract elements in xmm3/mem and put result in xmm1.
VFMADDSUB213PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, add/subtract elements in ymm3/mem and put result in ymm1.
VFMADDSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMADDSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMADDSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADDSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1and zmm2, add/subtract elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1.
GENERAL	VFMADDSUB231PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD
VFMADDSUB231PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, add/subtract elements in xmm1 and put result in xmm1.
VFMADDSUB231PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, add/subtract elements in ymm1 and put result in ymm1.
VFMADDSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMADDSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMADDSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMADDSUB231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMADDSUB231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFMADDSUB132PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS
VFMADDSUB132PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, add/subtract elements in xmm2 and put result in xmm1.
VFMADDSUB132PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, add/subtract elements in ymm2 and put result in ymm1.
VFMADDSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, add/subtract elements in zmm2 and put result in xmm1 subject to writemask k1.
VFMADDSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMADDSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADDSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1.
GENERAL	VFMADDSUB213PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS
VFMADDSUB213PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, add/subtract elements in xmm3/mem and put result in xmm1.
VFMADDSUB213PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, add/subtract elements in ymm3/mem and put result in ymm1.
VFMADDSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1.
VFMADDSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1.
VFMADDSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADDSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, add/subtract elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1.
GENERAL	VFMADDSUB231PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS
VFMADDSUB231PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, add/subtract elements in xmm1 and put result in xmm1.
VFMADDSUB231PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, add/subtract elements in ymm1 and put result in ymm1.
VFMADDSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMADDSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMADDSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMADDSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMADDSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFMSUB132PD	Fused Multiply-Subtract of Packed Double- Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD
VFMSUB132PD	XMM,XMM,XMM/M128	FMA	VFMSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1.
VFMSUB132PD	YMM,YMM,YMM/M256	FMA	VFMSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1.
VFMSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, subtract xmm2 and put result in xmm1 subject to writemask k1.
VFMSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, subtract ymm2 and put result in ymm1 subject to writemask k1.
VFMSUB132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUB132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, subtract zmm2 and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUB213PD	Fused Multiply-Subtract of Packed Double- Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD
VFMSUB213PD	XMM,XMM,XMM/M128	FMA	VFMSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1.
VFMSUB213PD	YMM,YMM,YMM/M256	FMA	VFMSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1.
VFMSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, subtract   xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, subtract ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm2, subtract zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUB231PD	Fused Multiply-Subtract of Packed Double- Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD
VFMSUB231PD	XMM,XMM,XMM/M128	FMA	VFMSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1.
VFMSUB231PD	YMM,YMM,YMM/M256	FMA	VFMSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1.S
VFMSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, subtract   xmm1 and put result in xmm1 subject to writemask k1.
VFMSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, subtract ymm1 and put result in ymm1 subject to writemask k1.
VFMSUB231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUB231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, subtract zmm1 and put result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFMSUB132PS	Fused Multiply-Subtract of Packed Single- Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS
VFMSUB132PS	XMM,XMM,XMM/M128	FMA	VFMSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1.
VFMSUB132PS	YMM,YMM,YMM/M256	FMA	VFMSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1.
VFMSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, subtract xmm2 and put result in xmm1.
VFMSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, subtract ymm2 and put result in ymm1.
VFMSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, subtract zmm2 and put result in zmm1.
GENERAL	VFMSUB213PS	Fused Multiply-Subtract of Packed Single- Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS
VFMSUB213PS	XMM,XMM,XMM/M128	FMA	VFMSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1.
VFMSUB213PS	YMM,YMM,YMM/M256	FMA	VFMSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1.
VFMSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, subtract xmm3/m128/m32bcst and put result in xmm1.
VFMSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, subtract ymm3/m256/m32bcst and put result in ymm1.
VFMSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, subtract zmm3/m512/m32bcst and put result in zmm1.
GENERAL	VFMSUB231PS	Fused Multiply-Subtract of Packed Single- Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS
VFMSUB231PS	XMM,XMM,XMM/M128	FMA	VFMSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1.
VFMSUB231PS	YMM,YMM,YMM/M256	FMA	VFMSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1.
VFMSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, subtract xmm1 and put result in xmm1.
VFMSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, subtract ymm1 and put result in ymm1.
VFMSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, subtract zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFMSUB132SD	Fused Multiply-Subtract of Scalar Double- Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD
VFMSUB132SD	XMM,XMM,XMM/M64	FMA	VFMSUB132SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1.
VFMSUB132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMSUB132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1.
GENERAL	VFMSUB213SD	Fused Multiply-Subtract of Scalar Double- Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD
VFMSUB213SD	XMM,XMM,XMM/M64	FMA	VFMSUB213SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1.
VFMSUB213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMSUB213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1.
GENERAL	VFMSUB231SD	Fused Multiply-Subtract of Scalar Double- Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD
VFMSUB231SD	XMM,XMM,XMM/M64	FMA	VFMSUB231SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1.
VFMSUB231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFMSUB231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFMSUB132SS	Fused Multiply-Subtract of Scalar Single- Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS
VFMSUB132SS	XMM,XMM,XMM/M32	FMA	VFMSUB132SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1.
VFMSUB132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMSUB132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1.
GENERAL	VFMSUB213SS	Fused Multiply-Subtract of Scalar Single- Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS
VFMSUB213SS	XMM,XMM,XMM/M32	FMA	VFMSUB213SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1.
VFMSUB213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMSUB213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1.
GENERAL	VFMSUB231SS	Fused Multiply-Subtract of Scalar Single- Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS
VFMSUB231SS	XMM,XMM,XMM/M32	FMA	VFMSUB231SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1.
VFMSUB231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFMSUB231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFMSUBADD132PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD
VFMSUBADD132PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, subtract/add elements in xmm2 and put result in xmm1.
VFMSUBADD132PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, subtract/add elements in ymm2 and put result in ymm1.
VFMSUBADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMSUBADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMSUBADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUBADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUBADD213PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD
VFMSUBADD213PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, subtract/add elements in xmm3/mem and put result in xmm1.
VFMSUBADD213PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, subtract/add elements in ymm3/mem and put result in ymm1.
VFMSUBADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMSUBADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMSUBADD213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUBADD213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUBADD231PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD
VFMSUBADD231PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, subtract/add elements in xmm1 and put result in xmm1.
VFMSUBADD231PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, subtract/add elements in ymm1 and put result in ymm1.
VFMSUBADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMSUBADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFMSUBADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMSUBADD231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFMSUBADD231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFMSUBADD132PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS
VFMSUBADD132PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, subtract/add elements in xmm2 and put result in xmm1.
VFMSUBADD132PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, subtract/add elements in ymm2 and put result in ymm1.
VFMSUBADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMSUBADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMSUBADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUBADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUBADD213PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS
VFMSUBADD213PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, subtract/add elements in xmm3/mem and put result in xmm1.
VFMSUBADD213PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, subtract/add elements in ymm3/mem and put result in ymm1.
VFMSUBADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1.
VFMSUBADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1.
VFMSUBADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUBADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1.
GENERAL	VFMSUBADD231PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS
VFMSUBADD231PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, subtract/add elements in xmm1 and put result in xmm1.
VFMSUBADD231PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, subtract/add elements in ymm1 and put result in ymm1.
VFMSUBADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMSUBADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFMSUBADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMSUBADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFMSUBADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VFNMADD132PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD
VFNMADD132PD	XMM,XMM,XMM/M128	FMA	VFNMADD132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PD	YMM,YMM,YMM/M256	FMA	VFNMADD132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMADD132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm2 and put result in zmm1.
GENERAL	VFNMADD213PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD
VFNMADD213PD	XMM,XMM,XMM/M128	FMA	VFNMADD213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213PD	YMM,YMM,YMM/M256	FMA	VFNMADD213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1.
VFNMADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m64bcst and put result in xmm1.
VFNMADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m64bcst and put result in ymm1.
VFNMADD213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMADD213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m64bcst and put result in zmm1.
GENERAL	VFNMADD231PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD
VFNMADD231PD	XMM,XMM,XMM/M128	FMA	VFNMADD231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PD	YMM,YMM,YMM/M256	FMA	VFNMADD231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMADD231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFNMADD132PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS
VFNMADD132PS	XMM,XMM,XMM/M128	FMA	VFNMADD132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PS	YMM,YMM,YMM/M256	FMA	VFNMADD132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_VL,AVX512_F	VFNMADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm2 and put result in zmm1.
GENERAL	VFNMADD213PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS
VFNMADD213PS	XMM,XMM,XMM/M128	FMA	VFNMADD213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213PS	YMM,YMM,YMM/M256	FMA	VFNMADD213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1.
VFNMADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m32bcst and put result in xmm1.
VFNMADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m32bcst and put result in ymm1.
VFNMADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFNMADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m32bcst and put result in zmm1.
GENERAL	VFNMADD231PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS
VFNMADD231PS	XMM,XMM,XMM/M128	FMA	VFNMADD231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PS	YMM,YMM,YMM/M256	FMA	VFNMADD231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFNMADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFNMADD132SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD
VFNMADD132SD	XMM,XMM,XMM/M64	FMA	VFNMADD132SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMADD132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm3/m64, negate the multiplication result and add to xmm2 and put result in xmm1.
GENERAL	VFNMADD213SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD
VFNMADD213SD	XMM,XMM,XMM/M64	FMA	VFNMADD213SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMADD213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m64 and put result in xmm1.
GENERAL	VFNMADD231SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD
VFNMADD231SD	XMM,XMM,XMM/M64	FMA	VFNMADD231SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMADD231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm2 and xmm3/m64, negate the multiplication result and add to xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFNMADD132SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS
VFNMADD132SS	XMM,XMM,XMM/M32	FMA	VFNMADD132SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMADD132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1.
GENERAL	VFNMADD213SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS
VFNMADD213SS	XMM,XMM,XMM/M32	FMA	VFNMADD213SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1.
VFNMADD213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMADD213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1.
GENERAL	VFNMADD231SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS
VFNMADD231SS	XMM,XMM,XMM/M32	FMA	VFNMADD231SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMADD231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFNMSUB132PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD
VFNMSUB132PD	XMM,XMM,XMM/M128	FMA	VFNMSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PD	YMM,YMM,YMM/M256	FMA	VFNMSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMSUB132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm2 and put result in zmm1.
GENERAL	VFNMSUB213PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD
VFNMSUB213PD	XMM,XMM,XMM/M128	FMA	VFNMSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213PD	YMM,YMM,YMM/M256	FMA	VFNMSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1.
VFNMSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m64bcst and put result in xmm1.
VFNMSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m64bcst and put result in ymm1.
VFNMSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m64bcst and put result in zmm1.
GENERAL	VFNMSUB231PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD
VFNMSUB231PD	XMM,XMM,XMM/M128	FMA	VFNMSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed DP FP values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PD	YMM,YMM,YMM/M256	FMA	VFNMSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed DP FP values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VFNMSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed DP FP values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VFNMSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed DP FP values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VFNMSUB231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed DP FP values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFNMSUB132PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS
VFNMSUB132PS	XMM,XMM,XMM/M128	FMA	VFNMSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PS	YMM,YMM,YMM/M256	FMA	VFNMSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFNMSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and subtract zmm2 and put result in zmm1.
GENERAL	VFNMSUB213PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS
VFNMSUB213PS	XMM,XMM,XMM/M128	FMA	VFNMSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213PS	YMM,YMM,YMM/M256	FMA	VFNMSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1.
VFNMSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m32bcst and put result in xmm1.
VFNMSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m32bcst and put result in ymm1.
VFNMSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFNMSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m32bcst and put result in zmm1.
GENERAL	VFNMSUB231PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS
VFNMSUB231PS	XMM,XMM,XMM/M128	FMA	VFNMSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed SP FP values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PS	YMM,YMM,YMM/M256	FMA	VFNMSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed SP FP values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VFNMSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed SP FP values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result subtract add to xmm1 and put result in xmm1.
VFNMSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VFNMSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed SP FP values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result subtract add to ymm1 and put result in ymm1.
VFNMSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VFNMSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed SP FP values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result subtract add to zmm1 and put result in zmm1.
;--------------------------------------------------------
GENERAL	VFNMSUB132SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD
VFNMSUB132SD	XMM,XMM,XMM/M64	FMA	VFNMSUB132SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMSUB132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm3/m64, negate the multiplication result and subtract xmm2 and put result in xmm1.
GENERAL	VFNMSUB213SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD
VFNMSUB213SD	XMM,XMM,XMM/M64	FMA	VFNMSUB213SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMSUB213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m64 and put result in xmm1.
GENERAL	VFNMSUB231SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD
VFNMSUB231SD	XMM,XMM,XMM/M64	FMA	VFNMSUB231SD XMM1,XMM2,XMM3/M64	Multiply scalar DP FP value from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VFNMSUB231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar DP FP value from xmm2 and xmm3/m64, negate the multiplication result and subtract xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFNMSUB132SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS
VFNMSUB132SS	XMM,XMM,XMM/M32	FMA	VFNMSUB132SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMSUB132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1.
GENERAL	VFNMSUB213SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS
VFNMSUB213SS	XMM,XMM,XMM/M32	FMA	VFNMSUB213SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1.
VFNMSUB213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMSUB213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1.
GENERAL	VFNMSUB231SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS
VFNMSUB231SS	XMM,XMM,XMM/M32	FMA	VFNMSUB231SS XMM1,XMM2,XMM3/M32	Multiply scalar SP FP value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VFNMSUB231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar SP FP value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1.
;--------------------------------------------------------
GENERAL	VFPCLASSPD	Tests Types Of a Packed Float64 Values	VFPCLASSPD
VFPCLASSPD	K{K},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VFPCLASSPD K2{K1},XMM2/M128/M64BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPD	K{K},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VFPCLASSPD K2{K1},YMM2/M256/M64BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPD	K{K},ZMM/M512/M64BCST,IMM8	AVX512_DQ	VFPCLASSPD K2{K1},ZMM2/M512/M64BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
;--------------------------------------------------------
GENERAL	VFPCLASSPS	Tests Types Of a Packed Float32 Values	VFPCLASSPS
VFPCLASSPS	K{K},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VFPCLASSPS K2{K1},XMM2/M128/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPS	K{K},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VFPCLASSPS K2{K1},YMM2/M256/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPS	K{K},ZMM/M512/M32BCST,IMM8	AVX512_DQ	VFPCLASSPS K2{K1},ZMM2/M512/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
;--------------------------------------------------------
GENERAL	VFPCLASSSD	Tests Types Of a Scalar Float64 Values	VFPCLASSSD
VFPCLASSSD	K{K},XMM/M64,IMM8	AVX512_DQ	VFPCLASSSD K2{K1},XMM2/M64,IMM8	Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result.
;--------------------------------------------------------
GENERAL	VFPCLASSSS	Tests Types Of a Scalar Float32 Values	VFPCLASSSS
VFPCLASSSS	K{K},XMM/M32,IMM8	AVX512_DQ	VFPCLASSSS K2{K1},XMM2/M32,IMM8	Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result.
;--------------------------------------------------------
GENERAL	VGATHERDPD	Gather Packed DP FP Values Using Signed Dword/Qword Indices	VGATHERDPD_VGATHERQPD
VGATHERDPD	XMM,VM32X,XMM	AVX2	VGATHERDPD XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather double-pre- cision FP values from memory conditioned on mask speci- fied by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERDPD	YMM,VM32X,YMM	AVX2	VGATHERDPD YMM1,VM32X,YMM2	Using dword indices specified in vm32x, gather double-pre- cision FP values from memory conditioned on mask speci- fied by ymm2. Conditionally gathered elements are merged into ymm1.
GENERAL	VGATHERQPD	Gather Packed DP FP Values Using Signed Dword/Qword Indices	VGATHERDPD_VGATHERQPD
VGATHERQPD	XMM,VM64X,XMM	AVX2	VGATHERQPD XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather double-pre- cision FP values from memory conditioned on mask speci- fied by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERQPD	YMM,VM64Y,YMM	AVX2	VGATHERQPD YMM1,VM64Y,YMM2	Using qword indices specified in vm64y, gather double-pre- cision FP values from memory conditioned on mask speci- fied by ymm2. Conditionally gathered elements are merged into ymm1.
;--------------------------------------------------------
GENERAL	VGATHERDPS	Gather Packed Single, Packed Double with Signed Dword	VGATHERDPS_VGATHERDPD
VGATHERDPS	XMM{K},VM32X	AVX512_VL,AVX512_F	VGATHERDPS XMM1{K1},VM32X	Using signed dword indices, gather SP FP values from memory using k1 as completion mask.
VGATHERDPS	YMM{K},VM32Y	AVX512_VL,AVX512_F	VGATHERDPS YMM1{K1},VM32Y	Using signed dword indices, gather SP FP values from memory using k1 as completion mask.
VGATHERDPS	ZMM{K},VM32Z	AVX512_F	VGATHERDPS ZMM1{K1},VM32Z	Using signed dword indices, gather SP FP values from memory using k1 as completion mask.
GENERAL	VGATHERDPD	Gather Packed Single, Packed Double with Signed Dword	VGATHERDPS_VGATHERDPD
VGATHERDPD	XMM{K},VM32X	AVX512_VL,AVX512_F	VGATHERDPD XMM1{K1},VM32X	Using signed dword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask.
VGATHERDPD	YMM{K},VM32X	AVX512_VL,AVX512_F	VGATHERDPD YMM1{K1},VM32X	Using signed dword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask.
VGATHERDPD	ZMM{K},VM32Y	AVX512_F	VGATHERDPD ZMM1{K1},VM32Y	Using signed dword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask.
;--------------------------------------------------------
GENERAL	VGATHERDPS	Gather Packed SP FP values Using Signed Dword/Qword Indices	VGATHERDPS_VGATHERQPS
VGATHERDPS	XMM,VM32X,XMM	AVX2	VGATHERDPS XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather single-preci- sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERDPS	YMM,VM32Y,YMM	AVX2	VGATHERDPS YMM1,VM32Y,YMM2	Using dword indices specified in vm32y, gather single-preci- sion FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
GENERAL	VGATHERQPS	Gather Packed SP FP values Using Signed Dword/Qword Indices	VGATHERDPS_VGATHERQPS
VGATHERQPS	XMM,VM64X,XMM	AVX2	VGATHERQPS XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather single-preci- sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERQPS	XMM,VM64Y,XMM	AVX2	VGATHERQPS XMM1,VM64Y,XMM2	Using qword indices specified in vm64y, gather single-preci- sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
;--------------------------------------------------------
GENERAL	VGATHERPF0DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD
VGATHERPF0DPS	VM32Z{K}	AVX512_PF	VGATHERPF0DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing SP data using opmask k1 and T0 hint.
GENERAL	VGATHERPF0QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD
VGATHERPF0QPS	VM64Z{K}	AVX512_PF	VGATHERPF0QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing SP data using opmask k1 and T0 hint.
GENERAL	VGATHERPF0DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD
VGATHERPF0DPD	VM32Y{K}	AVX512_PF	VGATHERPF0DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing DP data using opmask k1 and T0 hint.
GENERAL	VGATHERPF0QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD
VGATHERPF0QPD	VM64Z{K}	AVX512_PF	VGATHERPF0QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing DP data using opmask k1 and T0 hint.
;--------------------------------------------------------
GENERAL	VGATHERPF1DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD
VGATHERPF1DPS	VM32Z{K}	AVX512_PF	VGATHERPF1DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing SP data using opmask k1 and T1 hint.
GENERAL	VGATHERPF1QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD
VGATHERPF1QPS	VM64Z{K}	AVX512_PF	VGATHERPF1QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing SP data using opmask k1 and T1 hint.
GENERAL	VGATHERPF1DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD
VGATHERPF1DPD	VM32Y{K}	AVX512_PF	VGATHERPF1DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing DP data using opmask k1 and T1 hint.
GENERAL	VGATHERPF1QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD
VGATHERPF1QPD	VM64Z{K}	AVX512_PF	VGATHERPF1QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing DP data using opmask k1 and T1 hint.
;--------------------------------------------------------
GENERAL	VGATHERQPS	Gather Packed Single, Packed Double with Signed Qword Indices	VGATHERQPS_VGATHERQPD
VGATHERQPS	XMM{K},VM64X	AVX512_VL,AVX512_F	VGATHERQPS XMM1{K1},VM64X	Using signed qword indices, gather SP FP values from memory using k1 as completion mask.
VGATHERQPS	XMM{K},VM64Y	AVX512_VL,AVX512_F	VGATHERQPS XMM1{K1},VM64Y	Using signed qword indices, gather SP FP values from memory using k1 as completion mask.
VGATHERQPS	YMM{K},VM64Z	AVX512_F	VGATHERQPS YMM1{K1},VM64Z	Using signed qword indices, gather SP FP values from memory using k1 as completion mask.
GENERAL	VGATHERQPD	Gather Packed Single, Packed Double with Signed Qword Indices	VGATHERQPS_VGATHERQPD
VGATHERQPD	XMM{K},VM64X	AVX512_VL,AVX512_F	VGATHERQPD XMM1{K1},VM64X	Using signed qword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask.
VGATHERQPD	YMM{K},VM64Y	AVX512_VL,AVX512_F	VGATHERQPD YMM1{K1},VM64Y	Using signed qword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask.
VGATHERQPD	ZMM{K},VM64Z	AVX512_F	VGATHERQPD ZMM1{K1},VM64Z	Using signed qword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask.
;--------------------------------------------------------
GENERAL	VGETEXPPD	Convert Exponents of Packed DP FP Values to DP FP Values	VGETEXPPD
VGETEXPPD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VGETEXPPD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert the exponent of packed DP FP values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VGETEXPPD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert the exponent of packed DP FP values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_F	VGETEXPPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert the exponent of packed DP FP values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination under writemask k1.
;--------------------------------------------------------
GENERAL	VGETEXPPS	Convert Exponents of Packed SP FP Values to SP FP Values	VGETEXPPS
VGETEXPPS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VGETEXPPS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert the exponent of packed SP FP values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VGETEXPPS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert the exponent of packed SP FP values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_F	VGETEXPPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert the exponent of packed SP FP values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
;--------------------------------------------------------
GENERAL	VGETEXPSD	Convert Exponents of Scalar DP FP Values to DP FP Value	VGETEXPSD
VGETEXPSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512_F	VGETEXPSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Convert the biased exponent (bits 62:52) of the low DP FP value in xmm3/m64 to a DP FP value representing unbiased integer exponent. Stores the result to the low 64-bit of xmm1 under the writemask k1 and merge with the other elements of xmm2.
;--------------------------------------------------------
GENERAL	VGETEXPSS	Convert Exponents of Scalar SP FP Values to SP FP Value	VGETEXPSS
VGETEXPSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_F	VGETEXPSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Convert the biased exponent (bits 30:23) of the low SP FP value in xmm3/m32 to a SP FP value representing unbiased integer exponent. Stores the result to xmm1 under the writemask k1 and merge with the other elements of xmm2.
;--------------------------------------------------------
GENERAL	VGETMANTPD	Extract Float64 Vector of Normalized Mantissas from Float64 Vector	VGETMANTPD
VGETMANTPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VGETMANTPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Get Normalized Mantissa from float64 vector xmm2/m128/m64bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VGETMANTPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Get Normalized Mantissa from float64 vector ymm2/m256/m64bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512_F	VGETMANTPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Get Normalized Mantissa from float64 vector zmm2/m512/m64bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
;--------------------------------------------------------
GENERAL	VGETMANTPS	Extract Float32 Vector of Normalized Mantissas from Float32 Vector	VGETMANTPS
VGETMANTPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VGETMANTPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Get normalized mantissa from float32 vector xmm2/m128/m32bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VGETMANTPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Get normalized mantissa from float32 vector ymm2/m256/m32bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512_F	VGETMANTPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Get normalized mantissa from float32 vector zmm2/m512/m32bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
;--------------------------------------------------------
GENERAL	VGETMANTSD	Extract Float64 of Normalized Mantissas from Float64 Scalar	VGETMANTSD
VGETMANTSD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512_F	VGETMANTSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Extract the normalized mantissa of the low float64 element in xmm3/m64 using imm8 for sign control and mantissa interval normalization. Store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2.
;--------------------------------------------------------
GENERAL	VGETMANTSS	Extract Float32 Vector of Normalized Mantissa from Float32 Vector	VGETMANTSS
VGETMANTSS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512_F	VGETMANTSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Extract the normalized mantissa from the low float32 element of xmm3/m32 using imm8 for sign control and mantissa interval normalization, store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2.
;--------------------------------------------------------
GENERAL	VINSERTF128	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4
VINSERTF128	YMM,YMM,XMM/M128,IMM8	AVX	VINSERTF128 YMM1,YMM2,XMM3/M128,IMM8	Insert 128 bits of packed FP values from xmm3/m128 and the remaining values from ymm2 into ymm1.
GENERAL	VINSERTF32X4	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4
VINSERTF32X4	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512_VL,AVX512_F	VINSERTF32X4 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed SP FP values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTF32X4	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512_F	VINSERTF32X4 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed SP FP values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTF64X2	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4
VINSERTF64X2	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512_VL,AVX512_DQ	VINSERTF64X2 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed DP FP values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTF64X2	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512_DQ	VINSERTF64X2 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed DP FP values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTF32X8	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4
VINSERTF32X8	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512_DQ	VINSERTF32X8 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed SP FP values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTF64X4	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4
VINSERTF64X4	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512_F	VINSERTF64X4 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed DP FP values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	VINSERTI128	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4
VINSERTI128	YMM,YMM,XMM/M128,IMM8	AVX2	VINSERTI128 YMM1,YMM2,XMM3/M128,IMM8	Insert 128 bits of integer data from xmm3/m128 and the remaining values from ymm2 into ymm1.
GENERAL	VINSERTI32X4	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4
VINSERTI32X4	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512_VL,AVX512_F	VINSERTI32X4 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTI32X4	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512_F	VINSERTI32X4 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTI64X2	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4
VINSERTI64X2	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512_VL,AVX512_DQ	VINSERTI64X2 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTI64X2	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512_DQ	VINSERTI64X2 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTI32X8	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4
VINSERTI32X8	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512_DQ	VINSERTI32X8 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed doubleword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
GENERAL	VINSERTI64X4	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4
VINSERTI64X4	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512_F	VINSERTI64X4 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed quadword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	VMASKMOVPS	Conditional SIMD Packed Loads and Stores	VMASKMOV
VMASKMOVPS	XMM,XMM,M128	AVX	VMASKMOVPS XMM1,XMM2,M128	Conditionally load packed SP values from m128 using mask in xmm2 and store in xmm1.
VMASKMOVPS	YMM,YMM,M256	AVX	VMASKMOVPS YMM1,YMM2,M256	Conditionally load packed SP values from m256 using mask in ymm2 and store in ymm1.
VMASKMOVPS	M128,XMM,XMM	AVX	VMASKMOVPS M128,XMM1,XMM2	Conditionally store packed SP values from xmm2 using mask in xmm1.
VMASKMOVPS	M256,YMM,YMM	AVX	VMASKMOVPS M256,YMM1,YMM2	Conditionally store packed SP values from ymm2 using mask in ymm1.
GENERAL	VMASKMOVPD	Conditional SIMD Packed Loads and Stores	VMASKMOV
VMASKMOVPD	XMM,XMM,M128	AVX	VMASKMOVPD XMM1,XMM2,M128	Conditionally load packed DP values from m128 using mask in xmm2 and store in xmm1.
VMASKMOVPD	YMM,YMM,M256	AVX	VMASKMOVPD YMM1,YMM2,M256	Conditionally load packed DP values from m256 using mask in ymm2 and store in ymm1.
VMASKMOVPD	M128,XMM,XMM	AVX	VMASKMOVPD M128,XMM1,XMM2	Conditionally store packed DP values from xmm2 using mask in xmm1.
VMASKMOVPD	M256,YMM,YMM	AVX	VMASKMOVPD M256,YMM1,YMM2	Conditionally store packed DP values from ymm2 using mask in ymm1.
;--------------------------------------------------------
GENERAL	VP4DPWSSD	Dot Product of Signed Words with Dword Accumulation (4-iterations)	VP4DPWSSD
VP4DPWSSD	ZMM{K}{Z},ZMM,M128	AVX512_4VNNIW	VP4DPWSSD ZMM1{K1}{Z},ZMM2+3,M128	Multiply signed words from source register block indicated by zmm2 by signed words from m128 and accumulate resulting signed dwords in zmm1.
;--------------------------------------------------------
GENERAL	VP4DPWSSDS	Dot Product of Signed Words with Dword Accumulation and Saturation  (4-iterations)	VP4DPWSSDS
VP4DPWSSDS	ZMM{K}{Z},ZMM,M128	AVX512_4VNNIW	VP4DPWSSDS ZMM1{K1}{Z},ZMM2+3,M128	Multiply signed words from source register block indicated by zmm2 by signed words from m128 and accumulate the resulting dword results with signed saturation in zmm1.
;--------------------------------------------------------
GENERAL	VPBLENDD	Blend Packed Dwords	VPBLENDD
VPBLENDD	XMM,XMM,XMM/M128,IMM8	AVX2	VPBLENDD XMM1,XMM2,XMM3/M128,IMM8	Select dwords from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VPBLENDD	YMM,YMM,YMM/M256,IMM8	AVX2	VPBLENDD YMM1,YMM2,YMM3/M256,IMM8	Select dwords from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
;--------------------------------------------------------
GENERAL	VPBLENDMB	Blend Byte/Word Vectors Using an Opmask Control	VPBLENDMB_VPBLENDMW
VPBLENDMB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPBLENDMB XMM1{K1}{Z},XMM2,XMM3/M128	Blend byte integer vector xmm2 and byte vector xmm3/m128 and store the result in xmm1, under control mask.
VPBLENDMB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPBLENDMB YMM1{K1}{Z},YMM2,YMM3/M256	Blend byte integer vector ymm2 and byte vector ymm3/m256 and store the result in ymm1, under control mask.
VPBLENDMB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPBLENDMB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Blend byte integer vector zmm2 and byte vector zmm3/m512 and store the result in zmm1, under control mask.
GENERAL	VPBLENDMW	Blend Byte/Word Vectors Using an Opmask Control	VPBLENDMB_VPBLENDMW
VPBLENDMW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPBLENDMW XMM1{K1}{Z},XMM2,XMM3/M128	Blend word integer vector xmm2 and word vector xmm3/m128 and store the result in xmm1, under control mask.
VPBLENDMW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPBLENDMW YMM1{K1}{Z},YMM2,YMM3/M256	Blend word integer vector ymm2 and word vector ymm3/m256 and store the result in ymm1, under control mask.
VPBLENDMW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPBLENDMW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Blend word integer vector zmm2 and word vector zmm3/m512 and store the result in zmm1, under control mask.
;--------------------------------------------------------
GENERAL	VPBLENDMD	Blend Int32/Int64 Vectors Using an OpMask Control	VPBLENDMD_VPBLENDMQ
VPBLENDMD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPBLENDMD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Blend doubleword integer vector xmm2 and doubleword vector xmm3/m128/m32bcst and store the result in xmm1, under control mask.
VPBLENDMD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPBLENDMD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Blend doubleword integer vector ymm2 and doubleword vector ymm3/m256/m32bcst and store the result in ymm1, under control mask.
VPBLENDMD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPBLENDMD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Blend doubleword integer vector zmm2 and doubleword vector zmm3/m512/m32bcst and store the result in zmm1, under control mask.
GENERAL	VPBLENDMQ	Blend Int32/Int64 Vectors Using an OpMask Control	VPBLENDMD_VPBLENDMQ
VPBLENDMQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPBLENDMQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Blend quadword integer vector xmm2 and quadword vector xmm3/m128/m64bcst and store the result in xmm1, under control mask.
VPBLENDMQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPBLENDMQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Blend quadword integer vector ymm2 and quadword vector ymm3/m256/m64bcst and store the result in ymm1, under control mask.
VPBLENDMQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPBLENDMQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Blend quadword integer vector zmm2 and quadword vector zmm3/m512/m64bcst and store the result in zmm1, under control mask.
;--------------------------------------------------------
GENERAL	VPBROADCASTB	Load Integer and Broadcast	VPBROADCAST
VPBROADCASTB	XMM,XMM/M8	AVX2	VPBROADCASTB XMM1,XMM2/M8	Broadcast a byte integer in the source operand to sixteen locations in xmm1.
VPBROADCASTB	YMM,XMM/M8	AVX2	VPBROADCASTB YMM1,XMM2/M8	Broadcast a byte integer in the source operand to thirty-two locations in ymm1.
VPBROADCASTB	XMM{K}{Z},XMM/M8	AVX512_VL,AVX512_BW	VPBROADCASTB XMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTB	YMM{K}{Z},XMM/M8	AVX512_VL,AVX512_BW	VPBROADCASTB YMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTB	ZMM{K}{Z},XMM/M8	AVX512_BW	VPBROADCASTB ZMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to 64 locations in zmm1 subject to writemask k1.
GENERAL	VPBROADCASTW	Load Integer and Broadcast	VPBROADCAST
VPBROADCASTW	XMM,XMM/M16	AVX2	VPBROADCASTW XMM1,XMM2/M16	Broadcast a word integer in the source operand to eight locations in xmm1.
VPBROADCASTW	YMM,XMM/M16	AVX2	VPBROADCASTW YMM1,XMM2/M16	Broadcast a word integer in the source operand to sixteen locations in ymm1.
VPBROADCASTW	XMM{K}{Z},XMM/M16	AVX512_VL,AVX512_BW	VPBROADCASTW XMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTW	YMM{K}{Z},XMM/M16	AVX512_VL,AVX512_BW	VPBROADCASTW YMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTW	ZMM{K}{Z},XMM/M16	AVX512_BW	VPBROADCASTW ZMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to 32 locations in zmm1 subject to writemask k1.
GENERAL	VPBROADCASTD	Load Integer and Broadcast	VPBROADCAST
VPBROADCASTD	XMM,XMM/M32	AVX2	VPBROADCASTD XMM1,XMM2/M32	Broadcast a dword integer in the source operand to four locations in xmm1.
VPBROADCASTD	YMM,XMM/M32	AVX2	VPBROADCASTD YMM1,XMM2/M32	Broadcast a dword integer in the source operand to eight locations in ymm1.
VPBROADCASTD	XMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPBROADCASTD XMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTD	YMM{K}{Z},XMM/M32	AVX512_VL,AVX512_F	VPBROADCASTD YMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTD	ZMM{K}{Z},XMM/M32	AVX512_F	VPBROADCASTD ZMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in zmm1 subject to writemask k1.
GENERAL	VPBROADCASTQ	Load Integer and Broadcast	VPBROADCAST
VPBROADCASTQ	XMM,XMM/M64	AVX2	VPBROADCASTQ XMM1,XMM2/M64	Broadcast a qword element in source operand to two locations in xmm1.
VPBROADCASTQ	YMM,XMM/M64	AVX2	VPBROADCASTQ YMM1,XMM2/M64	Broadcast a qword element in source operand to four locations in ymm1.
VPBROADCASTQ	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPBROADCASTQ XMM1{K1}{Z},XMM2/M64	Broadcast a qword element in source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTQ	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_F	VPBROADCASTQ YMM1{K1}{Z},XMM2/M64	Broadcast a qword element in source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTQ	ZMM{K}{Z},XMM/M64	AVX512_F	VPBROADCASTQ ZMM1{K1}{Z},XMM2/M64	Broadcast a qword element in source operand to locations in zmm1 subject to writemask k1.
GENERAL	VBROADCASTI32X2	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI32X2	XMM{K}{Z},XMM/M64	AVX512_VL,AVX512_DQ	VBROADCASTI32X2 XMM1{K1}{Z},XMM2/M64	Broadcast two dword elements in source operand to locations in xmm1 subject to writemask k1.
VBROADCASTI32X2	YMM{K}{Z},XMM/M64	AVX512_VL,AVX512_DQ	VBROADCASTI32X2 YMM1{K1}{Z},XMM2/M64	Broadcast two dword elements in source operand to locations in ymm1 subject to writemask k1.
VBROADCASTI32X2	ZMM{K}{Z},XMM/M64	AVX512_DQ	VBROADCASTI32X2 ZMM1{K1}{Z},XMM2/M64	Broadcast two dword elements in source operand to locations in zmm1 subject to writemask k1.
GENERAL	VBROADCASTI128	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI128	YMM,M128	AVX2	VBROADCASTI128 YMM1,M128	Broadcast 128 bits of integer data in mem to low and high 128-bits in ymm1.
GENERAL	VBROADCASTI32X4	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI32X4	YMM{K}{Z},M128	AVX512_VL,AVX512_F	VBROADCASTI32X4 YMM1{K1}{Z},M128	Broadcast 128 bits of 4 doubleword integer data in mem to locations in ymm1 using writemask k1.
VBROADCASTI32X4	ZMM{K}{Z},M128	AVX512_F	VBROADCASTI32X4 ZMM1{K1}{Z},M128	Broadcast 128 bits of 4 doubleword integer data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTI64X2	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI64X2	YMM{K}{Z},M128	AVX512_VL,AVX512_DQ	VBROADCASTI64X2 YMM1{K1}{Z},M128	Broadcast 128 bits of 2 quadword integer data in mem to locations in ymm1 using writemask k1.
VBROADCASTI64X2	ZMM{K}{Z},M128	AVX512_DQ	VBROADCASTI64X2 ZMM1{K1}{Z},M128	Broadcast 128 bits of 2 quadword integer data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTI32X8	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI32X8	ZMM{K}{Z},M256	AVX512_DQ	VBROADCASTI32X8 ZMM1{K1}{Z},M256	Broadcast 256 bits of 8 doubleword integer data in mem to locations in zmm1 using writemask k1.
GENERAL	VBROADCASTI64X4	Load Integer and Broadcast	VPBROADCAST
VBROADCASTI64X4	ZMM{K}{Z},M256	AVX512_F	VBROADCASTI64X4 ZMM1{K1}{Z},M256	Broadcast 256 bits of 4 quadword integer data in mem to locations in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPBROADCASTB	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q
VPBROADCASTB	XMM{K}{Z},REG	AVX512_VL,AVX512_BW	VPBROADCASTB XMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 128-bit destination subject to writemask k1.
VPBROADCASTB	YMM{K}{Z},REG	AVX512_VL,AVX512_BW	VPBROADCASTB YMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 256-bit destination subject to writemask k1.
VPBROADCASTB	ZMM{K}{Z},REG	AVX512_BW	VPBROADCASTB ZMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 512-bit destination subject to writemask k1.
GENERAL	VPBROADCASTW	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q
VPBROADCASTW	XMM{K}{Z},REG	AVX512_VL,AVX512_BW	VPBROADCASTW XMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 128-bit destination subject to writemask k1.
VPBROADCASTW	YMM{K}{Z},REG	AVX512_VL,AVX512_BW	VPBROADCASTW YMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 256-bit destination subject to writemask k1.
VPBROADCASTW	ZMM{K}{Z},REG	AVX512_BW	VPBROADCASTW ZMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 512-bit destination subject to writemask k1.
GENERAL	VPBROADCASTD	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q
VPBROADCASTD	XMM{K}{Z},R32	AVX512_VL,AVX512_F	VPBROADCASTD XMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 128-bit destination subject to writemask k1.
VPBROADCASTD	YMM{K}{Z},R32	AVX512_VL,AVX512_F	VPBROADCASTD YMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 256-bit destination subject to writemask k1.
VPBROADCASTD	ZMM{K}{Z},R32	AVX512_F	VPBROADCASTD ZMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 512-bit destination subject to writemask k1.
GENERAL	VPBROADCASTQ	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q
VPBROADCASTQ	XMM{K}{Z},R64	AVX512_VL,AVX512_F	VPBROADCASTQ XMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 128-bit destination subject to writemask k1.
VPBROADCASTQ	YMM{K}{Z},R64	AVX512_VL,AVX512_F	VPBROADCASTQ YMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 256-bit destination subject to writemask k1.
VPBROADCASTQ	ZMM{K}{Z},R64	AVX512_F	VPBROADCASTQ ZMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 512-bit destination subject to writemask k1.
;--------------------------------------------------------
GENERAL	VPBROADCASTMB2Q	Broadcast Mask to Vector Register	VPBROADCASTM
VPBROADCASTMB2Q	XMM,K	AVX512_VL,AVX512_CD	VPBROADCASTMB2Q XMM1,K1	Broadcast low byte value in k1 to two locations in xmm1.
VPBROADCASTMB2Q	YMM,K	AVX512_VL,AVX512_CD	VPBROADCASTMB2Q YMM1,K1	Broadcast low byte value in k1 to four locations in ymm1.
VPBROADCASTMB2Q	ZMM,K	AVX512_CD	VPBROADCASTMB2Q ZMM1,K1	Broadcast low byte value in k1 to eight locations in zmm1.
GENERAL	VPBROADCASTMW2D	Broadcast Mask to Vector Register	VPBROADCASTM
VPBROADCASTMW2D	XMM,K	AVX512_VL,AVX512_CD	VPBROADCASTMW2D XMM1,K1	Broadcast low word value in k1 to four locations in xmm1.
VPBROADCASTMW2D	YMM,K	AVX512_VL,AVX512_CD	VPBROADCASTMW2D YMM1,K1	Broadcast low word value in k1 to eight locations in ymm1.
VPBROADCASTMW2D	ZMM,K	AVX512_CD	VPBROADCASTMW2D ZMM1,K1	Broadcast low word value in k1 to sixteen locations in zmm1.
;--------------------------------------------------------
GENERAL	VPCLMULQDQ	Carry-Less Multiplication Quadword	VPCLMULQDQ
VPCLMULQDQ	YMM,YMM,YMM/M256,IMM8	AVX512_VPCLMULQDQ	VPCLMULQDQ YMM1,YMM2,YMM3/M256,IMM8	Carry-less multiplication of one quadword of ymm2 by one quadword of ymm3/m256, stores the 128-bit result in ymm1. The immediate is used to determine which quadwords of ymm2 and ymm3/m256 should be used.
VPCLMULQDQ	XMM,XMM,XMM/M128,IMM8	AVX512_VL,AVX512_VPCLMULQDQ	VPCLMULQDQ XMM1,XMM2,XMM3/M128,IMM8	Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm2 and xmm3/m128 should be used.
VPCLMULQDQ	YMM,YMM,YMM/M256,IMM8	AVX512_VL,AVX512_VPCLMULQDQ	VPCLMULQDQ YMM1,YMM2,YMM3/M256,IMM8	Carry-less multiplication of one quadword of ymm2 by one quadword of ymm3/m256, stores the 128-bit result in ymm1. The immediate is used to determine which quadwords of ymm2 and ymm3/m256 should be used.
VPCLMULQDQ	ZMM,ZMM,ZMM/M512,IMM8	AVX512_F,AVX512_VPCLMULQDQ	VPCLMULQDQ ZMM1,ZMM2,ZMM3/M512,IMM8	Carry-less multiplication of one quadword of zmm2 by one quadword of zmm3/m512, stores the 128-bit result in zmm1. The immediate is used to determine which quadwords of zmm2 and zmm3/m512 should be used.
;--------------------------------------------------------
GENERAL	VPCMPB	Compare Packed Byte Values Into Mask	VPCMPB_VPCMPUB
VPCMPB	K{K},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPCMPB K1{K2},XMM2,XMM3/M128,IMM8	Compare packed signed byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPB	K{K},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPCMPB K1{K2},YMM2,YMM3/M256,IMM8	Compare packed signed byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPB	K{K},ZMM,ZMM/M512,IMM8	AVX512_BW	VPCMPB K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed signed byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
GENERAL	VPCMPUB	Compare Packed Byte Values Into Mask	VPCMPB_VPCMPUB
VPCMPUB	K{K},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPCMPUB K1{K2},XMM2,XMM3/M128,IMM8	Compare packed unsigned byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUB	K{K},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPCMPUB K1{K2},YMM2,YMM3/M256,IMM8	Compare packed unsigned byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUB	K{K},ZMM,ZMM/M512,IMM8	AVX512_BW	VPCMPUB K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed unsigned byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	VPCMPD	Compare Packed Integer Values into Mask	VPCMPD_VPCMPUD
VPCMPD	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPCMPD K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed signed doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPD	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPCMPD K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed signed doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPD	K{K},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VPCMPD K1{K2},ZMM2,ZMM3/M512/M32BCST,IMM8	Compare packed signed doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2.
GENERAL	VPCMPUD	Compare Packed Integer Values into Mask	VPCMPD_VPCMPUD
VPCMPUD	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPCMPUD K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed unsigned doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUD	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPCMPUD K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed unsigned doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUD	K{K},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VPCMPUD K1{K2},ZMM2,ZMM3/M512/M32BCST,IMM8	Compare packed unsigned doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2.
;--------------------------------------------------------
GENERAL	VPCMPQ	Compare Packed Integer Values into Mask	VPCMPQ_VPCMPUQ
VPCMPQ	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPCMPQ K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed signed quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPQ	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPCMPQ K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed signed quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPQ	K{K},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VPCMPQ K1{K2},ZMM2,ZMM3/M512/M64BCST,IMM8	Compare packed signed quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
GENERAL	VPCMPUQ	Compare Packed Integer Values into Mask	VPCMPQ_VPCMPUQ
VPCMPUQ	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPCMPUQ K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed unsigned quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUQ	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPCMPUQ K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed unsigned quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUQ	K{K},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VPCMPUQ K1{K2},ZMM2,ZMM3/M512/M64BCST,IMM8	Compare packed unsigned quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	VPCMPW	Compare Packed Word Values Into Mask	VPCMPW_VPCMPUW
VPCMPW	K{K},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPCMPW K1{K2},XMM2,XMM3/M128,IMM8	Compare packed signed word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPW	K{K},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPCMPW K1{K2},YMM2,YMM3/M256,IMM8	Compare packed signed word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPW	K{K},ZMM,ZMM/M512,IMM8	AVX512_BW	VPCMPW K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed signed word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
GENERAL	VPCMPUW	Compare Packed Word Values Into Mask	VPCMPW_VPCMPUW
VPCMPUW	K{K},XMM,XMM/M128,IMM8	AVX512_VL,AVX512_BW	VPCMPUW K1{K2},XMM2,XMM3/M128,IMM8	Compare packed unsigned word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUW	K{K},YMM,YMM/M256,IMM8	AVX512_VL,AVX512_BW	VPCMPUW K1{K2},YMM2,YMM3/M256,IMM8	Compare packed unsigned word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUW	K{K},ZMM,ZMM/M512,IMM8	AVX512_BW	VPCMPUW K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed unsigned word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	VPCOMPRESSB	Store Sparse Packed Byte/Word Integer Values into Dense Memory/Register	VPCOMPRESS
VPCOMPRESSB	M128{K},XMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSB M128{K1},XMM1	Compress up to 128 bits of packed byte values from xmm1 to m128 with writemask k1.
VPCOMPRESSB	XMM{K}{Z},XMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSB XMM1{K1}{Z},XMM2	Compress up to 128 bits of packed byte values from xmm2 to xmm1 with writemask k1.
VPCOMPRESSB	M256{K},YMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSB M256{K1},YMM1	Compress up to 256 bits of packed byte values from ymm1 to m256 with writemask k1.
VPCOMPRESSB	YMM{K}{Z},YMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSB YMM1{K1}{Z},YMM2	Compress up to 256 bits of packed byte values from ymm2 to ymm1 with writemask k1.
VPCOMPRESSB	M512{K},ZMM	AVX512_VBMI2	VPCOMPRESSB M512{K1},ZMM1	Compress up to 512 bits of packed byte values from zmm1 to m512 with writemask k1.
VPCOMPRESSB	ZMM{K}{Z},ZMM	AVX512_VBMI2	VPCOMPRESSB ZMM1{K1}{Z},ZMM2	Compress up to 512 bits of packed byte values from zmm2 to zmm1 with writemask k1.
GENERAL	VPCOMPRESSW	Store Sparse Packed Byte/Word Integer Values into Dense Memory/Register	VPCOMPRESS
VPCOMPRESSW	M128{K},XMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSW M128{K1},XMM1	Compress up to 128 bits of packed word values from xmm1 to m128 with writemask k1.
VPCOMPRESSW	XMM{K}{Z},XMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSW XMM1{K1}{Z},XMM2	Compress up to 128 bits of packed word values from xmm2 to xmm1 with writemask k1.
VPCOMPRESSW	M256{K},YMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSW M256{K1},YMM1	Compress up to 256 bits of packed word values from ymm1 to m256 with writemask k1.
VPCOMPRESSW	YMM{K}{Z},YMM	AVX512_VBMI2,AVX512_VL	VPCOMPRESSW YMM1{K1}{Z},YMM2	Compress up to 256 bits of packed word values from ymm2 to ymm1 with writemask k1.
VPCOMPRESSW	M512{K},ZMM	AVX512_VBMI2	VPCOMPRESSW M512{K1},ZMM1	Compress up to 512 bits of packed word values from zmm1 to m512 with writemask k1.
VPCOMPRESSW	ZMM{K}{Z},ZMM	AVX512_VBMI2	VPCOMPRESSW ZMM1{K1}{Z},ZMM2	Compress up to 512 bits of packed word values from zmm2 to zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VPCOMPRESSD	Store Sparse Packed Doubleword Integer Values into Dense Memory/Register	VPCOMPRESSD
VPCOMPRESSD	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VPCOMPRESSD XMM1/M128{K1}{Z},XMM2	Compress packed doubleword integer values from xmm2 to xmm1/m128 using controlmask k1.
VPCOMPRESSD	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VPCOMPRESSD YMM1/M256{K1}{Z},YMM2	Compress packed doubleword integer values from ymm2 to ymm1/m256 using controlmask k1.
VPCOMPRESSD	ZMM/M512{K}{Z},ZMM	AVX512_F	VPCOMPRESSD ZMM1/M512{K1}{Z},ZMM2	Compress packed doubleword integer values from zmm2 to zmm1/m512 using controlmask k1.
;--------------------------------------------------------
GENERAL	VPCOMPRESSQ	Store Sparse Packed Quadword Integer Values into Dense Memory/Register	VPCOMPRESSQ
VPCOMPRESSQ	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VPCOMPRESSQ XMM1/M128{K1}{Z},XMM2	Compress packed quadword integer values from xmm2 to xmm1/m128 using controlmask k1.
VPCOMPRESSQ	YMM/M256{K}{Z},YMM	AVX512_VL,AVX512_F	VPCOMPRESSQ YMM1/M256{K1}{Z},YMM2	Compress packed quadword integer values from ymm2 to ymm1/m256 using controlmask k1.
VPCOMPRESSQ	ZMM/M512{K}{Z},ZMM	AVX512_F	VPCOMPRESSQ ZMM1/M512{K1}{Z},ZMM2	Compress packed quadword integer values from zmm2 to zmm1/m512 using controlmask k1.
;--------------------------------------------------------
GENERAL	VPCONFLICTD	Detect Conflicts Within a Vector of Packed Dword/Qword Values into Dense Memory/ Register	VPCONFLICTD_Q
VPCONFLICTD	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_CD	VPCONFLICTD XMM1{K1}{Z},XMM2/M128/M32BCST	Detect duplicate double-word values in xmm2/m128/m32bcst using writemask k1.
VPCONFLICTD	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_CD	VPCONFLICTD YMM1{K1}{Z},YMM2/M256/M32BCST	Detect duplicate double-word values in ymm2/m256/m32bcst using writemask k1.
VPCONFLICTD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_CD	VPCONFLICTD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Detect duplicate double-word values in zmm2/m512/m32bcst using writemask k1.
GENERAL	VPCONFLICTQ	Detect Conflicts Within a Vector of Packed Dword/Qword Values into Dense Memory/ Register	VPCONFLICTD_Q
VPCONFLICTQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_CD	VPCONFLICTQ XMM1{K1}{Z},XMM2/M128/M64BCST	Detect duplicate quad-word values in xmm2/m128/m64bcst using writemask k1.
VPCONFLICTQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_CD	VPCONFLICTQ YMM1{K1}{Z},YMM2/M256/M64BCST	Detect duplicate quad-word values in ymm2/m256/m64bcst using writemask k1.
VPCONFLICTQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_CD	VPCONFLICTQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Detect duplicate quad-word values in zmm2/m512/m64bcst using writemask k1.
;--------------------------------------------------------
GENERAL	VPDPBUSD	Multiply and Add Unsigned and Signed Bytes	VPDPBUSD
VPDPBUSD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VNNI,AVX512_VL	VPDPBUSD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply groups of 4 pairs of signed bytes in xmm3/m128/m32bcst with corresponding unsigned bytes of xmm2, summing those products and adding them to doubleword result in xmm1 under writemask k1.
VPDPBUSD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VNNI,AVX512_VL	VPDPBUSD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply groups of 4 pairs of signed bytes in ymm3/m256/m32bcst with corresponding unsigned bytes of ymm2, summing those products and adding them to doubleword result in ymm1 under writemask k1.
VPDPBUSD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VNNI	VPDPBUSD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply groups of 4 pairs of signed bytes in zmm3/m512/m32bcst with corresponding unsigned bytes of zmm2, summing those products and adding them to doubleword result in zmm1 under writemask k1.
;--------------------------------------------------------
GENERAL	VPDPBUSDS	Multiply and Add Unsigned and Signed Bytes with Saturation	VPDPBUSDS
VPDPBUSDS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VNNI,AVX512_VL	VPDPBUSDS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply groups of 4 pairs signed bytes in xmm3/m128/m32bcst with corresponding unsigned bytes of xmm2, summing those products and adding them to doubleword result, with signed saturation in xmm1, under writemask k1.
VPDPBUSDS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VNNI,AVX512_VL	VPDPBUSDS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply groups of 4 pairs signed bytes in ymm3/m256/m32bcst with corresponding unsigned bytes of ymm2, summing those products and adding them to doubleword result, with signed saturation in ymm1, under writemask k1.
VPDPBUSDS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VNNI	VPDPBUSDS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply groups of 4 pairs signed bytes in zmm3/m512/m32bcst with corresponding unsigned bytes of zmm2, summing those products and adding them to doubleword result, with signed saturation in zmm1, under writemask k1.
;--------------------------------------------------------
GENERAL	VPDPWSSD	Multiply and Add Signed Word Integers	VPDPWSSD
VPDPWSSD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VNNI,AVX512_VL	VPDPWSSD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply groups of 2 pairs signed words in xmm3/m128/m32bcst with corresponding signed words of xmm2, summing those products and adding them to doubleword result in xmm1, under writemask k1.
VPDPWSSD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VNNI,AVX512_VL	VPDPWSSD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply groups of 2 pairs signed words in ymm3/m256/m32bcst with corresponding signed words of ymm2, summing those products and adding them to doubleword result in ymm1, under writemask k1.
VPDPWSSD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VNNI	VPDPWSSD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply groups of 2 pairs signed words in zmm3/m512/m32bcst with corresponding signed words of zmm2, summing those products and adding them to doubleword result in zmm1, under writemask k1.
;--------------------------------------------------------
GENERAL	VPDPWSSDS	Multiply and Add Word Integers with Saturation	VPDPWSSDS
VPDPWSSDS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VNNI,AVX512_VL	VPDPWSSDS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply groups of 2 pairs of signed words in xmm3/m128/m32bcst with corresponding signed words of xmm2, summing those products and adding them to doubleword result in xmm1, with signed saturation, under writemask k1.
VPDPWSSDS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VNNI,AVX512_VL	VPDPWSSDS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply groups of 2 pairs of signed words in ymm3/m256/m32bcst with corresponding signed words of ymm2, summing those products and adding them to doubleword result in ymm1, with signed saturation, under writemask k1.
VPDPWSSDS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VNNI	VPDPWSSDS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply groups of 2 pairs of signed words in zmm3/m512/m32bcst with corresponding signed words of zmm2, summing those products and adding them to doubleword result in zmm1, with signed saturation, under writemask k1.
;--------------------------------------------------------
GENERAL	VPERM2F128	Permute Floating-Point Values	VPERM2F128
VPERM2F128	YMM,YMM,YMM/M256,IMM8	AVX	VPERM2F128 YMM1,YMM2,YMM3/M256,IMM8	Permute 128-bit FP fields in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
;--------------------------------------------------------
GENERAL	VPERM2I128	Permute Integer Values	VPERM2I128
VPERM2I128	YMM,YMM,YMM/M256,IMM8	AVX2	VPERM2I128 YMM1,YMM2,YMM3/M256,IMM8	Permute 128-bit integer data in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
;--------------------------------------------------------
GENERAL	VPERMB	Permute Packed Bytes Elements	VPERMB
VPERMB	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_VBMI	VPERMB XMM1{K1}{Z},XMM2,XMM3/M128	Permute bytes in xmm3/m128 using byte indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMB	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_VBMI	VPERMB YMM1{K1}{Z},YMM2,YMM3/M256	Permute bytes in ymm3/m256 using byte indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_VBMI	VPERMB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute bytes in zmm3/m512 using byte indexes in zmm2 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMD	Permute Packed Doublewords/Words Elements	VPERMD_VPERMW
VPERMD	YMM,YMM,YMM/M256	AVX2	VPERMD YMM1,YMM2,YMM3/M256	Permute doublewords in ymm3/m256 using indices in ymm2 and store the result in ymm1.
VPERMD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute doublewords in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute doublewords in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 using writemask k1.
GENERAL	VPERMW	Permute Packed Doublewords/Words Elements	VPERMD_VPERMW
VPERMW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPERMW XMM1{K1}{Z},XMM2,XMM3/M128	Permute word integers in xmm3/m128 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPERMW YMM1{K1}{Z},YMM2,YMM3/M256	Permute word integers in ymm3/m256 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPERMW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute word integers in zmm3/m512 using indexes in zmm2 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMI2B	Full Permute of Bytes from Two Tables Overwriting the Index	VPERMI2B
VPERMI2B	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_VBMI	VPERMI2B XMM1{K1}{Z},XMM2,XMM3/M128	Permute bytes in xmm3/m128 and xmm2 using byte indexes in xmm1 and store the byte results in xmm1 using writemask k1.
VPERMI2B	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_VBMI	VPERMI2B YMM1{K1}{Z},YMM2,YMM3/M256	Permute bytes in ymm3/m256 and ymm2 using byte indexes in ymm1 and store the byte results in ymm1 using writemask k1.
VPERMI2B	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_VBMI	VPERMI2B ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute bytes in zmm3/m512 and zmm2 using byte indexes in zmm1 and store the byte results in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMI2W	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD
VPERMI2W	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPERMI2W XMM1{K1}{Z},XMM2,XMM3/M128	Permute word integers from two tables in xmm3/m128 and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2W	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPERMI2W YMM1{K1}{Z},YMM2,YMM3/M256	Permute word integers from two tables in ymm3/m256 and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2W	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPERMI2W ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute word integers from two tables in zmm3/m512 and zmm2 using indexes in zmm1 and store the result in zmm1 using writemask k1.
GENERAL	VPERMI2D	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD
VPERMI2D	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPERMI2D XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute double-words from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2D	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMI2D YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute double-words from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2D	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMI2D ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute double-words from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
GENERAL	VPERMI2Q	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD
VPERMI2Q	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPERMI2Q XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute quad-words from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2Q	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMI2Q YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute quad-words from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2Q	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMI2Q ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute quad-words from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
GENERAL	VPERMI2PS	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD
VPERMI2PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPERMI2PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute SP FP values from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMI2PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute SP FP values from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMI2PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute SP FP values from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
GENERAL	VPERMI2PD	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD
VPERMI2PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPERMI2PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute DP FP values from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMI2PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute DP FP values from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMI2PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute DP FP values from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMILPD	Permute In-Lane of Pairs of Double-Precision Floating-Point Values	VPERMILPD
VPERMILPD	XMM,XMM,XMM/M128	AVX	VPERMILPD XMM1,XMM2,XMM3/M128	Permute DP FP values in xmm2 using controls from xmm3/m128 and store result in xmm1.
VPERMILPD	YMM,YMM,YMM/M256	AVX	VPERMILPD YMM1,YMM2,YMM3/M256	Permute DP FP values in ymm2 using controls from ymm3/m256 and store result in ymm1.
VPERMILPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPERMILPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute DP FP values in xmm2 using control from xmm3/m128/m64bcst and store the result in xmm1 using writemask k1.
VPERMILPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMILPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute DP FP values in ymm2 using control from ymm3/m256/m64bcst and store the result in ymm1 using writemask k1.
VPERMILPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMILPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute DP FP values in zmm2 using control from zmm3/m512/m64bcst and store the result in zmm1 using writemask k1.
VPERMILPD	XMM,XMM/M128,IMM8	AVX	VPERMILPD XMM1,XMM2/M128,IMM8	Permute DP FP values in xmm2/m128 using controls from imm8.
VPERMILPD	YMM,YMM/M256,IMM8	AVX	VPERMILPD YMM1,YMM2/M256,IMM8	Permute DP FP values in ymm2/m256 using controls from imm8.
VPERMILPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPERMILPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Permute DP FP values in xmm2/m128/m64bcst using controls from imm8 and store the result in xmm1 using writemask k1.
VPERMILPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPERMILPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute DP FP values in ymm2/m256/m64bcst using controls from imm8 and store the result in ymm1 using writemask k1.
VPERMILPD	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPERMILPD ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute DP FP values in zmm2/m512/m64bcst using controls from imm8 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMILPS	Permute In-Lane of Quadruples of Single-Precision Floating-Point Values	VPERMILPS
VPERMILPS	XMM,XMM,XMM/M128	AVX	VPERMILPS XMM1,XMM2,XMM3/M128	Permute SP FP values in xmm2 using controls from xmm3/m128 and store result in xmm1.
VPERMILPS	XMM,XMM/M128,IMM8	AVX	VPERMILPS XMM1,XMM2/M128,IMM8	Permute SP FP values in xmm2/m128 using controls from imm8 and store result in xmm1.
VPERMILPS	YMM,YMM,YMM/M256	AVX	VPERMILPS YMM1,YMM2,YMM3/M256	Permute SP FP values in ymm2 using controls from ymm3/m256 and store result in ymm1.
VPERMILPS	YMM,YMM/M256,IMM8	AVX	VPERMILPS YMM1,YMM2/M256,IMM8	Permute SP FP values in ymm2/m256 using controls from imm8 and store result in ymm1.
VPERMILPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPERMILPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute SP FP values xmm2 using control from xmm3/m128/m32bcst and store the result in xmm1 using writemask k1.
VPERMILPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMILPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute SP FP values ymm2 using control from ymm3/m256/m32bcst and store the result in ymm1 using writemask k1.
VPERMILPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMILPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute SP FP values zmm2 using control from zmm3/m512/m32bcst and store the result in zmm1 using writemask k1.
VPERMILPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPERMILPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Permute SP FP values xmm2/m128/m32bcst using controls from imm8 and store the result in xmm1 using writemask k1.
VPERMILPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPERMILPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Permute SP FP values ymm2/m256/m32bcst using controls from imm8 and store the result in ymm1 using writemask k1.
VPERMILPS	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPERMILPS ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Permute SP FP values zmm2/m512/m32bcst using controls from imm8 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMPD	Permute Double-Precision Floating-Point Elements	VPERMPD
VPERMPD	YMM,YMM/M256,IMM8	AVX2	VPERMPD YMM1,YMM2/M256,IMM8	Permute DP FP elements in ymm2/m256 using indices in imm8 and store the result in ymm1.
VPERMPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPERMPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute DP FP elements in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1 subject to writemask k1.
VPERMPD	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPERMPD ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute DP FP elements in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1 subject to writemask k1.
VPERMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute DP FP elements in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1 subject to writemask k1.
VPERMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute DP FP elements in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VPERMPS	Permute Single-Precision Floating-Point Elements	VPERMPS
VPERMPS	YMM,YMM,YMM/M256	AVX2	VPERMPS YMM1,YMM2,YMM3/M256	Permute SP FP elements in ymm3/m256 using indices in ymm2 and store the result in ymm1.
VPERMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute SP FP elements in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 subject to write mask k1.
VPERMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute SP FP values in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 subject to write mask k1.
;--------------------------------------------------------
GENERAL	VPERMQ	Qwords Element Permutation	VPERMQ
VPERMQ	YMM,YMM/M256,IMM8	AVX2	VPERMQ YMM1,YMM2/M256,IMM8	Permute qwords in ymm2/m256 using indices in imm8 and store the result in ymm1.
VPERMQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPERMQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute qwords in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1.
VPERMQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPERMQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute qwords in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1.
VPERMQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute qwords in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1.
VPERMQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute qwords in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1.
;--------------------------------------------------------
GENERAL	VPERMT2B	Full Permute of Bytes from Two Tables Overwriting a Table	VPERMT2B
VPERMT2B	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_VBMI	VPERMT2B XMM1{K1}{Z},XMM2,XMM3/M128	Permute bytes in xmm3/m128 and xmm1 using byte indexes in xmm2 and store the byte results in xmm1 using writemask k1.
VPERMT2B	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_VBMI	VPERMT2B YMM1{K1}{Z},YMM2,YMM3/M256	Permute bytes in ymm3/m256 and ymm1 using byte indexes in ymm2 and store the byte results in ymm1 using writemask k1.
VPERMT2B	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_VBMI	VPERMT2B ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute bytes in zmm3/m512 and zmm1 using byte indexes in zmm2 and store the byte results in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPERMT2W	Full Permute from Two Tables Overwriting one Table	VPERMT2W_D_Q_PS_PD
VPERMT2W	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPERMT2W XMM1{K1}{Z},XMM2,XMM3/M128	Permute word integers from two tables in xmm3/m128 and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMT2W	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPERMT2W YMM1{K1}{Z},YMM2,YMM3/M256	Permute word integers from two tables in ymm3/m256 and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMT2W	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPERMT2W ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute word integers from two tables in zmm3/m512 and zmm1 using indexes in zmm2 and store the result in zmm1 using writemask k1.
GENERAL	VPERMT2D	Full Permute from Two Tables Overwriting one Table	VPERMT2W_D_Q_PS_PD
VPERMT2D	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPERMT2D XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute double-words from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMT2D	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMT2D YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute double-words from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMT2D	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMT2D ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute double-words from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1.
GENERAL	VPERMT2Q	Full Permute from Two Tables Overwriting one Table	VPERMT2W_D_Q_PS_PD
VPERMT2Q	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPERMT2Q XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute quad-words from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMT2Q	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMT2Q YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute quad-words from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMT2Q	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMT2Q ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute quad-words from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1.
GENERAL	VPERMT2PS	Full Permute from Two Tables Overwriting one Table	VPERMT2W_D_Q_PS_PD
VPERMT2PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPERMT2PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute SP FP values from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMT2PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPERMT2PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute SP FP values from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMT2PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPERMT2PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute SP FP values from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1.
GENERAL	VPERMT2PD	Full Permute from Two Tables Overwriting one Table	VPERMT2W_D_Q_PS_PD
VPERMT2PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPERMT2PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute DP FP values from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMT2PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPERMT2PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute DP FP values from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMT2PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPERMT2PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute DP FP values from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPEXPANDB	Expand Byte/Word Values	VPEXPAND
VPEXPANDB	XMM{K}{Z},M128	AVX512_VBMI2,AVX512_VL	VPEXPANDB XMM1{K1}{Z},M128	Expands up to 128 bits of packed byte values from m128 to xmm1 with writemask k1.
VPEXPANDB	XMM{K}{Z},XMM	AVX512_VBMI2,AVX512_VL	VPEXPANDB XMM1{K1}{Z},XMM2	Expands up to 128 bits of packed byte values from xmm2 to xmm1 with writemask k1.
VPEXPANDB	YMM{K}{Z},M256	AVX512_VBMI2,AVX512_VL	VPEXPANDB YMM1{K1}{Z},M256	Expands up to 256 bits of packed byte values from m256 to ymm1 with writemask k1.
VPEXPANDB	YMM{K}{Z},YMM	AVX512_VBMI2,AVX512_VL	VPEXPANDB YMM1{K1}{Z},YMM2	Expands up to 256 bits of packed byte values from ymm2 to ymm1 with writemask k1.
VPEXPANDB	ZMM{K}{Z},M512	AVX512_VBMI2	VPEXPANDB ZMM1{K1}{Z},M512	Expands up to 512 bits of packed byte values from m512 to zmm1 with writemask k1.
VPEXPANDB	ZMM{K}{Z},ZMM	AVX512_VBMI2	VPEXPANDB ZMM1{K1}{Z},ZMM2	Expands up to 512 bits of packed byte values from zmm2 to zmm1 with writemask k1.
GENERAL	VPEXPANDW	Expand Byte/Word Values	VPEXPAND
VPEXPANDW	XMM{K}{Z},M128	AVX512_VBMI2,AVX512_VL	VPEXPANDW XMM1{K1}{Z},M128	Expands up to 128 bits of packed word values from m128 to xmm1 with writemask k1.
VPEXPANDW	XMM{K}{Z},XMM	AVX512_VBMI2,AVX512_VL	VPEXPANDW XMM1{K1}{Z},XMM2	Expands up to 128 bits of packed word values from xmm2 to xmm1 with writemask k1.
VPEXPANDW	YMM{K}{Z},M256	AVX512_VBMI2,AVX512_VL	VPEXPANDW YMM1{K1}{Z},M256	Expands up to 256 bits of packed word values from m256 to ymm1 with writemask k1.
VPEXPANDW	YMM{K}{Z},YMM	AVX512_VBMI2,AVX512_VL	VPEXPANDW YMM1{K1}{Z},YMM2	Expands up to 256 bits of packed word values from ymm2 to ymm1 with writemask k1.
VPEXPANDW	ZMM{K}{Z},M512	AVX512_VBMI2	VPEXPANDW ZMM1{K1}{Z},M512	Expands up to 512 bits of packed word values from m512 to zmm1 with writemask k1.
VPEXPANDW	ZMM{K}{Z},ZMM	AVX512_VBMI2	VPEXPANDW ZMM1{K1}{Z},ZMM2	Expands up to 512 bits of packed byte integer values from zmm2 to zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VPEXPANDD	Load Sparse Packed Doubleword Integer Values from Dense Memory / Register	VPEXPANDD
VPEXPANDD	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPEXPANDD XMM1{K1}{Z},XMM2/M128	Expand packed double-word integer values from xmm2/m128 to xmm1 using writemask k1.
VPEXPANDD	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VPEXPANDD YMM1{K1}{Z},YMM2/M256	Expand packed double-word integer values from ymm2/m256 to ymm1 using writemask k1.
VPEXPANDD	ZMM{K}{Z},ZMM/M512	AVX512_F	VPEXPANDD ZMM1{K1}{Z},ZMM2/M512	Expand packed double-word integer values from zmm2/m512 to zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPEXPANDQ	Load Sparse Packed Quadword Integer Values from Dense Memory / Register	VPEXPANDQ
VPEXPANDQ	XMM{K}{Z},XMM/M128	AVX512_VL,AVX512_F	VPEXPANDQ XMM1{K1}{Z},XMM2/M128	Expand packed quad-word integer values from xmm2/m128 to xmm1 using writemask k1.
VPEXPANDQ	YMM{K}{Z},YMM/M256	AVX512_VL,AVX512_F	VPEXPANDQ YMM1{K1}{Z},YMM2/M256	Expand packed quad-word integer values from ymm2/m256 to ymm1 using writemask k1.
VPEXPANDQ	ZMM{K}{Z},ZMM/M512	AVX512_F	VPEXPANDQ ZMM1{K1}{Z},ZMM2/M512	Expand packed quad-word integer values from zmm2/m512 to zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPGATHERDD	Gather Packed Dword, Packed Qword with Signed Dword Indices	VPGATHERDD_VPGATHERDQ
VPGATHERDD	XMM{K},VM32X	AVX512_VL,AVX512_F	VPGATHERDD XMM1{K1},VM32X	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERDD	YMM{K},VM32Y	AVX512_VL,AVX512_F	VPGATHERDD YMM1{K1},VM32Y	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERDD	ZMM{K},VM32Z	AVX512_F	VPGATHERDD ZMM1{K1},VM32Z	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
GENERAL	VPGATHERDQ	Gather Packed Dword, Packed Qword with Signed Dword Indices	VPGATHERDD_VPGATHERDQ
VPGATHERDQ	XMM{K},VM32X	AVX512_VL,AVX512_F	VPGATHERDQ XMM1{K1},VM32X	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERDQ	YMM{K},VM32X	AVX512_VL,AVX512_F	VPGATHERDQ YMM1{K1},VM32X	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERDQ	ZMM{K},VM32Y	AVX512_F	VPGATHERDQ ZMM1{K1},VM32Y	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
;--------------------------------------------------------
GENERAL	VPGATHERDD	Gather Packed Dword Values Using Signed Dword/Qword Indices	VPGATHERDD_VPGATHERQD
VPGATHERDD	XMM,VM32X,XMM	AVX2	VPGATHERDD XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather dword val- ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERDD	YMM,VM32Y,YMM	AVX2	VPGATHERDD YMM1,VM32Y,YMM2	Using dword indices specified in vm32y, gather dword from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
GENERAL	VPGATHERQD	Gather Packed Dword Values Using Signed Dword/Qword Indices	VPGATHERDD_VPGATHERQD
VPGATHERQD	XMM,VM64X,XMM	AVX2	VPGATHERQD XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather dword val- ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERQD	XMM,VM64Y,XMM	AVX2	VPGATHERQD XMM1,VM64Y,XMM2	Using qword indices specified in vm64y, gather dword val- ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
;--------------------------------------------------------
GENERAL	VPGATHERDQ	Gather Packed Qword Values Using Signed Dword/Qword Indices	VPGATHERDQ_VPGATHERQQ
VPGATHERDQ	XMM,VM32X,XMM	AVX2	VPGATHERDQ XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather qword val- ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERDQ	YMM,VM32X,YMM	AVX2	VPGATHERDQ YMM1,VM32X,YMM2	Using dword indices specified in vm32x, gather qword val- ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
GENERAL	VPGATHERQQ	Gather Packed Qword Values Using Signed Dword/Qword Indices	VPGATHERDQ_VPGATHERQQ
VPGATHERQQ	XMM,VM64X,XMM	AVX2	VPGATHERQQ XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather qword val- ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERQQ	YMM,VM64Y,YMM	AVX2	VPGATHERQQ YMM1,VM64Y,YMM2	Using qword indices specified in vm64y, gather qword val- ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
;--------------------------------------------------------
GENERAL	VPGATHERQD	Gather Packed Dword, Packed Qword with Signed Qword Indices	VPGATHERQD_VPGATHERQQ
VPGATHERQD	XMM{K},VM64X	AVX512_VL,AVX512_F	VPGATHERQD XMM1{K1},VM64X	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERQD	XMM{K},VM64Y	AVX512_VL,AVX512_F	VPGATHERQD XMM1{K1},VM64Y	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERQD	YMM{K},VM64Z	AVX512_F	VPGATHERQD YMM1{K1},VM64Z	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
GENERAL	VPGATHERQQ	Gather Packed Dword, Packed Qword with Signed Qword Indices	VPGATHERQD_VPGATHERQQ
VPGATHERQQ	XMM{K},VM64X	AVX512_VL,AVX512_F	VPGATHERQQ XMM1{K1},VM64X	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERQQ	YMM{K},VM64Y	AVX512_VL,AVX512_F	VPGATHERQQ YMM1{K1},VM64Y	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERQQ	ZMM{K},VM64Z	AVX512_F	VPGATHERQQ ZMM1{K1},VM64Z	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
;--------------------------------------------------------
GENERAL	VPLZCNTD	Count the Number of Leading Zero Bits for Packed Dword, Packed Qword Values	VPLZCNTD_Q
VPLZCNTD	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_CD	VPLZCNTD XMM1{K1}{Z},XMM2/M128/M32BCST	Count the number of leading zero bits in each dword element of xmm2/m128/m32bcst using writemask k1.
VPLZCNTD	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_CD	VPLZCNTD YMM1{K1}{Z},YMM2/M256/M32BCST	Count the number of leading zero bits in each dword element of ymm2/m256/m32bcst using writemask k1.
VPLZCNTD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_CD	VPLZCNTD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Count the number of leading zero bits in each dword element of zmm2/m512/m32bcst using writemask k1.
GENERAL	VPLZCNTQ	Count the Number of Leading Zero Bits for Packed Dword, Packed Qword Values	VPLZCNTD_Q
VPLZCNTQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_CD	VPLZCNTQ XMM1{K1}{Z},XMM2/M128/M64BCST	Count the number of leading zero bits in each qword element of xmm2/m128/m64bcst using writemask k1.
VPLZCNTQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_CD	VPLZCNTQ YMM1{K1}{Z},YMM2/M256/M64BCST	Count the number of leading zero bits in each qword element of ymm2/m256/m64bcst using writemask k1.
VPLZCNTQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_CD	VPLZCNTQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Count the number of leading zero bits in each qword element of zmm2/m512/m64bcst using writemask k1.
;--------------------------------------------------------
GENERAL	VPMADD52HUQ	Packed Multiply of Unsigned 52-bit Unsigned Integers and Add High 52-bit Products to 64-bit Accumulators	VPMADD52HUQ
VPMADD52HUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_IFMA,AVX512_VL	VPMADD52HUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply unsigned 52-bit integers in xmm2 and xmm3/m128 and add the high 52 bits of the 104- bit product to the qword unsigned integers in xmm1 using writemask k1.
VPMADD52HUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_IFMA,AVX512_VL	VPMADD52HUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply unsigned 52-bit integers in ymm2 and ymm3/m128 and add the high 52 bits of the 104- bit product to the qword unsigned integers in ymm1 using writemask k1.
VPMADD52HUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_IFMA	VPMADD52HUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply unsigned 52-bit integers in zmm2 and zmm3/m128 and add the high 52 bits of the 104- bit product to the qword unsigned integers in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPMADD52LUQ	Packed Multiply of Unsigned 52-bit Integers and Add the Low 52-bit Products to Qword Accumulators	VPMADD52LUQ
VPMADD52LUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_IFMA,AVX512_VL	VPMADD52LUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply unsigned 52-bit integers in xmm2 and xmm3/m128 and add the low 52 bits of the 104-bit product to the qword unsigned integers in xmm1 using writemask k1.
VPMADD52LUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_IFMA,AVX512_VL	VPMADD52LUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply unsigned 52-bit integers in ymm2 and ymm3/m128 and add the low 52 bits of the 104-bit product to the qword unsigned integers in ymm1 using writemask k1.
VPMADD52LUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_IFMA	VPMADD52LUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply unsigned 52-bit integers in zmm2 and zmm3/m128 and add the low 52 bits of the 104-bit product to the qword unsigned integers in zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPMASKMOVD	Conditional SIMD Integer Packed Loads and Stores	VPMASKMOV
VPMASKMOVD	XMM,XMM,M128	AVX2	VPMASKMOVD XMM1,XMM2,M128	Conditionally load dword values from m128 using mask in xmm2 and store in xmm1.
VPMASKMOVD	YMM,YMM,M256	AVX2	VPMASKMOVD YMM1,YMM2,M256	Conditionally load dword values from m256 using mask in ymm2 and store in ymm1.
VPMASKMOVD	M128,XMM,XMM	AVX2	VPMASKMOVD M128,XMM1,XMM2	Conditionally store dword values from xmm2 using mask in xmm1.
VPMASKMOVD	M256,YMM,YMM	AVX2	VPMASKMOVD M256,YMM1,YMM2	Conditionally store dword values from ymm2 using mask in ymm1.
GENERAL	VPMASKMOVQ	Conditional SIMD Integer Packed Loads and Stores	VPMASKMOV
VPMASKMOVQ	XMM,XMM,M128	AVX2	VPMASKMOVQ XMM1,XMM2,M128	Conditionally load qword values from m128 using mask in xmm2 and store in xmm1.
VPMASKMOVQ	YMM,YMM,M256	AVX2	VPMASKMOVQ YMM1,YMM2,M256	Conditionally load qword values from m256 using mask in ymm2 and store in ymm1.
VPMASKMOVQ	M128,XMM,XMM	AVX2	VPMASKMOVQ M128,XMM1,XMM2	Conditionally store qword values from xmm2 using mask in xmm1.
VPMASKMOVQ	M256,YMM,YMM	AVX2	VPMASKMOVQ M256,YMM1,YMM2	Conditionally store qword values from ymm2 using mask in ymm1.
;--------------------------------------------------------
GENERAL	VPMOVB2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M
VPMOVB2M	K,XMM	AVX512_VL,AVX512_BW	VPMOVB2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in XMM1.
VPMOVB2M	K,YMM	AVX512_VL,AVX512_BW	VPMOVB2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in YMM1.
VPMOVB2M	K,ZMM	AVX512_BW	VPMOVB2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in ZMM1.
GENERAL	VPMOVW2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M
VPMOVW2M	K,XMM	AVX512_VL,AVX512_BW	VPMOVW2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in XMM1.
VPMOVW2M	K,YMM	AVX512_VL,AVX512_BW	VPMOVW2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in YMM1.
VPMOVW2M	K,ZMM	AVX512_BW	VPMOVW2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in ZMM1.
GENERAL	VPMOVD2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M
VPMOVD2M	K,XMM	AVX512_VL,AVX512_DQ	VPMOVD2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in XMM1.
VPMOVD2M	K,YMM	AVX512_VL,AVX512_DQ	VPMOVD2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in YMM1.
VPMOVD2M	K,ZMM	AVX512_DQ	VPMOVD2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in ZMM1.
GENERAL	VPMOVQ2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M
VPMOVQ2M	K,XMM	AVX512_VL,AVX512_DQ	VPMOVQ2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in XMM1.
VPMOVQ2M	K,YMM	AVX512_VL,AVX512_DQ	VPMOVQ2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in YMM1.
VPMOVQ2M	K,ZMM	AVX512_DQ	VPMOVQ2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in ZMM1.
;--------------------------------------------------------
GENERAL	VPMOVDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB
VPMOVDB	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed double-word integers from xmm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1.
VPMOVDB	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed double-word integers from ymm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1.
VPMOVDB	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed double-word integers from zmm2 into 16 packed byte integers in xmm1/m128 with truncation under writemask k1.
GENERAL	VPMOVSDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB
VPMOVSDB	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVSDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed signed double-word integers from xmm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSDB	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVSDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed signed double-word integers from ymm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1.
VPMOVSDB	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVSDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed signed double-word integers from zmm2 into 16 packed signed byte integers in xmm1/m128 using signed saturation under writemask k1.
GENERAL	VPMOVUSDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB
VPMOVUSDB	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVUSDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSDB	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVUSDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSDB	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVUSDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned byte integers in xmm1/m128 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	VPMOVDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW
VPMOVDW	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed double-word integers from xmm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1.
VPMOVDW	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed double-word integers from ymm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1.
VPMOVDW	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed double-word integers from zmm2 into 16 packed word integers in ymm1/m256 with truncation under writemask k1.
GENERAL	VPMOVSDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW
VPMOVSDW	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVSDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed signed double-word integers from xmm2 into 4 packed signed word integers in ymm1/m64 using signed saturation under writemask k1.
VPMOVSDW	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVSDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed signed double-word integers from ymm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1.
VPMOVSDW	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVSDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed signed double-word integers from zmm2 into 16 packed signed word integers in ymm1/m256 using signed saturation under writemask k1.
GENERAL	VPMOVUSDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW
VPMOVUSDW	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVUSDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSDW	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVUSDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSDW	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVUSDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned word integers in ymm1/m256 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	VPMOVM2B	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q
VPMOVM2B	XMM,K	AVX512_VL,AVX512_BW	VPMOVM2B XMM1,K1	Sets each byte in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2B	YMM,K	AVX512_VL,AVX512_BW	VPMOVM2B YMM1,K1	Sets each byte in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2B	ZMM,K	AVX512_BW	VPMOVM2B ZMM1,K1	Sets each byte in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
GENERAL	VPMOVM2W	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q
VPMOVM2W	XMM,K	AVX512_VL,AVX512_BW	VPMOVM2W XMM1,K1	Sets each word in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2W	YMM,K	AVX512_VL,AVX512_BW	VPMOVM2W YMM1,K1	Sets each word in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2W	ZMM,K	AVX512_BW	VPMOVM2W ZMM1,K1	Sets each word in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
GENERAL	VPMOVM2D	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q
VPMOVM2D	XMM,K	AVX512_VL,AVX512_DQ	VPMOVM2D XMM1,K1	Sets each doubleword in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2D	YMM,K	AVX512_VL,AVX512_DQ	VPMOVM2D YMM1,K1	Sets each doubleword in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2D	ZMM,K	AVX512_DQ	VPMOVM2D ZMM1,K1	Sets each doubleword in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
GENERAL	VPMOVM2Q	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q
VPMOVM2Q	XMM,K	AVX512_VL,AVX512_DQ	VPMOVM2Q XMM1,K1	Sets each quadword in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2Q	YMM,K	AVX512_VL,AVX512_DQ	VPMOVM2Q YMM1,K1	Sets each quadword in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2Q	ZMM,K	AVX512_DQ	VPMOVM2Q ZMM1,K1	Sets each quadword in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
;--------------------------------------------------------
GENERAL	VPMOVQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB
VPMOVQB	XMM/M16{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed byte integers in xmm1/m16 with truncation under writemask k1.
VPMOVQB	XMM/M32{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1.
VPMOVQB	XMM/M64{K}{Z},ZMM	AVX512_F	VPMOVQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1.
GENERAL	VPMOVSQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB
VPMOVSQB	XMM/M16{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVSQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed byte integers in xmm1/m16 using signed saturation under writemask k1.
VPMOVSQB	XMM/M32{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVSQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSQB	XMM/M64{K}{Z},ZMM	AVX512_F	VPMOVSQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1.
GENERAL	VPMOVUSQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB
VPMOVUSQB	XMM/M16{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVUSQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned byte integers in xmm1/m16 using unsigned saturation under writemask k1.
VPMOVUSQB	XMM/M32{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVUSQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSQB	XMM/M64{K}{Z},ZMM	AVX512_F	VPMOVUSQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	VPMOVQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD
VPMOVQD	XMM/M128{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVQD XMM1/M128{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed double-word integers in xmm1/m128 with truncation subject to writemask k1.
VPMOVQD	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed double-word integers in xmm1/m128 with truncation subject to writemask k1.
VPMOVQD	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed double-word integers in ymm1/m256 with truncation subject to writemask k1.
GENERAL	VPMOVSQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD
VPMOVSQD	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVSQD XMM1/M64{K1}{Z},XMM2	Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed double-word integers in xmm1/m64 using signed saturation subject to writemask k1.
VPMOVSQD	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVSQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed double-word integers in xmm1/m128 using signed saturation subject to writemask k1.
VPMOVSQD	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVSQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed double-word integers in ymm1/m256 using signed saturation subject to writemask k1.
GENERAL	VPMOVUSQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD
VPMOVUSQD	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVUSQD XMM1/M64{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned double-word integers in xmm1/m64 using unsigned saturation subject to writemask k1.
VPMOVUSQD	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVUSQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned double-word integers in xmm1/m128 using unsigned saturation subject to writemask k1.
VPMOVUSQD	YMM/M256{K}{Z},ZMM	AVX512_F	VPMOVUSQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned double-word integers in ymm1/m256 using unsigned saturation subject to writemask k1.
;--------------------------------------------------------
GENERAL	VPMOVQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW
VPMOVQW	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVQW XMM1/M32{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed word integers in xmm1/m32 with truncation under writemask k1.
VPMOVQW	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1.
VPMOVQW	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1.
GENERAL	VPMOVSQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW
VPMOVSQW	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVSQW XMM1/M32{K1}{Z},XMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSQW	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVSQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed word integers in xmm1/m64 using signed saturation under writemask k1.
VPMOVSQW	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVSQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1.
GENERAL	VPMOVUSQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW
VPMOVUSQW	XMM/M32{K}{Z},XMM	AVX512_VL,AVX512_F	VPMOVUSQW XMM1/M32{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned word integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSQW	XMM/M64{K}{Z},YMM	AVX512_VL,AVX512_F	VPMOVUSQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSQW	XMM/M128{K}{Z},ZMM	AVX512_F	VPMOVUSQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	VPMOVWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB
VPMOVWB	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_BW	VPMOVWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed word integers from xmm2 into 8 packed bytes in xmm1/m64 with truncation under writemask k1.
VPMOVWB	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_BW	VPMOVWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed word integers from ymm2 into 16 packed bytes in xmm1/m128 with truncation under writemask k1.
VPMOVWB	YMM/M256{K}{Z},ZMM	AVX512_BW	VPMOVWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed word integers from zmm2 into 32 packed bytes in ymm1/m256 with truncation under writemask k1.
GENERAL	VPMOVSWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB
VPMOVSWB	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_BW	VPMOVSWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed signed word integers from xmm2 into 8 packed signed bytes in xmm1/m64 using signed saturation under writemask k1.
VPMOVSWB	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_BW	VPMOVSWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed signed word integers from ymm2 into 16 packed signed bytes in xmm1/m128 using signed saturation under writemask k1.
VPMOVSWB	YMM/M256{K}{Z},ZMM	AVX512_BW	VPMOVSWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed signed word integers from zmm2 into 32 packed signed bytes in ymm1/m256 using signed saturation under writemask k1.
GENERAL	VPMOVUSWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB
VPMOVUSWB	XMM/M64{K}{Z},XMM	AVX512_VL,AVX512_BW	VPMOVUSWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed unsigned word integers from xmm2 into 8 packed unsigned bytes in 8mm1/m64 using unsigned saturation under writemask k1.
VPMOVUSWB	XMM/M128{K}{Z},YMM	AVX512_VL,AVX512_BW	VPMOVUSWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed unsigned word integers from ymm2 into 16 packed unsigned bytes in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSWB	YMM/M256{K}{Z},ZMM	AVX512_BW	VPMOVUSWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed unsigned word integers from zmm2 into 32 packed unsigned bytes in ymm1/m256 using unsigned saturation under writemask k1.
;--------------------------------------------------------
GENERAL	VPMULTISHIFTQB	Select Packed Unaligned Bytes from Quadword Sources	VPMULTISHIFTQB
VPMULTISHIFTQB	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VBMI,AVX512_VL	VPMULTISHIFTQB XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Select unaligned bytes from qwords in xmm3/m128/m64bcst using control bytes in xmm2, write byte results to xmm1 under k1.
VPMULTISHIFTQB	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VBMI,AVX512_VL	VPMULTISHIFTQB YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Select unaligned bytes from qwords in ymm3/m256/m64bcst using control bytes in ymm2, write byte results to ymm1 under k1.
VPMULTISHIFTQB	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_VBMI	VPMULTISHIFTQB ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Select unaligned bytes from qwords in zmm3/m512/m64bcst using control bytes in zmm2, write byte results to zmm1 under k1.
;--------------------------------------------------------
GENERAL	VPOPCNTB	Return the Count of Number of Bits Set to 1 in BYTE/WORD/DWORD/QWORD	VPOPCNT
VPOPCNTB	XMM{K}{Z},XMM/M128	AVX512_BITALG,AVX512_VL	VPOPCNTB XMM1{K1}{Z},XMM2/M128	Counts the number of bits set to one in xmm2/m128 and puts the result in xmm1 with writemask k1.
VPOPCNTB	YMM{K}{Z},YMM/M256	AVX512_BITALG,AVX512_VL	VPOPCNTB YMM1{K1}{Z},YMM2/M256	Counts the number of bits set to one in ymm2/m256 and puts the result in ymm1 with writemask k1.
VPOPCNTB	ZMM{K}{Z},ZMM/M512	AVX512_BITALG	VPOPCNTB ZMM1{K1}{Z},ZMM2/M512	Counts the number of bits set to one in zmm2/m512 and puts the result in zmm1 with writemask k1.
GENERAL	VPOPCNTW	Return the Count of Number of Bits Set to 1 in BYTE/WORD/DWORD/QWORD	VPOPCNT
VPOPCNTW	XMM{K}{Z},XMM/M128	AVX512_BITALG,AVX512_VL	VPOPCNTW XMM1{K1}{Z},XMM2/M128	Counts the number of bits set to one in xmm2/m128 and puts the result in xmm1 with writemask k1.
VPOPCNTW	YMM{K}{Z},YMM/M256	AVX512_BITALG,AVX512_VL	VPOPCNTW YMM1{K1}{Z},YMM2/M256	Counts the number of bits set to one in ymm2/m256 and puts the result in ymm1 with writemask k1.
VPOPCNTW	ZMM{K}{Z},ZMM/M512	AVX512_BITALG	VPOPCNTW ZMM1{K1}{Z},ZMM2/M512	Counts the number of bits set to one in zmm2/m512 and puts the result in zmm1 with writemask k1.
GENERAL	VPOPCNTD	Return the Count of Number of Bits Set to 1 in BYTE/WORD/DWORD/QWORD	VPOPCNT
VPOPCNTD	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VPOPCNTDQ,AVX512_VL	VPOPCNTD XMM1{K1}{Z},XMM2/M128/M32BCST	Counts the number of bits set to one in xmm2/m128/m32bcst and puts the result in xmm1 with writemask k1.
VPOPCNTD	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VPOPCNTDQ,AVX512_VL	VPOPCNTD YMM1{K1}{Z},YMM2/M256/M32BCST	Counts the number of bits set to one in ymm2/m256/m32bcst and puts the result in ymm1 with writemask k1.
VPOPCNTD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_VPOPCNTDQ	VPOPCNTD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Counts the number of bits set to one in zmm2/m512/m32bcst and puts the result in zmm1 with writemask k1.
GENERAL	VPOPCNTQ	Return the Count of Number of Bits Set to 1 in BYTE/WORD/DWORD/QWORD	VPOPCNT
VPOPCNTQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VPOPCNTDQ,AVX512_VL	VPOPCNTQ XMM1{K1}{Z},XMM2/M128/M64BCST	Counts the number of bits set to one in xmm2/m128/m32bcst and puts the result in xmm1 with writemask k1.
VPOPCNTQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VPOPCNTDQ,AVX512_VL	VPOPCNTQ YMM1{K1}{Z},YMM2/M256/M64BCST	Counts the number of bits set to one in ymm2/m256/m32bcst and puts the result in ymm1 with writemask k1.
VPOPCNTQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_VPOPCNTDQ	VPOPCNTQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Counts the number of bits set to one in zmm2/m512/m64bcst and puts the result in zmm1 with writemask k1.
;--------------------------------------------------------
GENERAL	VPROLVD	Bit Rotate Left	VPROLD_VPROLVD_VPROLQ_VPROLVQ
VPROLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPROLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Rotate doublewords in xmm2 left by count in the corresponding element of xmm3/m128/m32bcst. Result written to xmm1 under writemask k1.
VPROLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPROLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Rotate doublewords in ymm2 left by count in the corresponding element of ymm3/m256/m32bcst. Result written to ymm1 under writemask k1.
VPROLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPROLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Rotate left of doublewords in zmm2 by count in the corresponding element of zmm3/m512/m32bcst. Result written to zmm1 using writemask k1.
GENERAL	VPROLD	Bit Rotate Left	VPROLD_VPROLVD_VPROLQ_VPROLVQ
VPROLD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPROLD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rotate doublewords in xmm2/m128/m32bcst left by imm8. Result written to xmm1 using writemask k1.
VPROLD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPROLD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rotate doublewords in ymm2/m256/m32bcst left by imm8. Result written to ymm1 using writemask k1.
VPROLD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPROLD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Rotate left of doublewords in zmm3/m512/m32bcst by imm8. Result written to zmm1 using writemask k1.
GENERAL	VPROLVQ	Bit Rotate Left	VPROLD_VPROLVD_VPROLQ_VPROLVQ
VPROLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPROLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Rotate quadwords in xmm2 left by count in the corresponding element of xmm3/m128/m64bcst. Result written to xmm1 under writemask k1.
VPROLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPROLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Rotate quadwords in ymm2 left by count in the corresponding element of ymm3/m256/m64bcst. Result written to ymm1 under writemask k1.
VPROLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPROLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Rotate quadwords in zmm2 left by count in the corresponding element of zmm3/m512/m64bcst. Result written to zmm1under writemask k1.
GENERAL	VPROLQ	Bit Rotate Left	VPROLD_VPROLVD_VPROLQ_VPROLVQ
VPROLQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPROLQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rotate quadwords in xmm2/m128/m64bcst left by imm8. Result written to xmm1 using writemask k1.
VPROLQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPROLQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rotate quadwords in ymm2/m256/m64bcst left by imm8. Result written to ymm1 using writemask k1.
VPROLQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPROLQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Rotate quadwords in zmm2/m512/m64bcst left by imm8. Result written to zmm1 using writemask k1.
;--------------------------------------------------------
GENERAL	VPRORVD	Bit Rotate  Right	VPRORD_VPRORVD_VPRORQ_VPRORVQ
VPRORVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPRORVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Rotate doublewords in xmm2 right by count in the corresponding element of xmm3/m128/m32bcst, store result using writemask k1.
VPRORVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPRORVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Rotate doublewords in ymm2 right by count in the corresponding element of ymm3/m256/m32bcst, store using result writemask k1.
VPRORVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPRORVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Rotate doublewords in zmm2 right by count in the corresponding element of zmm3/m512/m32bcst, store result using writemask k1.
GENERAL	VPRORD	Bit Rotate  Right	VPRORD_VPRORVD_VPRORQ_VPRORVQ
VPRORD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPRORD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rotate doublewords in xmm2/m128/m32bcst right by imm8, store result using writemask k1.
VPRORD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPRORD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rotate doublewords in ymm2/m256/m32bcst right by imm8, store result using writemask k1.
VPRORD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512_F	VPRORD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Rotate doublewords in zmm2/m512/m32bcst right by imm8, store result using writemask k1.
GENERAL	VPRORVQ	Bit Rotate  Right	VPRORD_VPRORVD_VPRORQ_VPRORVQ
VPRORVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPRORVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Rotate quadwords in xmm2 right by count in the corresponding element of xmm3/m128/m64bcst, store result using writemask k1.
VPRORVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPRORVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Rotate quadwords in ymm2 right by count in the corresponding element of ymm3/m256/m64bcst, store result using writemask k1.
VPRORVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPRORVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Rotate quadwords in zmm2 right by count in the corresponding element of zmm3/m512/m64bcst, store result using writemask k1.
GENERAL	VPRORQ	Bit Rotate  Right	VPRORD_VPRORVD_VPRORQ_VPRORVQ
VPRORQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPRORQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rotate quadwords in xmm2/m128/m64bcst right by imm8, store result using writemask k1.
VPRORQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPRORQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rotate quadwords in ymm2/m256/m64bcst right by imm8, store result using writemask k1.
VPRORQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512_F	VPRORQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Rotate quadwords in zmm2/m512/m64bcst right by imm8, store result using writemask k1.
;--------------------------------------------------------
GENERAL	VPSCATTERDD	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ
VPSCATTERDD	VM32X{K},XMM	AVX512_VL,AVX512_F	VPSCATTERDD VM32X{K1},XMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
VPSCATTERDD	VM32Y{K},YMM	AVX512_VL,AVX512_F	VPSCATTERDD VM32Y{K1},YMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
VPSCATTERDD	VM32Z{K},ZMM	AVX512_F	VPSCATTERDD VM32Z{K1},ZMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
GENERAL	VPSCATTERDQ	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ
VPSCATTERDQ	VM32X{K},XMM	AVX512_VL,AVX512_F	VPSCATTERDQ VM32X{K1},XMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
VPSCATTERDQ	VM32X{K},YMM	AVX512_VL,AVX512_F	VPSCATTERDQ VM32X{K1},YMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
VPSCATTERDQ	VM32Y{K},ZMM	AVX512_F	VPSCATTERDQ VM32Y{K1},ZMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
GENERAL	VPSCATTERQD	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ
VPSCATTERQD	VM64X{K},XMM	AVX512_VL,AVX512_F	VPSCATTERQD VM64X{K1},XMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
VPSCATTERQD	VM64Y{K},XMM	AVX512_VL,AVX512_F	VPSCATTERQD VM64Y{K1},XMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
VPSCATTERQD	VM64Z{K},YMM	AVX512_F	VPSCATTERQD VM64Z{K1},YMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
GENERAL	VPSCATTERQQ	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ
VPSCATTERQQ	VM64X{K},XMM	AVX512_VL,AVX512_F	VPSCATTERQQ VM64X{K1},XMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
VPSCATTERQQ	VM64Y{K},YMM	AVX512_VL,AVX512_F	VPSCATTERQQ VM64Y{K1},YMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
VPSCATTERQQ	VM64Z{K},ZMM	AVX512_F	VPSCATTERQQ VM64Z{K1},ZMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
;--------------------------------------------------------
GENERAL	VPSHLDW	Concatenate and Shift Packed Data Left Logical	VPSHLD
VPSHLDW	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDW XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
VPSHLDW	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDW YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
VPSHLDW	ZMM{K}{Z},ZMM,ZMM/M512,IMM8	AVX512_VBMI2	VPSHLDW ZMM1{K1}{Z},ZMM2,ZMM3/M512,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
GENERAL	VPSHLDD	Concatenate and Shift Packed Data Left Logical	VPSHLD
VPSHLDD	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
VPSHLDD	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
VPSHLDD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_VBMI2	VPSHLDD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
GENERAL	VPSHLDQ	Concatenate and Shift Packed Data Left Logical	VPSHLD
VPSHLDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into xmm1.
VPSHLDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHLDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into ymm1.
VPSHLDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_VBMI2	VPSHLDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the left by constant value in imm8 into zmm1.
;--------------------------------------------------------
GENERAL	VPSHLDVW	Concatenate and Variable Shift Packed Data Left Logical	VPSHLDV
VPSHLDVW	XMM{K}{Z},XMM,XMM/M128	AVX512_VBMI2,AVX512_VL	VPSHLDVW XMM1{K1}{Z},XMM2,XMM3/M128	Concatenate xmm1 and xmm2, extract result shifted to the left by value in xmm3/m128 into xmm1.
VPSHLDVW	YMM{K}{Z},YMM,YMM/M256	AVX512_VBMI2,AVX512_VL	VPSHLDVW YMM1{K1}{Z},YMM2,YMM3/M256	Concatenate ymm1 and ymm2, extract result shifted to the left by value in xmm3/m256 into ymm1.
VPSHLDVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_VBMI2	VPSHLDVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Concatenate zmm1 and zmm2, extract result shifted to the left by value in zmm3/m512 into zmm1.
GENERAL	VPSHLDVD	Concatenate and Variable Shift Packed Data Left Logical	VPSHLDV
VPSHLDVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VBMI2,AVX512_VL	VPSHLDVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Concatenate xmm1 and xmm2, extract result shifted to the left by value in xmm3/m128 into xmm1.
VPSHLDVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VBMI2,AVX512_VL	VPSHLDVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Concatenate ymm1 and ymm2, extract result shifted to the left by value in xmm3/m256 into ymm1.
VPSHLDVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VBMI2	VPSHLDVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Concatenate zmm1 and zmm2, extract result shifted to the left by value in zmm3/m512 into zmm1.
GENERAL	VPSHLDVQ	Concatenate and Variable Shift Packed Data Left Logical	VPSHLDV
VPSHLDVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VBMI2,AVX512_VL	VPSHLDVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Concatenate xmm1 and xmm2, extract result shifted to the left by value in xmm3/m128 into xmm1.
VPSHLDVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VBMI2,AVX512_VL	VPSHLDVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Concatenate ymm1 and ymm2, extract result shifted to the left by value in xmm3/m256 into ymm1.
VPSHLDVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_VBMI2	VPSHLDVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Concatenate zmm1 and zmm2, extract result shifted to the left by value in zmm3/m512 into zmm1.
;--------------------------------------------------------
GENERAL	VPSHRDW	Concatenate and Shift Packed Data Right Logical	VPSHRD
VPSHRDW	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDW XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into xmm1.
VPSHRDW	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDW YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into ymm1.
VPSHRDW	ZMM{K}{Z},ZMM,ZMM/M512,IMM8	AVX512_VBMI2	VPSHRDW ZMM1{K1}{Z},ZMM2,ZMM3/M512,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into zmm1.
GENERAL	VPSHRDD	Concatenate and Shift Packed Data Right Logical	VPSHRD
VPSHRDD	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into xmm1.
VPSHRDD	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into ymm1.
VPSHRDD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_VBMI2	VPSHRDD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into zmm1.
GENERAL	VPSHRDQ	Concatenate and Shift Packed Data Right Logical	VPSHRD
VPSHRDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into xmm1.
VPSHRDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VBMI2,AVX512_VL	VPSHRDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into ymm1.
VPSHRDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_VBMI2	VPSHRDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Concatenate destination and source operands, extract result shifted to the right by constant value in imm8 into zmm1.
;--------------------------------------------------------
GENERAL	VPSHRDVW	Concatenate and Variable Shift Packed Data Right Logical	VPSHRDV
VPSHRDVW	XMM{K}{Z},XMM,XMM/M128	AVX512_VBMI2,AVX512_VL	VPSHRDVW XMM1{K1}{Z},XMM2,XMM3/M128	Concatenate xmm1 and xmm2, extract result shifted to the right by value in xmm3/m128 into xmm1.
VPSHRDVW	YMM{K}{Z},YMM,YMM/M256	AVX512_VBMI2,AVX512_VL	VPSHRDVW YMM1{K1}{Z},YMM2,YMM3/M256	Concatenate ymm1 and ymm2, extract result shifted to the right by value in xmm3/m256 into ymm1.
VPSHRDVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_VBMI2	VPSHRDVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Concatenate zmm1 and zmm2, extract result shifted to the right by value in zmm3/m512 into zmm1.
GENERAL	VPSHRDVD	Concatenate and Variable Shift Packed Data Right Logical	VPSHRDV
VPSHRDVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VBMI2,AVX512_VL	VPSHRDVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Concatenate xmm1 and xmm2, extract result shifted to the right by value in xmm3/m128 into xmm1.
VPSHRDVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VBMI2,AVX512_VL	VPSHRDVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Concatenate ymm1 and ymm2, extract result shifted to the right by value in xmm3/m256 into ymm1.
VPSHRDVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_VBMI2	VPSHRDVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Concatenate zmm1 and zmm2, extract result shifted to the right by value in zmm3/m512 into zmm1.
GENERAL	VPSHRDVQ	Concatenate and Variable Shift Packed Data Right Logical	VPSHRDV
VPSHRDVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VBMI2,AVX512_VL	VPSHRDVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Concatenate xmm1 and xmm2, extract result shifted to the right by value in xmm3/m128 into xmm1.
VPSHRDVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VBMI2,AVX512_VL	VPSHRDVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Concatenate ymm1 and ymm2, extract result shifted to the right by value in xmm3/m256 into ymm1.
VPSHRDVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_VBMI2	VPSHRDVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Concatenate zmm1 and zmm2, extract result shifted to the right by value in zmm3/m512 into zmm1.
;--------------------------------------------------------
GENERAL	VPSHUFBITQMB	Shuffle Bits from Quadword Elements Using Byte Indexes into Mask	VPSHUFBITQMB
VPSHUFBITQMB	K{K},XMM,XMM/M128	AVX512_BITALG,AVX512_VL	VPSHUFBITQMB K1{K2},XMM2,XMM3/M128	Extract values in xmm2 using control bits of xmm3/m128 with writemask k2 and leave the result in mask register k1.
VPSHUFBITQMB	K{K},YMM,YMM/M256	AVX512_BITALG,AVX512_VL	VPSHUFBITQMB K1{K2},YMM2,YMM3/M256	Extract values in ymm2 using control bits of ymm3/m256 with writemask k2 and leave the result in mask register k1.
VPSHUFBITQMB	K{K},ZMM,ZMM/M512	AVX512_BITALG	VPSHUFBITQMB K1{K2},ZMM2,ZMM3/M512	Extract values in zmm2 using control bits of zmm3/m512 with writemask k2 and leave the result in mask register k1.
;--------------------------------------------------------
GENERAL	VPSLLVD	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ
VPSLLVD	XMM,XMM,XMM/M128	AVX2	VPSLLVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSLLVD	YMM,YMM,YMM/M256	AVX2	VPSLLVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSLLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPSLLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1.
VPSLLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPSLLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1.
VPSLLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPSLLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1.
GENERAL	VPSLLVQ	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ
VPSLLVQ	XMM,XMM,XMM/M128	AVX2	VPSLLVQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSLLVQ	YMM,YMM,YMM/M256	AVX2	VPSLLVQ YMM1,YMM2,YMM3/M256	Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSLLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPSLLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1.
VPSLLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPSLLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1.
VPSLLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPSLLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1.
GENERAL	VPSLLVW	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ
VPSLLVW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSLLVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1.
VPSLLVW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSLLVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1.
VPSLLVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSLLVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 left by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1.
;--------------------------------------------------------
GENERAL	VPSRAVD	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ
VPSRAVD	XMM,XMM,XMM/M128	AVX2	VPSRAVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits.
VPSRAVD	YMM,YMM,YMM/M256	AVX2	VPSRAVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits.
VPSRAVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPSRAVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in sign bits using writemask k1.
VPSRAVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPSRAVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in sign bits using writemask k1.
VPSRAVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPSRAVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in sign bits using writemask k1.
GENERAL	VPSRAVW	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ
VPSRAVW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRAVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAVW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSRAVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits using writemask k1.
VPSRAVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSRAVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in sign bits using writemask k1.
GENERAL	VPSRAVQ	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ
VPSRAVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPSRAVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in sign bits using writemask k1.
VPSRAVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPSRAVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in sign bits using writemask k1.
VPSRAVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPSRAVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in sign bits using writemask k1.
;--------------------------------------------------------
GENERAL	VPSRLVD	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ
VPSRLVD	XMM,XMM,XMM/M128	AVX2	VPSRLVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSRLVD	YMM,YMM,YMM/M256	AVX2	VPSRLVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSRLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPSRLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1.
VPSRLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPSRLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1.
VPSRLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_F	VPSRLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1.
GENERAL	VPSRLVQ	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ
VPSRLVQ	XMM,XMM,XMM/M128	AVX2	VPSRLVQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSRLVQ	YMM,YMM,YMM/M256	AVX2	VPSRLVQ YMM1,YMM2,YMM3/M256	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSRLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPSRLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1.
VPSRLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPSRLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1.
VPSRLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_F	VPSRLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1.
GENERAL	VPSRLVW	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ
VPSRLVW	XMM{K}{Z},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPSRLVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1.
VPSRLVW	YMM{K}{Z},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPSRLVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1.
VPSRLVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512_BW	VPSRLVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1.
;--------------------------------------------------------
GENERAL	VPTERNLOGD	Bitwise Ternary Logic	VPTERNLOGD_VPTERNLOGQ
VPTERNLOGD	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VPTERNLOGD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m32bcst as source operands and writing the result to xmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGD	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VPTERNLOGD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m32bcst as source operands and writing the result to ymm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VPTERNLOGD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m32bcst as source operands and writing the result to zmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
GENERAL	VPTERNLOGQ	Bitwise Ternary Logic	VPTERNLOGD_VPTERNLOGQ
VPTERNLOGQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VPTERNLOGQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m64bcst as source operands and writing the result to xmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VPTERNLOGQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m64bcst as source operands and writing the result to ymm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VPTERNLOGQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m64bcst as source operands and writing the result to zmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
;--------------------------------------------------------
GENERAL	VPTESTMB	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ
VPTESTMB	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPTESTMB K2{K1},XMM2,XMM3/M128	Bitwise AND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMB	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPTESTMB K2{K1},YMM2,YMM3/M256	Bitwise AND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMB	K{K},ZMM,ZMM/M512	AVX512_BW	VPTESTMB K2{K1},ZMM2,ZMM3/M512	Bitwise AND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTMW	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ
VPTESTMW	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPTESTMW K2{K1},XMM2,XMM3/M128	Bitwise AND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMW	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPTESTMW K2{K1},YMM2,YMM3/M256	Bitwise AND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMW	K{K},ZMM,ZMM/M512	AVX512_BW	VPTESTMW K2{K1},ZMM2,ZMM3/M512	Bitwise AND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTMD	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ
VPTESTMD	K{K},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPTESTMD K2{K1},XMM2,XMM3/M128/M32BCST	Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMD	K{K},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPTESTMD K2{K1},YMM2,YMM3/M256/M32BCST	Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMD	K{K},ZMM,ZMM/M512/M32BCST	AVX512_F	VPTESTMD K2{K1},ZMM2,ZMM3/M512/M32BCST	Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTMQ	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ
VPTESTMQ	K{K},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPTESTMQ K2{K1},XMM2,XMM3/M128/M64BCST	Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMQ	K{K},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPTESTMQ K2{K1},YMM2,YMM3/M256/M64BCST	Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512_F	VPTESTMQ K2{K1},ZMM2,ZMM3/M512/M64BCST	Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
;--------------------------------------------------------
GENERAL	VPTESTNMB	Logical NAND and Set	VPTESTNMB_W_D_Q
VPTESTNMB	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPTESTNMB K2{K1},XMM2,XMM3/M128	Bitwise NAND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMB	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPTESTNMB K2{K1},YMM2,YMM3/M256	Bitwise NAND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMB	K{K},ZMM,ZMM/M512	AVX512_F,AVX512_BW	VPTESTNMB K2{K1},ZMM2,ZMM3/M512	Bitwise NAND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTNMW	Logical NAND and Set	VPTESTNMB_W_D_Q
VPTESTNMW	K{K},XMM,XMM/M128	AVX512_VL,AVX512_BW	VPTESTNMW K2{K1},XMM2,XMM3/M128	Bitwise NAND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMW	K{K},YMM,YMM/M256	AVX512_VL,AVX512_BW	VPTESTNMW K2{K1},YMM2,YMM3/M256	Bitwise NAND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMW	K{K},ZMM,ZMM/M512	AVX512_F,AVX512_BW	VPTESTNMW K2{K1},ZMM2,ZMM3/M512	Bitwise NAND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTNMD	Logical NAND and Set	VPTESTNMB_W_D_Q
VPTESTNMD	K{K},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VPTESTNMD K2{K1},XMM2,XMM3/M128/M32BCST	Bitwise NAND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMD	K{K},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VPTESTNMD K2{K1},YMM2,YMM3/M256/M32BCST	Bitwise NAND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMD	K{K},ZMM,ZMM/M512/M32BCST	AVX512_F	VPTESTNMD K2{K1},ZMM2,ZMM3/M512/M32BCST	Bitwise NAND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
GENERAL	VPTESTNMQ	Logical NAND and Set	VPTESTNMB_W_D_Q
VPTESTNMQ	K{K},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VPTESTNMQ K2{K1},XMM2,XMM3/M128/M64BCST	Bitwise NAND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMQ	K{K},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VPTESTNMQ K2{K1},YMM2,YMM3/M256/M64BCST	Bitwise NAND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512_F	VPTESTNMQ K2{K1},ZMM2,ZMM3/M512/M64BCST	Bitwise NAND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
;--------------------------------------------------------
GENERAL	VRANGEPD	Range Restriction Calculation For Packed Pairs of Float64 Values	VRANGEPD
VRANGEPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VRANGEPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Calculate two RANGE operation output value from 2 pairs of DP FP values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VRANGEPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Calculate four RANGE operation output value from 4pairs of DP FP values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512_DQ	VRANGEPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Calculate eight RANGE operation output value from 8 pairs of DP FP values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
;--------------------------------------------------------
GENERAL	VRANGEPS	Range Restriction Calculation For Packed Pairs of Float32 Values	VRANGEPS
VRANGEPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VRANGEPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Calculate four RANGE operation output value from 4 pairs of SP FP values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VRANGEPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Calculate eight RANGE operation output value from 8 pairs of SP FP values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512_DQ	VRANGEPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Calculate 16 RANGE operation output value from 16 pairs of SP FP values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
;--------------------------------------------------------
GENERAL	VRANGESD	Range Restriction Calculation From a pair of Scalar Float64 Values	VRANGESD
VRANGESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512_DQ	VRANGESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Calculate a RANGE operation output value from 2 DP FP values in xmm2 and xmm3/m64, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation.
;--------------------------------------------------------
GENERAL	VRANGESS	Range Restriction Calculation From a Pair of Scalar Float32 Values	VRANGESS
VRANGESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512_DQ	VRANGESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Calculate a RANGE operation output value from 2 SP FP values in xmm2 and xmm3/m32, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation.
;--------------------------------------------------------
GENERAL	VRCP14PD	Compute Approximate Reciprocals of Packed Float64 Values	VRCP14PD
VRCP14PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VRCP14PD XMM1{K1}{Z},XMM2/M128/M64BCST	Computes the approximate reciprocals of the packed DP FP values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask.
VRCP14PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VRCP14PD YMM1{K1}{Z},YMM2/M256/M64BCST	Computes the approximate reciprocals of the packed DP FP values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask.
VRCP14PD	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_F	VRCP14PD ZMM1{K1}{Z},ZMM2/M512/M64BCST	Computes the approximate reciprocals of the packed DP FP values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask.
;--------------------------------------------------------
GENERAL	VRCP14PS	Compute Approximate Reciprocals of Packed Float32 Values	VRCP14PS
VRCP14PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VRCP14PS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes the approximate reciprocals of the packed SP FP values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask.
VRCP14PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VRCP14PS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes the approximate reciprocals of the packed SP FP values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask.
VRCP14PS	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_F	VRCP14PS ZMM1{K1}{Z},ZMM2/M512/M32BCST	Computes the approximate reciprocals of the packed SP FP values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
;--------------------------------------------------------
GENERAL	VRCP14SD	Compute Approximate Reciprocal of Scalar Float64 Value	VRCP14SD
VRCP14SD	XMM{K}{Z},XMM,XMM/M64	AVX512_F	VRCP14SD XMM1{K1}{Z},XMM2,XMM3/M64	Computes the approximate reciprocal of the scalar DP FP value in xmm3/m64 and stores the result in xmm1 using writemask k1. Also, upper DP FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	VRCP14SS	Compute Approximate Reciprocal of Scalar Float32 Value	VRCP14SS
VRCP14SS	XMM{K}{Z},XMM,XMM/M32	AVX512_F	VRCP14SS XMM1{K1}{Z},XMM2,XMM3/M32	Computes the approximate reciprocal of the scalar SP FP value in xmm3/m32 and stores the results in xmm1 using writemask k1. Also, upper DP FP value (bits[127:32]) from xmm2 is copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	VRCP28PD	Approximation to the Reciprocal of Packed Double-Precision Floating-Point Values  with Less Than 2^-28 Relative Error	VRCP28PD
VRCP28PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_ER	VRCP28PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes the approximate reciprocals ( < 2^-28 relative error) of the packed DP FP values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask.
;--------------------------------------------------------
GENERAL	VRCP28PS	Approximation to the Reciprocal of Packed Single-Precision Floating-Point Values  with Less Than 2^-28 Relative Error	VRCP28PS
VRCP28PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_ER	VRCP28PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes the approximate reciprocals ( < 2^-28 relative error) of the packed SP FP values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
;--------------------------------------------------------
GENERAL	VRCP28SD	Approximation to the Reciprocal of Scalar Double-Precision Floating-Point Value  with Less Than 2^-28 Relative Error	VRCP28SD
VRCP28SD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512_ER	VRCP28SD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Computes the approximate reciprocal ( < 2^-28 relative error) of the scalar DP FP value in xmm3/m64 and stores the results in xmm1. Under writemask. Also, upper DP FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	VRCP28SS	Approximation to the Reciprocal of Scalar Single-Precision Floating-Point Value  with Less Than 2^-28 Relative Error	VRCP28SS
VRCP28SS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_ER	VRCP28SS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Computes the approximate reciprocal ( < 2^-28 relative error) of the scalar SP FP value in xmm3/m32 and stores the results in xmm1. Under writemask. Also, upper 3 SP FP values (bits[127:32]) from xmm2 is copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	VREDUCEPD	Perform Reduction Transformation on Packed Float64 Values	VREDUCEPD
VREDUCEPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VREDUCEPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Perform reduction transformation on packed DP floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1.
VREDUCEPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_DQ	VREDUCEPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Perform reduction transformation on packed DP floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1.
VREDUCEPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512_DQ	VREDUCEPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Perform reduction transformation on DP floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1.
;--------------------------------------------------------
GENERAL	VREDUCEPS	Perform Reduction Transformation on Packed Float32 Values	VREDUCEPS
VREDUCEPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VREDUCEPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Perform reduction transformation on packed SP floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1.
VREDUCEPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_DQ	VREDUCEPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Perform reduction transformation on packed SP floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1.
VREDUCEPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512_DQ	VREDUCEPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Perform reduction transformation on packed SP floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1.
;--------------------------------------------------------
GENERAL	VREDUCESD	Perform a Reduction Transformation on a Scalar Float64 Value	VREDUCESD
VREDUCESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512_DQ	VREDUCESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Perform a reduction transformation on a scalar DP floating point value in xmm3/m64 by subtracting a number of fraction bits specified by the imm8 field. Also, upper double precision FP value (bits[127:64]) from xmm2 are copied to xmm1[127:64]. Stores the result in xmm1 register.
;--------------------------------------------------------
GENERAL	VREDUCESS	Perform a Reduction Transformation on a Scalar Float32 Value	VREDUCESS
VREDUCESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512_DQ	VREDUCESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Perform a reduction transformation on a scalar SP floating point value in xmm3/m32 by subtracting a number of fraction bits specified by the imm8 field. Also, upper single precision FP values (bits[127:32]) from xmm2 are copied to xmm1[127:32]. Stores the result in xmm1 register.
;--------------------------------------------------------
GENERAL	VRNDSCALEPD	Round Packed Float64 Values To Include A Given Number Of Fraction Bits	VRNDSCALEPD
VRNDSCALEPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512_VL,AVX512_F	VRNDSCALEPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rounds packed DP floating point values in xmm2/m128/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask.
VRNDSCALEPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VRNDSCALEPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rounds packed DP floating point values in ymm2/m256/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask.
VRNDSCALEPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512_F	VRNDSCALEPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Rounds packed DP FP values in zmm2/m512/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask k1.
;--------------------------------------------------------
GENERAL	VRNDSCALEPS	Round Packed Float32 Values To Include A Given Number Of Fraction Bits	VRNDSCALEPS
VRNDSCALEPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512_VL,AVX512_F	VRNDSCALEPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rounds packed SP floating point values in xmm2/m128/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask.
VRNDSCALEPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VRNDSCALEPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rounds packed SP floating point values in ymm2/m256/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask.
VRNDSCALEPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512_F	VRNDSCALEPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Rounds packed SP FP values in zmm2/m512/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask.
;--------------------------------------------------------
GENERAL	VRNDSCALESD	Round Scalar Float64 Value To Include A Given Number Of Fraction Bits	VRNDSCALESD
VRNDSCALESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512_F	VRNDSCALESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Rounds scalar DP FP value in xmm3/m64 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register.
;--------------------------------------------------------
GENERAL	VRNDSCALESS	Round Scalar Float32 Value To Include A Given Number Of Fraction Bits	VRNDSCALESS
VRNDSCALESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512_F	VRNDSCALESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Rounds scalar SP FP value in xmm3/m32 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask.
;--------------------------------------------------------
GENERAL	VRSQRT14PD	Compute Approximate Reciprocals of Square Roots of Packed Float64 Values	VRSQRT14PD
VRSQRT14PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512_VL,AVX512_F	VRSQRT14PD XMM1{K1}{Z},XMM2/M128/M64BCST	Computes the approximate reciprocal square roots of the packed DP FP values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask.
VRSQRT14PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512_VL,AVX512_F	VRSQRT14PD YMM1{K1}{Z},YMM2/M256/M64BCST	Computes the approximate reciprocal square roots of the packed DP FP values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask.
VRSQRT14PD	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512_F	VRSQRT14PD ZMM1{K1}{Z},ZMM2/M512/M64BCST	Computes the approximate reciprocal square roots of the packed DP FP values in zmm2/m512/m64bcst and stores the results in zmm1 under writemask.
;--------------------------------------------------------
GENERAL	VRSQRT14PS	Compute Approximate Reciprocals of Square Roots of Packed Float32 Values	VRSQRT14PS
VRSQRT14PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512_VL,AVX512_F	VRSQRT14PS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes the approximate reciprocal square roots of the packed SP FP values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask.
VRSQRT14PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512_VL,AVX512_F	VRSQRT14PS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes the approximate reciprocal square roots of the packed SP FP values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask.
VRSQRT14PS	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512_F	VRSQRT14PS ZMM1{K1}{Z},ZMM2/M512/M32BCST	Computes the approximate reciprocal square roots of the packed SP FP values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
;--------------------------------------------------------
GENERAL	VRSQRT14SD	Compute Approximate Reciprocal of Square Root of Scalar Float64 Value	VRSQRT14SD
VRSQRT14SD	XMM{K}{Z},XMM,XMM/M64	AVX512_F	VRSQRT14SD XMM1{K1}{Z},XMM2,XMM3/M64	Computes the approximate reciprocal square root of the scalar DP FP value in xmm3/m64 and stores the result in the low quadword element of xmm1 using writemask k1. Bits[127:64] of xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	VRSQRT14SS	Compute Approximate Reciprocal of Square Root of Scalar Float32 Value	VRSQRT14SS
VRSQRT14SS	XMM{K}{Z},XMM,XMM/M32	AVX512_F	VRSQRT14SS XMM1{K1}{Z},XMM2,XMM3/M32	Computes the approximate reciprocal square root of the scalar SP FP value in xmm3/m32 and stores the result in the low doubleword element of xmm1 using writemask k1. Bits[127:32] of xmm2 is copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	VRSQRT28PD	Approximation to the Reciprocal Square Root of Packed Double-Precision  Floating-Point Values with Less Than 2^-28 Relative Error	VRSQRT28PD
VRSQRT28PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512_ER	VRSQRT28PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes approximations to the Reciprocal square root (<2^- 28 relative error) of the packed DP FP values from zmm2/m512/m64bcst and stores result in zmm1with writemask k1.
;--------------------------------------------------------
GENERAL	VRSQRT28PS	Approximation to the Reciprocal Square Root of Packed Single-Precision  Floating-Point Values with Less Than 2^-28 Relative Error	VRSQRT28PS
VRSQRT28PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512_ER	VRSQRT28PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes approximations to the Reciprocal square root (<2^-28 relative error) of the packed SP FP values from zmm2/m512/m32bcst and stores result in zmm1with writemask k1.
;--------------------------------------------------------
GENERAL	VRSQRT28SD	Approximation to the Reciprocal Square Root of Scalar Double-Precision  Floating-Point Value with Less Than 2^-28 Relative Error	VRSQRT28SD
VRSQRT28SD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512_ER	VRSQRT28SD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Computes approximate reciprocal square root (<2^-28 relative error) of the scalar DP FP value from xmm3/m64 and stores result in xmm1with writemask k1. Also, upper DP FP value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
;--------------------------------------------------------
GENERAL	VRSQRT28SS	Approximation to the Reciprocal Square Root of Scalar Single-Precision Floating-  Point Value with Less Than 2^-28 Relative Error	VRSQRT28SS
VRSQRT28SS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512_ER	VRSQRT28SS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Computes approximate reciprocal square root (<2^-28 relative error) of the scalar SP FP value from xmm3/m32 and stores result in xmm1with writemask k1. Also, upper 3 SP FP value (bits[127:32]) from xmm2 is copied to xmm1[127:32].
;--------------------------------------------------------
GENERAL	VSCALEFPD	Scale Packed Float64 Values With Float64 Values	VSCALEFPD
VSCALEFPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_F	VSCALEFPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Scale the packed DP FP values in xmm2 using values from xmm3/m128/m64bcst. Under writemask k1.
VSCALEFPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_F	VSCALEFPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Scale the packed DP FP values in ymm2 using values from ymm3/m256/m64bcst. Under writemask k1.
VSCALEFPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512_F	VSCALEFPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Scale the packed DP FP values in zmm2 using values from zmm3/m512/m64bcst. Under writemask k1.
;--------------------------------------------------------
GENERAL	VSCALEFPS	Scale Packed Float32 Values With Float32 Values	VSCALEFPS
VSCALEFPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_F	VSCALEFPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Scale the packed SP FP values in xmm2 using values from xmm3/m128/m32bcst. Under writemask k1.
VSCALEFPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_F	VSCALEFPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Scale the packed SP values in ymm2 using floating point values from ymm3/m256/m32bcst. Under writemask k1.
VSCALEFPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512_F	VSCALEFPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Scale the packed SP FP values in zmm2 using FP values from zmm3/m512/m32bcst. Under writemask k1.
;--------------------------------------------------------
GENERAL	VSCALEFSD	Scale Scalar Float64 Values With Float64 Values	VSCALEFSD
VSCALEFSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512_F	VSCALEFSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Scale the scalar DP FP values in xmm2 using the value from xmm3/m64. Under writemask k1.
;--------------------------------------------------------
GENERAL	VSCALEFSS	Scale Scalar Float32 Value With Float32 Value	VSCALEFSS
VSCALEFSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512_F	VSCALEFSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Scale the scalar SP FP value in xmm2 using FP value from xmm3/m32. Under writemask k1.
;--------------------------------------------------------
GENERAL	VSCATTERDPS	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD
VSCATTERDPS	VM32X{K},XMM	AVX512_VL,AVX512_F	VSCATTERDPS VM32X{K1},XMM1	Using signed dword indices, scatter SP FP values to memory using writemask k1.
VSCATTERDPS	VM32Y{K},YMM	AVX512_VL,AVX512_F	VSCATTERDPS VM32Y{K1},YMM1	Using signed dword indices, scatter SP FP values to memory using writemask k1.
VSCATTERDPS	VM32Z{K},ZMM	AVX512_F	VSCATTERDPS VM32Z{K1},ZMM1	Using signed dword indices, scatter SP FP values to memory using writemask k1.
GENERAL	VSCATTERDPD	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD
VSCATTERDPD	VM32X{K},XMM	AVX512_VL,AVX512_F	VSCATTERDPD VM32X{K1},XMM1	Using signed dword indices, scatter DP FP values to memory using writemask k1.
VSCATTERDPD	VM32X{K},YMM	AVX512_VL,AVX512_F	VSCATTERDPD VM32X{K1},YMM1	Using signed dword indices, scatter DP FP values to memory using writemask k1.
VSCATTERDPD	VM32Y{K},ZMM	AVX512_F	VSCATTERDPD VM32Y{K1},ZMM1	Using signed dword indices, scatter DP FP values to memory using writemask k1.
GENERAL	VSCATTERQPS	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD
VSCATTERQPS	VM64X{K},XMM	AVX512_VL,AVX512_F	VSCATTERQPS VM64X{K1},XMM1	Using signed qword indices, scatter SP FP values to memory using writemask k1.
VSCATTERQPS	VM64Y{K},XMM	AVX512_VL,AVX512_F	VSCATTERQPS VM64Y{K1},XMM1	Using signed qword indices, scatter SP FP values to memory using writemask k1.
VSCATTERQPS	VM64Z{K},YMM	AVX512_F	VSCATTERQPS VM64Z{K1},YMM1	Using signed qword indices, scatter SP FP values to memory using writemask k1.
GENERAL	VSCATTERQPD	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD
VSCATTERQPD	VM64X{K},XMM	AVX512_VL,AVX512_F	VSCATTERQPD VM64X{K1},XMM1	Using signed qword indices, scatter DP FP values to memory using writemask k1.
VSCATTERQPD	VM64Y{K},YMM	AVX512_VL,AVX512_F	VSCATTERQPD VM64Y{K1},YMM1	Using signed qword indices, scatter DP FP values to memory using writemask k1.
VSCATTERQPD	VM64Z{K},ZMM	AVX512_F	VSCATTERQPD VM64Z{K1},ZMM1	Using signed qword indices, scatter DP FP values to memory using writemask k1.
;--------------------------------------------------------
GENERAL	VSCATTERPF0DPS	Sparse Prefetch  Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD
VSCATTERPF0DPS	VM32Z{K}	AVX512_PF	VSCATTERPF0DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing SP data using writemask k1 and T0 hint with intent to write.
GENERAL	VSCATTERPF0QPS	Sparse Prefetch  Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD
VSCATTERPF0QPS	VM64Z{K}	AVX512_PF	VSCATTERPF0QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing SP data using writemask k1 and T0 hint with intent to write.
GENERAL	VSCATTERPF0DPD	Sparse Prefetch  Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD
VSCATTERPF0DPD	VM32Y{K}	AVX512_PF	VSCATTERPF0DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing DP data using writemask k1 and T0 hint with intent to write.
GENERAL	VSCATTERPF0QPD	Sparse Prefetch  Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD
VSCATTERPF0QPD	VM64Z{K}	AVX512_PF	VSCATTERPF0QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing DP data using writemask k1 and T0 hint with intent to write.
;--------------------------------------------------------
GENERAL	VSCATTERPF1DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD
VSCATTERPF1DPS	VM32Z{K}	AVX512_PF	VSCATTERPF1DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing SP data using writemask k1 and T1 hint with intent to write.
GENERAL	VSCATTERPF1QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD
VSCATTERPF1QPS	VM64Z{K}	AVX512_PF	VSCATTERPF1QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing SP data using writemask k1 and T1 hint with intent to write.
GENERAL	VSCATTERPF1DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD
VSCATTERPF1DPD	VM32Y{K}	AVX512_PF	VSCATTERPF1DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing DP data using writemask k1 and T1 hint with intent to write.
GENERAL	VSCATTERPF1QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD
VSCATTERPF1QPD	VM64Z{K}	AVX512_PF	VSCATTERPF1QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing DP data using writemask k1 and T1 hint with intent to write.
;--------------------------------------------------------
GENERAL	VSHUFF32X4	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2
VSHUFF32X4	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VSHUFF32X4 YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shuffle 128-bit packed SP FP values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1.
VSHUFF32X4	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VSHUFF32X4 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shuffle 128-bit packed SP FP values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1.
GENERAL	VSHUFF64X2	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2
VSHUFF64X2	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VSHUFF64X2 YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle 128-bit packed DP FP values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1.
VSHUFF64X2	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VSHUFF64X2 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle 128-bit packed DP FP values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1.
GENERAL	VSHUFI32X4	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2
VSHUFI32X4	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512_VL,AVX512_F	VSHUFI32X4 YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shuffle 128-bit packed double-word values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1.
VSHUFI32X4	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512_F	VSHUFI32X4 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shuffle 128-bit packed double-word values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1.
GENERAL	VSHUFI64X2	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2
VSHUFI64X2	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512_VL,AVX512_F	VSHUFI64X2 YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle 128-bit packed quad-word values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1.
VSHUFI64X2	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512_F	VSHUFI64X2 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle 128-bit packed quad-word values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1.
;--------------------------------------------------------
GENERAL	VTESTPS	Packed Bit Test	VTESTPD_VTESTPS
VTESTPS	XMM,XMM/M128	AVX	VTESTPS XMM1,XMM2/M128	Set ZF and CF depending on sign bit AND and ANDN of packed SP FP sources.
VTESTPS	YMM,YMM/M256	AVX	VTESTPS YMM1,YMM2/M256	Set ZF and CF depending on sign bit AND and ANDN of packed SP FP sources.
GENERAL	VTESTPD	Packed Bit Test	VTESTPD_VTESTPS
VTESTPD	XMM,XMM/M128	AVX	VTESTPD XMM1,XMM2/M128	Set ZF and CF depending on sign bit AND and ANDN of packed DP FP sources.
VTESTPD	YMM,YMM/M256	AVX	VTESTPD YMM1,YMM2/M256	Set ZF and CF depending on sign bit AND and ANDN of packed DP FP sources.
;--------------------------------------------------------
GENERAL	VZEROALL	Zero All YMM Registers	VZEROALL
VZEROALL		AVX	VZEROALL	Zero all YMM registers.
;--------------------------------------------------------
GENERAL	VZEROUPPER	Zero Upper Bits of YMM Registers	VZEROUPPER
VZEROUPPER		AVX	VZEROUPPER	Zero upper 128 bits of all YMM registers.
;--------------------------------------------------------
GENERAL	WAIT	Wait	WAIT_FWAIT
WAIT		8086	WAIT	Check pending unmasked FP exceptions.
GENERAL	FWAIT	Wait	WAIT_FWAIT
FWAIT		8086	FWAIT	Check pending unmasked FP exceptions.
;--------------------------------------------------------
GENERAL	WBINVD	Write Back and Invalidate Cache	WBINVD
WBINVD		8086	WBINVD	Write back and flush Internal caches; initiate writing-back and flushing of external caches.
;--------------------------------------------------------
GENERAL	WBNOINVD	Write Back and Do Not Invalidate Cache	WBNOINVD
WBNOINVD			WBNOINVD	initiate writing-back without flushing of external caches.
;--------------------------------------------------------
GENERAL	WRFSBASE	Write FS/GS Segment Base	WRFSBASE_WRGSBASE
WRFSBASE	R32	FSGSBASE	WRFSBASE R32	Load the FS base address with the 32-bit value in the source register.
WRFSBASE	R64	FSGSBASE	WRFSBASE R64	Load the FS base address with the 64-bit value in the source register.
GENERAL	WRGSBASE	Write FS/GS Segment Base	WRFSBASE_WRGSBASE
WRGSBASE	R32	FSGSBASE	WRGSBASE R32	Load the GS base address with the 32-bit value in the source register.
WRGSBASE	R64	FSGSBASE	WRGSBASE R64	Load the GS base address with the 64-bit value in the source register.
;--------------------------------------------------------
GENERAL	WRMSR	Write to Model Specific Register	WRMSR
WRMSR		8086	WRMSR	Write the value in EDX:EAX to MSR specified by ECX.
;--------------------------------------------------------
GENERAL	WRPKRU	Write Data to User Page Key Register	WRPKRU
WRPKRU		8086	WRPKRU	Writes EAX into PKRU.
;--------------------------------------------------------
GENERAL	XABORT	Transactional Abort	XABORT
XABORT	IMM8	RTM	XABORT IMM8	Causes an RTM abort if in RTM execution
;--------------------------------------------------------
GENERAL	XACQUIRE	Hardware Lock Elision Prefix Hints	XACQUIRE_XRELEASE
XACQUIRE		8086	XACQUIRE	A hint used with an “XACQUIRE-enabled“ instruction to start lock elision on the instruction memory operand address.
GENERAL	XRELEASE	Hardware Lock Elision Prefix Hints	XACQUIRE_XRELEASE
XRELEASE		8086	XRELEASE	A hint used with an “XRELEASE-enabled“ instruction to end lock elision on the instruction memory operand address.
;--------------------------------------------------------
GENERAL	XADD	Exchange and Add	XADD
XADD	R/M8,R8	8086	XADD R/M8,R8	Exchange r8 and r/m8; load sum into r/m8.
XADD	R/M8,R8	8086	XADD R/M8,R8	Exchange r8 and r/m8; load sum into r/m8.
XADD	R/M16,R16	8086	XADD R/M16,R16	Exchange r16 and r/m16; load sum into r/m16.
XADD	R/M32,R32	386	XADD R/M32,R32	Exchange r32 and r/m32; load sum into r/m32.
XADD	R/M64,R64	X64	XADD R/M64,R64	Exchange r64 and r/m64; load sum into r/m64.
;--------------------------------------------------------
GENERAL	XBEGIN	Transactional Begin	XBEGIN
XBEGIN	REL16	RTM	XBEGIN REL16	Specifies the start of an RTM region. Provides a 16-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
XBEGIN	REL32	RTM	XBEGIN REL32	Specifies the start of an RTM region. Provides a 32-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
;--------------------------------------------------------
GENERAL	XCHG	Exchange Register/Memory with Register	XCHG
XCHG	AX,R16	8086	XCHG AX,R16	Exchange r16 with AX.
XCHG	R16,AX	8086	XCHG R16,AX	Exchange AX with r16.
XCHG	EAX,R32	386	XCHG EAX,R32	Exchange r32 with EAX.
XCHG	RAX,R64	X64	XCHG RAX,R64	Exchange r64 with RAX.
XCHG	R32,EAX	386	XCHG R32,EAX	Exchange EAX with r32.
XCHG	R64,RAX	X64	XCHG R64,RAX	Exchange RAX with r64.
XCHG	R/M8,R8	8086	XCHG R/M8,R8	Exchange r8 (byte register) with byte from r/m8.
XCHG	R/M8,R8	8086	XCHG R/M8,R8	Exchange r8 (byte register) with byte from r/m8.
XCHG	R8,R/M8	8086	XCHG R8,R/M8	Exchange byte from r/m8 with r8 (byte register).
XCHG	R8,R/M8	8086	XCHG R8,R/M8	Exchange byte from r/m8 with r8 (byte register).
XCHG	R/M16,R16	8086	XCHG R/M16,R16	Exchange r16 with word from r/m16.
XCHG	R16,R/M16	8086	XCHG R16,R/M16	Exchange word from r/m16 with r16.
XCHG	R/M32,R32	386	XCHG R/M32,R32	Exchange r32 with doubleword from r/m32.
XCHG	R/M64,R64	X64	XCHG R/M64,R64	Exchange r64 with quadword from r/m64.
XCHG	R32,R/M32	386	XCHG R32,R/M32	Exchange doubleword from r/m32 with r32.
XCHG	R64,R/M64	X64	XCHG R64,R/M64	Exchange quadword from r/m64 with r64.
;--------------------------------------------------------
GENERAL	XEND	Transactional End	XEND
XEND		RTM	XEND	Specifies the end of an RTM code region.
;--------------------------------------------------------
GENERAL	XGETBV	Get Value of Extended Control Register	XGETBV
XGETBV		8086	XGETBV	Reads an XCR specified by ECX into EDX:EAX.
;--------------------------------------------------------
GENERAL	XLAT	Table Look-up Translation	XLAT_XLATB
XLAT	M8	8086	XLAT M8	Set AL to memory byte DS:[(E)BX + unsigned AL].
GENERAL	XLATB	Table Look-up Translation	XLAT_XLATB
XLATB		8086	XLATB	Set AL to memory byte DS:[(E)BX + unsigned AL].
XLATB		8086	XLATB	Set AL to memory byte [RBX + unsigned AL].
;--------------------------------------------------------
GENERAL	XOR	Logical Exclusive OR	XOR
XOR	AL,IMM8	8086	XOR AL,IMM8	AL XOR imm8.
XOR	AX,IMM16	8086	XOR AX,IMM16	AX XOR imm16.
XOR	EAX,IMM32	386	XOR EAX,IMM32	EAX XOR imm32.
XOR	RAX,IMM32	386	XOR RAX,IMM32	RAX XOR imm32 (sign-extended).
XOR	R/M8,IMM8	8086	XOR R/M8,IMM8	r/m8 XOR imm8.
XOR	R/M8,IMM8	8086	XOR R/M8,IMM8	r/m8 XOR imm8.
XOR	R/M16,IMM16	8086	XOR R/M16,IMM16	r/m16 XOR imm16.
XOR	R/M32,IMM32	386	XOR R/M32,IMM32	r/m32 XOR imm32.
XOR	R/M64,IMM32	X64	XOR R/M64,IMM32	r/m64 XOR imm32 (sign-extended).
XOR	R/M16,IMM8	8086	XOR R/M16,IMM8	r/m16 XOR imm8 (sign-extended).
XOR	R/M32,IMM8	386	XOR R/M32,IMM8	r/m32 XOR imm8 (sign-extended).
XOR	R/M64,IMM8	X64	XOR R/M64,IMM8	r/m64 XOR imm8 (sign-extended).
XOR	R/M8,R8	8086	XOR R/M8,R8	r/m8 XOR r8.
XOR	R/M8,R8	8086	XOR R/M8,R8	r/m8 XOR r8.
XOR	R/M16,R16	8086	XOR R/M16,R16	r/m16 XOR r16.
XOR	R/M32,R32	386	XOR R/M32,R32	r/m32 XOR r32.
XOR	R/M64,R64	X64	XOR R/M64,R64	r/m64 XOR r64.
XOR	R8,R/M8	8086	XOR R8,R/M8	r8 XOR r/m8.
XOR	R8,R/M8	8086	XOR R8,R/M8	r8 XOR r/m8.
XOR	R16,R/M16	8086	XOR R16,R/M16	r16 XOR r/m16.
XOR	R32,R/M32	386	XOR R32,R/M32	r32 XOR r/m32.
XOR	R64,R/M64	X64	XOR R64,R/M64	r64 XOR r/m64.
;--------------------------------------------------------
GENERAL	XORPD	Bitwise Logical XOR of Packed Double Precision Floating-Point Values	XORPD
XORPD	XMM,XMM/M128	SSE2	XORPD XMM1,XMM2/M128	Return the bitwise logical XOR of packed DP FP values in xmm1 and xmm2/mem.
GENERAL	VXORPD	Bitwise Logical XOR of Packed Double Precision Floating-Point Values	XORPD
VXORPD	XMM,XMM,XMM/M128	AVX	VXORPD XMM1,XMM2,XMM3/M128	Return the bitwise logical XOR of packed DP FP values in xmm2 and xmm3/mem.
VXORPD	YMM,YMM,YMM/M256	AVX	VXORPD YMM1,YMM2,YMM3/M256	Return the bitwise logical XOR of packed DP FP values in ymm2 and ymm3/mem.
VXORPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512_VL,AVX512_DQ	VXORPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical XOR of packed DP FP values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VXORPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512_VL,AVX512_DQ	VXORPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical XOR of packed DP FP values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VXORPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512_DQ	VXORPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical XOR of packed DP FP values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	XORPS	Bitwise Logical XOR of Packed Single Precision Floating-Point Values	XORPS
XORPS	XMM,XMM/M128	SSE	XORPS XMM1,XMM2/M128	Return the bitwise logical XOR of packed SP FP values in xmm1 and xmm2/mem.
GENERAL	VXORPS	Bitwise Logical XOR of Packed Single Precision Floating-Point Values	XORPS
VXORPS	XMM,XMM,XMM/M128	AVX	VXORPS XMM1,XMM2,XMM3/M128	Return the bitwise logical XOR of packed SP FP values in xmm2 and xmm3/mem.
VXORPS	YMM,YMM,YMM/M256	AVX	VXORPS YMM1,YMM2,YMM3/M256	Return the bitwise logical XOR of packed SP FP values in ymm2 and ymm3/mem.
VXORPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512_VL,AVX512_DQ	VXORPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical XOR of packed SP FP values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VXORPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512_VL,AVX512_DQ	VXORPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical XOR of packed SP FP values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VXORPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512_DQ	VXORPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical XOR of packed SP FP values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
;--------------------------------------------------------
GENERAL	XRSTOR	Restore Processor Extended States	XRSTOR
XRSTOR	MEM	XSAVEOPT	XRSTOR MEM	Restore state components specified by EDX:EAX from mem.
GENERAL	XRSTOR64	Restore Processor Extended States	XRSTOR
XRSTOR64	MEM	XSAVEOPT	XRSTOR64 MEM	Restore state components specified by EDX:EAX from mem.
;--------------------------------------------------------
GENERAL	XRSTORS	Restore Processor Extended States Supervisor	XRSTORS
XRSTORS	MEM	XSAVEOPT	XRSTORS MEM	Restore state components specified by EDX:EAX from mem.
GENERAL	XRSTORS64	Restore Processor Extended States Supervisor	XRSTORS
XRSTORS64	MEM	XSAVEOPT	XRSTORS64 MEM	Restore state components specified by EDX:EAX from mem.
;--------------------------------------------------------
GENERAL	XSAVE	Save Processor Extended States	XSAVE
XSAVE	MEM	XSAVEOPT	XSAVE MEM	Save state components specified by EDX:EAX to mem.
GENERAL	XSAVE64	Save Processor Extended States	XSAVE
XSAVE64	MEM	XSAVEOPT	XSAVE64 MEM	Save state components specified by EDX:EAX to mem.
;--------------------------------------------------------
GENERAL	XSAVEC	Save Processor Extended States with Compaction	XSAVEC
XSAVEC	MEM	XSAVEOPT	XSAVEC MEM	Save state components specified by EDX:EAX to mem with compaction.
GENERAL	XSAVEC64	Save Processor Extended States with Compaction	XSAVEC
XSAVEC64	MEM	XSAVEOPT	XSAVEC64 MEM	Save state components specified by EDX:EAX to mem with compaction.
;--------------------------------------------------------
GENERAL	XSAVEOPT	Save Processor Extended States Optimized	XSAVEOPT
XSAVEOPT	MEM	XSAVEOPT	XSAVEOPT MEM	Save state components specified by EDX:EAX to mem, optimizing if possible.
GENERAL	XSAVEOPT64	Save Processor Extended States Optimized	XSAVEOPT
XSAVEOPT64	MEM	XSAVEOPT	XSAVEOPT64 MEM	Save state components specified by EDX:EAX to mem, optimizing if possible.
;--------------------------------------------------------
GENERAL	XSAVES	Save Processor Extended States Supervisor	XSAVES
XSAVES	MEM	XSAVEOPT	XSAVES MEM	Save state components specified by EDX:EAX to mem with compaction, optimizing if possible.
GENERAL	XSAVES64	Save Processor Extended States Supervisor	XSAVES
XSAVES64	MEM	XSAVEOPT	XSAVES64 MEM	Save state components specified by EDX:EAX to mem with compaction, optimizing if possible.
;--------------------------------------------------------
GENERAL	XSETBV	Set Extended Control Register	XSETBV
XSETBV		8086	XSETBV	Write the value in EDX:EAX to the XCR specified by ECX.
;--------------------------------------------------------
GENERAL	XTEST	Test If In Transactional Execution	XTEST
XTEST		HLE,RTM	XTEST	Test if executing in a transactional region
