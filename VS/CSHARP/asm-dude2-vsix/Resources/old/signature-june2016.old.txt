GENERAL	AAA	ASCII Adjust After Addition	AAA.html
GENERAL	AAD	ASCII Adjust AX Before Division	AAD.html
GENERAL	AAM	ASCII Adjust AX After Multiply	AAM.html
GENERAL	AAS	ASCII Adjust AL After Subtraction	AAS.html
GENERAL	ADC	Add with Carry	ADC.html
GENERAL	ADCX	Unsigned Integer Addition of Two Operands with Carry Flag	ADCX.html
GENERAL	ADD	Add	ADD.html
GENERAL	ADDPD	Add Packed Double-Precision Floating-Point Values	ADDPD.html
GENERAL	VADDPD	Add Packed Double-Precision Floating-Point Values	ADDPD.html
GENERAL	ADDPS	Add Packed Single-Precision Floating-Point Values	ADDPS.html
GENERAL	VADDPS	Add Packed Single-Precision Floating-Point Values	ADDPS.html
GENERAL	ADDSD	Add Scalar Double-Precision Floating-Point Values	ADDSD.html
GENERAL	VADDSD	Add Scalar Double-Precision Floating-Point Values	ADDSD.html
GENERAL	ADDSS	Add Scalar Single-Precision Floating-Point Values	ADDSS.html
GENERAL	VADDSS	Add Scalar Single-Precision Floating-Point Values	ADDSS.html
GENERAL	ADDSUBPD	Packed Double-FP Add/Subtract	ADDSUBPD.html
GENERAL	VADDSUBPD	Packed Double-FP Add/Subtract	ADDSUBPD.html
GENERAL	ADDSUBPS	Packed Single-FP Add/Subtract	ADDSUBPS.html
GENERAL	VADDSUBPS	Packed Single-FP Add/Subtract	ADDSUBPS.html
GENERAL	ADOX	Unsigned Integer Addition of Two Operands with Overflow Flag	ADOX.html
GENERAL	AESDEC	Perform One Round of an AES Decryption Flow	AESDEC.html
GENERAL	VAESDEC	Perform One Round of an AES Decryption Flow	AESDEC.html
GENERAL	VAESDECLAST	Perform Last Round of an AES Decryption Flow	AESDECLAST.html
GENERAL	AESDECLAST	Perform Last Round of an AES Decryption Flow	AESDECLAST.html
GENERAL	AESENC	Perform One Round of an AES Encryption Flow	AESENC.html
GENERAL	VAESENC	Perform One Round of an AES Encryption Flow	AESENC.html
GENERAL	AESENCLAST	Perform Last Round of an AES Encryption Flow	AESENCLAST.html
GENERAL	VAESENCLAST	Perform Last Round of an AES Encryption Flow	AESENCLAST.html
GENERAL	VAESIMC	Perform the AES InvMixColumn Transformation	AESIMC.html
GENERAL	AESIMC	Perform the AES InvMixColumn Transformation	AESIMC.html
GENERAL	VAESKEYGENASSIST	AES Round Key Generation Assist	AESKEYGENASSIST.html
GENERAL	AESKEYGENASSIST	AES Round Key Generation Assist	AESKEYGENASSIST.html
GENERAL	AND	Logical AND	AND.html
GENERAL	ANDN	Logical AND NOT	ANDN.html
GENERAL	ANDNPD	Bitwise Logical AND NOT of Packed Double Precision Floating-Point Values	ANDNPD.html
GENERAL	VANDNPD	Bitwise Logical AND NOT of Packed Double Precision Floating-Point Values	ANDNPD.html
GENERAL	ANDNPS	Bitwise Logical AND NOT of Packed Single Precision Floating-Point Values	ANDNPS.html
GENERAL	VANDNPS	Bitwise Logical AND NOT of Packed Single Precision Floating-Point Values	ANDNPS.html
GENERAL	ANDPD	Bitwise Logical AND of Packed Double Precision Floating-Point Values	ANDPD.html
GENERAL	VANDPD	Bitwise Logical AND of Packed Double Precision Floating-Point Values	ANDPD.html
GENERAL	ANDPS	Bitwise Logical AND of Packed Single Precision Floating-Point Values	ANDPS.html
GENERAL	VANDPS	Bitwise Logical AND of Packed Single Precision Floating-Point Values	ANDPS.html
GENERAL	ARPL	Adjust RPL Field of Segment Selector	ARPL.html
GENERAL	BEXTR	Bit Field Extract	BEXTR.html
GENERAL	BLENDPD	Blend Packed Double Precision Floating-Point Values	BLENDPD.html
GENERAL	VBLENDPD	Blend Packed Double Precision Floating-Point Values	BLENDPD.html
GENERAL	BLENDPS	Blend Packed Single Precision Floating-Point Values	BLENDPS.html
GENERAL	VBLENDPS	Blend Packed Single Precision Floating-Point Values	BLENDPS.html
GENERAL	VBLENDVPD	Variable Blend Packed Double Precision Floating-Point Values	BLENDVPD.html
GENERAL	BLENDVPD	Variable Blend Packed Double Precision Floating-Point Values	BLENDVPD.html
GENERAL	VBLENDVPS	Variable Blend Packed Single Precision Floating-Point Values	BLENDVPS.html
GENERAL	BLENDVPS	Variable Blend Packed Single Precision Floating-Point Values	BLENDVPS.html
GENERAL	BLSI	Extract Lowest Set Isolated Bit	BLSI.html
GENERAL	BLSMSK	Get Mask Up to Lowest Set Bit	BLSMSK.html
GENERAL	BLSR	Reset Lowest Set Bit	BLSR.html
GENERAL	BNDCL	Check Lower Bound	BNDCL.html
GENERAL	BNDCN	Check Upper Bound	BNDCU_BNDCN.html
GENERAL	BNDCU	Check Upper Bound	BNDCU_BNDCN.html
GENERAL	BNDLDX	Load Extended Bounds Using Address Translation	BNDLDX.html
GENERAL	BNDMK	Make Bounds	BNDMK.html
GENERAL	BNDMOV	Move Bounds	BNDMOV.html
GENERAL	BNDSTX	Store Extended Bounds Using Address Translation	BNDSTX.html
GENERAL	BOUND	Check Array Index Against Bounds	BOUND.html
GENERAL	BSF	Bit Scan Forward	BSF.html
GENERAL	BSR	Bit Scan Reverse	BSR.html
GENERAL	BSWAP	Byte Swap	BSWAP.html
GENERAL	BT	Bit Test	BT.html
GENERAL	BTC	Bit Test and Complement	BTC.html
GENERAL	BTR	Bit Test and Reset	BTR.html
GENERAL	BTS	Bit Test and Set	BTS.html
GENERAL	BZHI	Zero High Bits Starting with Specified Bit Position	BZHI.html
GENERAL	CALL	Call Procedure	CALL.html
GENERAL	CBW	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE.html
GENERAL	CWDE	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE.html
GENERAL	CDQE	Convert Byte to Word/Convert Word to Doubleword/Convert Doubleword to Quadword	CBW_CWDE_CDQE.html
GENERAL	CLAC	Clear AC Flag in EFLAGS Register	CLAC.html
GENERAL	CLC	Clear Carry Flag	CLC.html
GENERAL	CLD	Clear Direction Flag	CLD.html
GENERAL	CLFLUSH	Flush Cache Line	CLFLUSH.html
GENERAL	CLFLUSHOPT	Flush Cache Line Optimized	CLFLUSHOPT.html
GENERAL	CLI	Clear Interrupt Flag	CLI.html
GENERAL	CLTS	Clear Task-Switched Flag in CR0	CLTS.html
GENERAL	CMC	Complement Carry Flag	CMC.html
GENERAL	CMOVE	Conditional Move	CMOVcc.html
GENERAL	CMOVBE	Conditional Move	CMOVcc.html
GENERAL	CMOVG	Conditional Move	CMOVcc.html
GENERAL	CMOVAE	Conditional Move	CMOVcc.html
GENERAL	CMOVA	Conditional Move	CMOVcc.html
GENERAL	CMOVB	Conditional Move	CMOVcc.html
GENERAL	CMOVC	Conditional Move	CMOVcc.html
GENERAL	CMOVNAE	Conditional Move	CMOVcc.html
GENERAL	CMOVL	Conditional Move	CMOVcc.html
GENERAL	CMOVNA	Conditional Move	CMOVcc.html
GENERAL	CMOVNB	Conditional Move	CMOVcc.html
GENERAL	CMOVGE	Conditional Move	CMOVcc.html
GENERAL	CMOVLE	Conditional Move	CMOVcc.html
GENERAL	CMP	Compare Two Operands	CMP.html
GENERAL	VCMPPD	Compare Packed Double-Precision Floating-Point Values	CMPPD.html
GENERAL	CMPPD	Compare Packed Double-Precision Floating-Point Values	CMPPD.html
GENERAL	VCMPPS	Compare Packed Single-Precision Floating-Point Values	CMPPS.html
GENERAL	CMPPS	Compare Packed Single-Precision Floating-Point Values	CMPPS.html
GENERAL	CMPSD	Compare Scalar Double-Precision Floating-Point Value	CMPSD.html
GENERAL	VCMPSD	Compare Scalar Double-Precision Floating-Point Value	CMPSD.html
GENERAL	CMPSS	Compare Scalar Single-Precision Floating-Point Value	CMPSS.html
GENERAL	VCMPSS	Compare Scalar Single-Precision Floating-Point Value	CMPSS.html
GENERAL	CMPSQ	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ.html
GENERAL	CMPSB	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ.html
GENERAL	CMPS	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ.html
GENERAL	CMPSD	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ.html
GENERAL	CMPSW	Compare String Operands	CMPS_CMPSB_CMPSW_CMPSD_CMPSQ.html
GENERAL	CMPXCHG	Compare and Exchange	CMPXCHG.html
GENERAL	CMPXCHG16B	Compare and Exchange Bytes	CMPXCHG8B_CMPXCHG16B.html
GENERAL	CMPXCHG8B	Compare and Exchange Bytes	CMPXCHG8B_CMPXCHG16B.html
GENERAL	VCOMISD	Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS	COMISD.html
GENERAL	COMISD	Compare Scalar Ordered Double-Precision Floating-Point Values and Set EFLAGS	COMISD.html
GENERAL	VCOMISS	Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS	COMISS.html
GENERAL	COMISS	Compare Scalar Ordered Single-Precision Floating-Point Values and Set EFLAGS	COMISS.html
GENERAL	CPUID	CPU Identification	CPUID.html
GENERAL	CRC32	Accumulate CRC32 Value	CRC32.html
GENERAL	VCVTDQ2PD	Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values	CVTDQ2PD.html
GENERAL	CVTDQ2PD	Convert Packed Doubleword Integers to Packed Double-Precision Floating-Point Values	CVTDQ2PD.html
GENERAL	VCVTDQ2PS	Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values	CVTDQ2PS.html
GENERAL	CVTDQ2PS	Convert Packed Doubleword Integers to Packed Single-Precision Floating-Point Values	CVTDQ2PS.html
GENERAL	VCVTPD2DQ	Convert Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTPD2DQ.html
GENERAL	CVTPD2DQ	Convert Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTPD2DQ.html
GENERAL	CVTPD2PI	Convert Packed Double-Precision FP Values to Packed Dword Integers	CVTPD2PI.html
GENERAL	CVTPD2PS	Convert Packed Double-Precision Floating-Point Values to Packed Single-Precision Floating-Point Values	CVTPD2PS.html
GENERAL	VCVTPD2PS	Convert Packed Double-Precision Floating-Point Values to Packed Single-Precision Floating-Point Values	CVTPD2PS.html
GENERAL	CVTPI2PD	Convert Packed Dword Integers to Packed Double-Precision FP Values	CVTPI2PD.html
GENERAL	CVTPI2PS	Convert Packed Dword Integers to Packed Single-Precision FP Values	CVTPI2PS.html
GENERAL	CVTPS2DQ	Convert Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTPS2DQ.html
GENERAL	VCVTPS2DQ	Convert Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTPS2DQ.html
GENERAL	VCVTPS2PD	Convert Packed Single-Precision Floating-Point Values to Packed Double-Precision Floating-Point Values	CVTPS2PD.html
GENERAL	CVTPS2PD	Convert Packed Single-Precision Floating-Point Values to Packed Double-Precision Floating-Point Values	CVTPS2PD.html
GENERAL	CVTPS2PI	Convert Packed Single-Precision FP Values to Packed Dword Integers	CVTPS2PI.html
GENERAL	VCVTSD2SI	Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer	CVTSD2SI.html
GENERAL	CVTSD2SI	Convert Scalar Double-Precision Floating-Point Value to Doubleword Integer	CVTSD2SI.html
GENERAL	VCVTSD2SS	Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value	CVTSD2SS.html
GENERAL	CVTSD2SS	Convert Scalar Double-Precision Floating-Point Value to Scalar Single-Precision Floating-Point Value	CVTSD2SS.html
GENERAL	VCVTSI2SD	Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value	CVTSI2SD.html
GENERAL	CVTSI2SD	Convert Doubleword Integer to Scalar Double-Precision Floating-Point Value	CVTSI2SD.html
GENERAL	CVTSI2SS	Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value	CVTSI2SS.html
GENERAL	VCVTSI2SS	Convert Doubleword Integer to Scalar Single-Precision Floating-Point Value	CVTSI2SS.html
GENERAL	CVTSS2SD	Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value	CVTSS2SD.html
GENERAL	VCVTSS2SD	Convert Scalar Single-Precision Floating-Point Value to Scalar Double-Precision Floating-Point Value	CVTSS2SD.html
GENERAL	CVTSS2SI	Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer	CVTSS2SI.html
GENERAL	VCVTSS2SI	Convert Scalar Single-Precision Floating-Point Value to Doubleword Integer	CVTSS2SI.html
GENERAL	VCVTTPD2DQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTTPD2DQ.html
GENERAL	CVTTPD2DQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Doubleword Integers	CVTTPD2DQ.html
GENERAL	CVTTPD2PI	Convert with Truncation Packed Double-Precision FP Values to Packed Dword Integers	CVTTPD2PI.html
GENERAL	CVTTPS2DQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTTPS2DQ.html
GENERAL	VCVTTPS2DQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Signed Doubleword Integer Values	CVTTPS2DQ.html
GENERAL	CVTTPS2PI	Convert with Truncation Packed Single-Precision FP Values to Packed Dword Integers	CVTTPS2PI.html
GENERAL	VCVTTSD2SI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Integer	CVTTSD2SI.html
GENERAL	CVTTSD2SI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Signed Integer	CVTTSD2SI.html
GENERAL	VCVTTSS2SI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Integer	CVTTSS2SI.html
GENERAL	CVTTSS2SI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Integer	CVTTSS2SI.html
GENERAL	CQO	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO.html
GENERAL	CWD	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO.html
GENERAL	CDQ	Convert Word to Doubleword/Convert Doubleword to Quadword	CWD_CDQ_CQO.html
GENERAL	DAA	Decimal Adjust AL after Addition	DAA.html
GENERAL	DAS	Decimal Adjust AL after Subtraction	DAS.html
GENERAL	DEC	Decrement by 1	DEC.html
GENERAL	DIV	Unsigned Divide	DIV.html
GENERAL	DIVPD	Divide Packed Double-Precision Floating-Point Values	DIVPD.html
GENERAL	VDIVPD	Divide Packed Double-Precision Floating-Point Values	DIVPD.html
GENERAL	DIVPS	Divide Packed Single-Precision Floating-Point Values	DIVPS.html
GENERAL	VDIVPS	Divide Packed Single-Precision Floating-Point Values	DIVPS.html
GENERAL	DIVSD	Divide Scalar Double-Precision Floating-Point Value	DIVSD.html
GENERAL	VDIVSD	Divide Scalar Double-Precision Floating-Point Value	DIVSD.html
GENERAL	DIVSS	Divide Scalar Single-Precision Floating-Point Values	DIVSS.html
GENERAL	VDIVSS	Divide Scalar Single-Precision Floating-Point Values	DIVSS.html
GENERAL	DPPD	Dot Product of Packed Double Precision Floating-Point Values	DPPD.html
GENERAL	VDPPD	Dot Product of Packed Double Precision Floating-Point Values	DPPD.html
GENERAL	VDPPS	Dot Product of Packed Single Precision Floating-Point Values	DPPS.html
GENERAL	DPPS	Dot Product of Packed Single Precision Floating-Point Values	DPPS.html
GENERAL	EMMS	Empty MMX Technology State	EMMS.html
GENERAL	ENTER	Make Stack Frame for Procedure Parameters	ENTER.html
GENERAL	EXTRACTPS	Extract Packed Floating-Point Values	EXTRACTPS.html
GENERAL	VEXTRACTPS	Extract Packed Floating-Point Values	EXTRACTPS.html
GENERAL	F2XM1	Compute 2x–1	F2XM1.html
GENERAL	FABS	Absolute Value	FABS.html
GENERAL	FIADD	Add	FADD_FADDP_FIADD.html
GENERAL	FADD	Add	FADD_FADDP_FIADD.html
GENERAL	FADDP	Add	FADD_FADDP_FIADD.html
GENERAL	FBLD	Load Binary Coded Decimal	FBLD.html
GENERAL	FBSTP	Store BCD Integer and Pop	FBSTP.html
GENERAL	FCHS	Change Sign	FCHS.html
GENERAL	FCLEX	Clear Exceptions	FCLEX_FNCLEX.html
GENERAL	FNCLEX	Clear Exceptions	FCLEX_FNCLEX.html
GENERAL	FCMOVNB	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVNBE	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVBE	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVNE	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVE	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVU	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCMOVB	Floating-Point Conditional Move	FCMOVcc.html
GENERAL	FCOMI	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP_FUCOMI_FUCOMIP.html
GENERAL	FCOMIP	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP_FUCOMI_FUCOMIP.html
GENERAL	FUCOMIP	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP_FUCOMI_FUCOMIP.html
GENERAL	FUCOMI	Compare Floating Point Values and Set EFLAGS	FCOMI_FCOMIP_FUCOMI_FUCOMIP.html
GENERAL	FCOMPP	Compare Floating Point Values	FCOM_FCOMP_FCOMPP.html
GENERAL	FCOM	Compare Floating Point Values	FCOM_FCOMP_FCOMPP.html
GENERAL	FCOMP	Compare Floating Point Values	FCOM_FCOMP_FCOMPP.html
GENERAL	FCOS	Cosine	FCOS.html
GENERAL	FDECSTP	Decrement Stack-Top Pointer	FDECSTP.html
GENERAL	FIDIVR	Reverse Divide	FDIVR_FDIVRP_FIDIVR.html
GENERAL	FDIVRP	Reverse Divide	FDIVR_FDIVRP_FIDIVR.html
GENERAL	FDIVR	Reverse Divide	FDIVR_FDIVRP_FIDIVR.html
GENERAL	FIDIV	Divide	FDIV_FDIVP_FIDIV.html
GENERAL	FDIV	Divide	FDIV_FDIVP_FIDIV.html
GENERAL	FDIVP	Divide	FDIV_FDIVP_FIDIV.html
GENERAL	FFREE	Free Floating-Point Register	FFREE.html
GENERAL	FICOM	Compare Integer	FICOM_FICOMP.html
GENERAL	FICOMP	Compare Integer	FICOM_FICOMP.html
GENERAL	FILD	Load Integer	FILD.html
GENERAL	FINCSTP	Increment Stack-Top Pointer	FINCSTP.html
GENERAL	FNINIT	Initialize Floating-Point Unit	FINIT_FNINIT.html
GENERAL	FINIT	Initialize Floating-Point Unit	FINIT_FNINIT.html
GENERAL	FISTTP	Store Integer with Truncation	FISTTP.html
GENERAL	FISTP	Store Integer	FIST_FISTP.html
GENERAL	FIST	Store Integer	FIST_FISTP.html
GENERAL	FLD	Load Floating Point Value	FLD.html
GENERAL	FLDLN2	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDL2T	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLD1	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDL2E	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDPI	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDLG2	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDZ	Load Constant	FLD1_FLDL2T_FLDL2E_FLDPI_FLDLG2_FLDLN2_FLDZ.html
GENERAL	FLDCW	Load x87 FPU Control Word	FLDCW.html
GENERAL	FLDENV	Load x87 FPU Environment	FLDENV.html
GENERAL	FMULP	Multiply	FMUL_FMULP_FIMUL.html
GENERAL	FIMUL	Multiply	FMUL_FMULP_FIMUL.html
GENERAL	FMUL	Multiply	FMUL_FMULP_FIMUL.html
GENERAL	FNOP	No Operation	FNOP.html
GENERAL	FPATAN	Partial Arctangent	FPATAN.html
GENERAL	FPREM	Partial Remainder	FPREM.html
GENERAL	FPREM1	Partial Remainder	FPREM1.html
GENERAL	FPTAN	Partial Tangent	FPTAN.html
GENERAL	FRNDINT	Round to Integer	FRNDINT.html
GENERAL	FRSTOR	Restore x87 FPU State	FRSTOR.html
GENERAL	FNSAVE	Store x87 FPU State	FSAVE_FNSAVE.html
GENERAL	FSAVE	Store x87 FPU State	FSAVE_FNSAVE.html
GENERAL	FSCALE	Scale	FSCALE.html
GENERAL	FSIN	Sine	FSIN.html
GENERAL	FSINCOS	Sine and Cosine	FSINCOS.html
GENERAL	FSQRT	Square Root	FSQRT.html
GENERAL	FNSTCW	Store x87 FPU Control Word	FSTCW_FNSTCW.html
GENERAL	FSTCW	Store x87 FPU Control Word	FSTCW_FNSTCW.html
GENERAL	FNSTENV	Store x87 FPU Environment	FSTENV_FNSTENV.html
GENERAL	FSTENV	Store x87 FPU Environment	FSTENV_FNSTENV.html
GENERAL	FNSTSW	Store x87 FPU Status Word	FSTSW_FNSTSW.html
GENERAL	FSTSW	Store x87 FPU Status Word	FSTSW_FNSTSW.html
GENERAL	FST	Store Floating Point Value	FST_FSTP.html
GENERAL	FSTP	Store Floating Point Value	FST_FSTP.html
GENERAL	FSUBRP	Reverse Subtract	FSUBR_FSUBRP_FISUBR.html
GENERAL	FSUBR	Reverse Subtract	FSUBR_FSUBRP_FISUBR.html
GENERAL	FISUBR	Reverse Subtract	FSUBR_FSUBRP_FISUBR.html
GENERAL	FISUB	Subtract	FSUB_FSUBP_FISUB.html
GENERAL	FSUBP	Subtract	FSUB_FSUBP_FISUB.html
GENERAL	FSUB	Subtract	FSUB_FSUBP_FISUB.html
GENERAL	FTST	TEST	FTST.html
GENERAL	FUCOM	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP.html
GENERAL	FUCOMPP	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP.html
GENERAL	FUCOMP	Unordered Compare Floating Point Values	FUCOM_FUCOMP_FUCOMPP.html
GENERAL	FXAM	Examine Floating-Point	FXAM.html
GENERAL	FXCH	Exchange Register Contents	FXCH.html
GENERAL	FXRSTOR	Restore x87 FPU, MMX, XMM, and MXCSR State	FXRSTOR.html
GENERAL	FXSAVE	Save x87 FPU, MMX Technology, and SSE State	FXSAVE.html
GENERAL	FXTRACT	Extract Exponent and Significand	FXTRACT.html
GENERAL	FYL2X	Compute y ∗ log2x	FYL2X.html
GENERAL	FYL2XP1	Compute y ∗ log2(x +1)	FYL2XP1.html
GENERAL	VHADDPD	Packed Double-FP Horizontal Add	HADDPD.html
GENERAL	HADDPD	Packed Double-FP Horizontal Add	HADDPD.html
GENERAL	VHADDPS	Packed Single-FP Horizontal Add	HADDPS.html
GENERAL	HADDPS	Packed Single-FP Horizontal Add	HADDPS.html
GENERAL	HLT	Halt	HLT.html
GENERAL	HSUBPD	Packed Double-FP Horizontal Subtract	HSUBPD.html
GENERAL	VHSUBPD	Packed Double-FP Horizontal Subtract	HSUBPD.html
GENERAL	HSUBPS	Packed Single-FP Horizontal Subtract	HSUBPS.html
GENERAL	VHSUBPS	Packed Single-FP Horizontal Subtract	HSUBPS.html
GENERAL	IDIV	Signed Divide	IDIV.html
GENERAL	IMUL	Signed Multiply	IMUL.html
GENERAL	IN	Input from Port	IN.html
GENERAL	INC	Increment by 1	INC.html
GENERAL	INSERTPS	Insert Scalar Single-Precision Floating-Point Value	INSERTPS.html
GENERAL	VINSERTPS	Insert Scalar Single-Precision Floating-Point Value	INSERTPS.html
GENERAL	INSD	Input from Port to String	INS_INSB_INSW_INSD.html
GENERAL	INSB	Input from Port to String	INS_INSB_INSW_INSD.html
GENERAL	INSW	Input from Port to String	INS_INSB_INSW_INSD.html
GENERAL	INS	Input from Port to String	INS_INSB_INSW_INSD.html
GENERAL	INTO	Call to Interrupt Procedure	INT n_INTO_INT 3.html
GENERAL	INT n	Call to Interrupt Procedure	INT n_INTO_INT 3.html
GENERAL	INT 3	Call to Interrupt Procedure	INT n_INTO_INT 3.html
GENERAL	INVD	Invalidate Internal Caches	INVD.html
GENERAL	INVLPG	Invalidate TLB Entries	INVLPG.html
GENERAL	INVPCID	Invalidate Process-Context Identifier	INVPCID.html
GENERAL	IRETD	Interrupt Return	IRET_IRETD.html
GENERAL	IRET	Interrupt Return	IRET_IRETD.html
GENERAL	JNE	Jump if Condition Is Met	Jcc.html
GENERAL	JNGE	Jump if Condition Is Met	Jcc.html
GENERAL	JL	Jump if Condition Is Met	Jcc.html
GENERAL	JLE	Jump if Condition Is Met	Jcc.html
GENERAL	JNG	Jump if Condition Is Met	Jcc.html
GENERAL	JRCXZ	Jump if Condition Is Met	Jcc.html
GENERAL	JO	Jump if Condition Is Met	Jcc.html
GENERAL	JP	Jump if Condition Is Met	Jcc.html
GENERAL	JPO	Jump if Condition Is Met	Jcc.html
GENERAL	JNL	Jump if Condition Is Met	Jcc.html
GENERAL	JS	Jump if Condition Is Met	Jcc.html
GENERAL	JNAE	Jump if Condition Is Met	Jcc.html
GENERAL	JNO	Jump if Condition Is Met	Jcc.html
GENERAL	JBE	Jump if Condition Is Met	Jcc.html
GENERAL	JNP	Jump if Condition Is Met	Jcc.html
GENERAL	JNS	Jump if Condition Is Met	Jcc.html
GENERAL	JZ	Jump if Condition Is Met	Jcc.html
GENERAL	JNZ	Jump if Condition Is Met	Jcc.html
GENERAL	JCXZ	Jump if Condition Is Met	Jcc.html
GENERAL	JECXZ	Jump if Condition Is Met	Jcc.html
GENERAL	JNLE	Jump if Condition Is Met	Jcc.html
GENERAL	JNBE	Jump if Condition Is Met	Jcc.html
GENERAL	JGE	Jump if Condition Is Met	Jcc.html
GENERAL	JAE	Jump if Condition Is Met	Jcc.html
GENERAL	JA	Jump if Condition Is Met	Jcc.html
GENERAL	JB	Jump if Condition Is Met	Jcc.html
GENERAL	JC	Jump if Condition Is Met	Jcc.html
GENERAL	JE	Jump if Condition Is Met	Jcc.html
GENERAL	JNA	Jump if Condition Is Met	Jcc.html
GENERAL	JG	Jump if Condition Is Met	Jcc.html
GENERAL	JNC	Jump if Condition Is Met	Jcc.html
GENERAL	JPE	Jump if Condition Is Met	Jcc.html
GENERAL	JNB	Jump if Condition Is Met	Jcc.html
GENERAL	JMP	Jump	JMP.html
GENERAL	KADDD	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD.html
GENERAL	KADDB	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD.html
GENERAL	KADDQ	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD.html
GENERAL	KADDW	ADD Two Masks	KADDW_KADDB_KADDQ_KADDD.html
GENERAL	KANDND	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND.html
GENERAL	KANDNB	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND.html
GENERAL	KANDNW	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND.html
GENERAL	KANDNQ	Bitwise Logical AND NOT Masks	KANDNW_KANDNB_KANDNQ_KANDND.html
GENERAL	KANDW	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD.html
GENERAL	KANDD	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD.html
GENERAL	KANDB	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD.html
GENERAL	KANDQ	Bitwise Logical AND Masks	KANDW_KANDB_KANDQ_KANDD.html
GENERAL	KMOVW	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD.html
GENERAL	KMOVB	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD.html
GENERAL	KMOVQ	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD.html
GENERAL	KMOVD	Move from and to Mask Registers	KMOVW_KMOVB_KMOVQ_KMOVD.html
GENERAL	KNOTD	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD.html
GENERAL	KNOTW	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD.html
GENERAL	KNOTQ	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD.html
GENERAL	KNOTB	NOT Mask Register	KNOTW_KNOTB_KNOTQ_KNOTD.html
GENERAL	KORTESTB	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD.html
GENERAL	KORTESTQ	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD.html
GENERAL	KORTESTW	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD.html
GENERAL	KORTESTD	OR Masks And Set Flags	KORTESTW_KORTESTB_KORTESTQ_KORTESTD.html
GENERAL	KORQ	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD.html
GENERAL	KORD	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD.html
GENERAL	KORB	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD.html
GENERAL	KORW	Bitwise Logical OR Masks	KORW_KORB_KORQ_KORD.html
GENERAL	KSHIFTLW	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD.html
GENERAL	KSHIFTLQ	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD.html
GENERAL	KSHIFTLB	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD.html
GENERAL	KSHIFTLD	Shift Left Mask Registers	KSHIFTLW_KSHIFTLB_KSHIFTLQ_KSHIFTLD.html
GENERAL	KSHIFTRQ	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD.html
GENERAL	KSHIFTRB	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD.html
GENERAL	KSHIFTRD	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD.html
GENERAL	KSHIFTRW	Shift Right Mask Registers	KSHIFTRW_KSHIFTRB_KSHIFTRQ_KSHIFTRD.html
GENERAL	KTESTW	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD.html
GENERAL	KTESTD	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD.html
GENERAL	KTESTB	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD.html
GENERAL	KTESTQ	Packed Bit Test Masks and Set Flags	KTESTW_KTESTB_KTESTQ_KTESTD.html
GENERAL	KUNPCKBW	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ.html
GENERAL	KUNPCKWD	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ.html
GENERAL	KUNPCKDQ	Unpack for Mask Registers	KUNPCKBW_KUNPCKWD_KUNPCKDQ.html
GENERAL	KXNORQ	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD.html
GENERAL	KXNORB	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD.html
GENERAL	KXNORD	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD.html
GENERAL	KXNORW	Bitwise Logical XNOR Masks	KXNORW_KXNORB_KXNORQ_KXNORD.html
GENERAL	KXORB	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD.html
GENERAL	KXORQ	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD.html
GENERAL	KXORW	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD.html
GENERAL	KXORD	Bitwise Logical XOR Masks	KXORW_KXORB_KXORQ_KXORD.html
GENERAL	LAHF	Load Status Flags into AH Register	LAHF.html
GENERAL	LAR	Load Access Rights Byte	LAR.html
GENERAL	VLDDQU	Load Unaligned Integer 128 Bits	LDDQU.html
GENERAL	LDDQU	Load Unaligned Integer 128 Bits	LDDQU.html
GENERAL	LDMXCSR	Load MXCSR Register	LDMXCSR.html
GENERAL	VLDMXCSR	Load MXCSR Register	LDMXCSR.html
GENERAL	LFS	Load Far Pointer	LDS_LES_LFS_LGS_LSS.html
GENERAL	LGS	Load Far Pointer	LDS_LES_LFS_LGS_LSS.html
GENERAL	LDS	Load Far Pointer	LDS_LES_LFS_LGS_LSS.html
GENERAL	LES	Load Far Pointer	LDS_LES_LFS_LGS_LSS.html
GENERAL	LSS	Load Far Pointer	LDS_LES_LFS_LGS_LSS.html
GENERAL	LEA	Load Effective Address	LEA.html
GENERAL	LEAVE	High Level Procedure Exit	LEAVE.html
GENERAL	LFENCE	Load Fence	LFENCE.html
GENERAL	LGDT	Load Global/Interrupt Descriptor Table Register	LGDT_LIDT.html
GENERAL	LIDT	Load Global/Interrupt Descriptor Table Register	LGDT_LIDT.html
GENERAL	LLDT	Load Local Descriptor Table Register	LLDT.html
GENERAL	LMSW	Load Machine Status Word	LMSW.html
GENERAL	LOCK	Assert LOCK# Signal Prefix	LOCK.html
GENERAL	LODS	Load String	LODS_LODSB_LODSW_LODSD_LODSQ.html
GENERAL	LODSD	Load String	LODS_LODSB_LODSW_LODSD_LODSQ.html
GENERAL	LODSB	Load String	LODS_LODSB_LODSW_LODSD_LODSQ.html
GENERAL	LODSW	Load String	LODS_LODSB_LODSW_LODSD_LODSQ.html
GENERAL	LODSQ	Load String	LODS_LODSB_LODSW_LODSD_LODSQ.html
GENERAL	LOOP	Loop According to ECX Counter	LOOP_LOOPcc.html
GENERAL	LOOPcc	Loop According to ECX Counter	LOOP_LOOPcc.html
GENERAL	LSL	Load Segment Limit	LSL.html
GENERAL	LTR	Load Task Register	LTR.html
GENERAL	LZCNT	Count the Number of Leading Zero Bits	LZCNT.html
GENERAL	VMASKMOVDQU	Store Selected Bytes of Double Quadword	MASKMOVDQU.html
GENERAL	MASKMOVDQU	Store Selected Bytes of Double Quadword	MASKMOVDQU.html
GENERAL	MASKMOVQ	Store Selected Bytes of Quadword	MASKMOVQ.html
GENERAL	MAXPD	Maximum of Packed Double-Precision Floating-Point Values	MAXPD.html
GENERAL	VMAXPD	Maximum of Packed Double-Precision Floating-Point Values	MAXPD.html
GENERAL	MAXPS	Maximum of Packed Single-Precision Floating-Point Values	MAXPS.html
GENERAL	VMAXPS	Maximum of Packed Single-Precision Floating-Point Values	MAXPS.html
GENERAL	VMAXSD	Return Maximum Scalar Double-Precision Floating-Point Value	MAXSD.html
GENERAL	MAXSD	Return Maximum Scalar Double-Precision Floating-Point Value	MAXSD.html
GENERAL	VMAXSS	Return Maximum Scalar Single-Precision Floating-Point Value	MAXSS.html
GENERAL	MAXSS	Return Maximum Scalar Single-Precision Floating-Point Value	MAXSS.html
GENERAL	MFENCE	Memory Fence	MFENCE.html
GENERAL	VMINPD	Minimum of Packed Double-Precision Floating-Point Values	MINPD.html
GENERAL	MINPD	Minimum of Packed Double-Precision Floating-Point Values	MINPD.html
GENERAL	MINPS	Minimum of Packed Single-Precision Floating-Point Values	MINPS.html
GENERAL	VMINPS	Minimum of Packed Single-Precision Floating-Point Values	MINPS.html
GENERAL	MINSD	Return Minimum Scalar Double-Precision Floating-Point Value	MINSD.html
GENERAL	VMINSD	Return Minimum Scalar Double-Precision Floating-Point Value	MINSD.html
GENERAL	VMINSS	Return Minimum Scalar Single-Precision Floating-Point Value	MINSS.html
GENERAL	MINSS	Return Minimum Scalar Single-Precision Floating-Point Value	MINSS.html
GENERAL	MONITOR	Set Up Monitor Address	MONITOR.html
GENERAL	MOV	Move to/from Debug Registers	MOV.html
GENERAL	MOVAPD	Move Aligned Packed Double-Precision Floating-Point Values	MOVAPD.html
GENERAL	VMOVAPD	Move Aligned Packed Double-Precision Floating-Point Values	MOVAPD.html
GENERAL	MOVAPS	Move Aligned Packed Single-Precision Floating-Point Values	MOVAPS.html
GENERAL	VMOVAPS	Move Aligned Packed Single-Precision Floating-Point Values	MOVAPS.html
GENERAL	MOVBE	Move Data After Swapping Bytes	MOVBE.html
GENERAL	MOVDDUP	Replicate Double FP Values	MOVDDUP.html
GENERAL	VMOVDDUP	Replicate Double FP Values	MOVDDUP.html
GENERAL	MOVDQ2Q	Move Quadword from XMM to MMX Technology Register	MOVDQ2Q.html
GENERAL	MOVDQA	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64.html
GENERAL	VMOVDQA	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64.html
GENERAL	VMOVDQA64	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64.html
GENERAL	VMOVDQA32	Move Aligned Packed Integer Values	MOVDQA,VMOVDQA32_64.html
GENERAL	VMOVDQU32	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	VMOVDQU8	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	VMOVDQU64	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	MOVDQU	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	VMOVDQU	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	VMOVDQU16	Move Unaligned Packed Integer Values	MOVDQU,VMOVDQU8_16_32_64.html
GENERAL	VMOVQ	Move Doubleword/Move Quadword	MOVD_MOVQ.html
GENERAL	MOVD	Move Doubleword/Move Quadword	MOVD_MOVQ.html
GENERAL	VMOVD	Move Doubleword/Move Quadword	MOVD_MOVQ.html
GENERAL	MOVQ	Move Doubleword/Move Quadword	MOVD_MOVQ.html
GENERAL	VMOVHLPS	Move Packed Single-Precision Floating-Point Values High to Low	MOVHLPS.html
GENERAL	MOVHLPS	Move Packed Single-Precision Floating-Point Values High to Low	MOVHLPS.html
GENERAL	VMOVHPD	Move High Packed Double-Precision Floating-Point Value	MOVHPD.html
GENERAL	MOVHPD	Move High Packed Double-Precision Floating-Point Value	MOVHPD.html
GENERAL	MOVHPS	Move High Packed Single-Precision Floating-Point Values	MOVHPS.html
GENERAL	VMOVHPS	Move High Packed Single-Precision Floating-Point Values	MOVHPS.html
GENERAL	MOVLHPS	Move Packed Single-Precision Floating-Point Values Low to High	MOVLHPS.html
GENERAL	VMOVLHPS	Move Packed Single-Precision Floating-Point Values Low to High	MOVLHPS.html
GENERAL	MOVLPD	Move Low Packed Double-Precision Floating-Point Value	MOVLPD.html
GENERAL	VMOVLPD	Move Low Packed Double-Precision Floating-Point Value	MOVLPD.html
GENERAL	MOVLPS	Move Low Packed Single-Precision Floating-Point Values	MOVLPS.html
GENERAL	VMOVLPS	Move Low Packed Single-Precision Floating-Point Values	MOVLPS.html
GENERAL	VMOVMSKPD	Extract Packed Double-Precision Floating-Point Sign Mask	MOVMSKPD.html
GENERAL	MOVMSKPD	Extract Packed Double-Precision Floating-Point Sign Mask	MOVMSKPD.html
GENERAL	VMOVMSKPS	Extract Packed Single-Precision Floating-Point Sign Mask	MOVMSKPS.html
GENERAL	MOVMSKPS	Extract Packed Single-Precision Floating-Point Sign Mask	MOVMSKPS.html
GENERAL	VMOVNTDQ	Store Packed Integers Using Non-Temporal Hint	MOVNTDQ.html
GENERAL	MOVNTDQ	Store Packed Integers Using Non-Temporal Hint	MOVNTDQ.html
GENERAL	MOVNTDQA	Load Double Quadword Non-Temporal Aligned Hint	MOVNTDQA.html
GENERAL	VMOVNTDQA	Load Double Quadword Non-Temporal Aligned Hint	MOVNTDQA.html
GENERAL	MOVNTI	Store Doubleword Using Non-Temporal Hint	MOVNTI.html
GENERAL	MOVNTPD	Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPD.html
GENERAL	VMOVNTPD	Store Packed Double-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPD.html
GENERAL	MOVNTPS	Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPS.html
GENERAL	VMOVNTPS	Store Packed Single-Precision Floating-Point Values Using Non-Temporal Hint	MOVNTPS.html
GENERAL	MOVNTQ	Store of Quadword Using Non-Temporal Hint	MOVNTQ.html
GENERAL	VMOVQ	Move Quadword	MOVQ.html
GENERAL	MOVQ	Move Quadword	MOVQ.html
GENERAL	MOVQ2DQ	Move Quadword from MMX Technology to XMM Register	MOVQ2DQ.html
GENERAL	MOVSD	Move or Merge Scalar Double-Precision Floating-Point Value	MOVSD.html
GENERAL	VMOVSD	Move or Merge Scalar Double-Precision Floating-Point Value	MOVSD.html
GENERAL	VMOVSHDUP	Replicate Single FP Values	MOVSHDUP.html
GENERAL	MOVSHDUP	Replicate Single FP Values	MOVSHDUP.html
GENERAL	VMOVSLDUP	Replicate Single FP Values	MOVSLDUP.html
GENERAL	MOVSLDUP	Replicate Single FP Values	MOVSLDUP.html
GENERAL	MOVSS	Move or Merge Scalar Single-Precision Floating-Point Value	MOVSS.html
GENERAL	VMOVSS	Move or Merge Scalar Single-Precision Floating-Point Value	MOVSS.html
GENERAL	MOVSX	Move with Sign-Extension	MOVSX_MOVSXD.html
GENERAL	MOVSXD	Move with Sign-Extension	MOVSX_MOVSXD.html
GENERAL	MOVSQ	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ.html
GENERAL	MOVSB	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ.html
GENERAL	MOVSD	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ.html
GENERAL	MOVSW	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ.html
GENERAL	MOVS	Move Data from String to String	MOVS_MOVSB_MOVSW_MOVSD_MOVSQ.html
GENERAL	MOVUPD	Move Unaligned Packed Double-Precision Floating-Point Values	MOVUPD.html
GENERAL	VMOVUPD	Move Unaligned Packed Double-Precision Floating-Point Values	MOVUPD.html
GENERAL	MOVUPS	Move Unaligned Packed Single-Precision Floating-Point Values	MOVUPS.html
GENERAL	VMOVUPS	Move Unaligned Packed Single-Precision Floating-Point Values	MOVUPS.html
GENERAL	MOVZX	Move with Zero-Extend	MOVZX.html
GENERAL	VMPSADBW	Compute Multiple Packed Sums of Absolute Difference	MPSADBW.html
GENERAL	MPSADBW	Compute Multiple Packed Sums of Absolute Difference	MPSADBW.html
GENERAL	MUL	Unsigned Multiply	MUL.html
GENERAL	VMULPD	Multiply Packed Double-Precision Floating-Point Values	MULPD.html
GENERAL	MULPD	Multiply Packed Double-Precision Floating-Point Values	MULPD.html
GENERAL	MULPS	Multiply Packed Single-Precision Floating-Point Values	MULPS.html
GENERAL	VMULPS	Multiply Packed Single-Precision Floating-Point Values	MULPS.html
GENERAL	MULSD	Multiply Scalar Double-Precision Floating-Point Value	MULSD.html
GENERAL	VMULSD	Multiply Scalar Double-Precision Floating-Point Value	MULSD.html
GENERAL	MULSS	Multiply Scalar Single-Precision Floating-Point Values	MULSS.html
GENERAL	VMULSS	Multiply Scalar Single-Precision Floating-Point Values	MULSS.html
GENERAL	MULX	Unsigned Multiply Without Affecting Flags	MULX.html
GENERAL	MWAIT	Monitor Wait	MWAIT.html
GENERAL	NEG	Two's Complement Negation	NEG.html
GENERAL	NOP	No Operation	NOP.html
GENERAL	NOT	One's Complement Negation	NOT.html
GENERAL	OR	Logical Inclusive OR	OR.html
GENERAL	ORPD	Bitwise Logical OR of Packed Double Precision Floating-Point Values	ORPD.html
GENERAL	VORPD	Bitwise Logical OR of Packed Double Precision Floating-Point Values	ORPD.html
GENERAL	ORPS	Bitwise Logical OR of Packed Single Precision Floating-Point Values	ORPS.html
GENERAL	VORPS	Bitwise Logical OR of Packed Single Precision Floating-Point Values	ORPS.html
GENERAL	OUT	Output to Port	OUT.html
GENERAL	OUTS	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD.html
GENERAL	OUTSW	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD.html
GENERAL	OUTSB	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD.html
GENERAL	OUTSD	Output String to Port	OUTS_OUTSB_OUTSW_OUTSD.html
GENERAL	PABSW	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	PABSB	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	PABSD	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	VPABSW	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	PABSQ	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	VPABSD	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	VPABSB	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	VPABSQ	Packed Absolute Value	PABSB_PABSW_PABSD_PABSQ.html
GENERAL	PACKSSDW	Pack with Signed Saturation	PACKSSWB_PACKSSDW.html
GENERAL	VPACKSSWB	Pack with Signed Saturation	PACKSSWB_PACKSSDW.html
GENERAL	PACKSSWB	Pack with Signed Saturation	PACKSSWB_PACKSSDW.html
GENERAL	VPACKSSDW	Pack with Signed Saturation	PACKSSWB_PACKSSDW.html
GENERAL	PACKUSDW	Pack with Unsigned Saturation	PACKUSDW.html
GENERAL	VPACKUSDW	Pack with Unsigned Saturation	PACKUSDW.html
GENERAL	PACKUSWB	Pack with Unsigned Saturation	PACKUSWB.html
GENERAL	VPACKUSWB	Pack with Unsigned Saturation	PACKUSWB.html
GENERAL	PADDW	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	PADDB	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	PADDQ	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	PADDD	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	VPADDW	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	VPADDD	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	VPADDQ	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	VPADDB	Add Packed Integers	PADDB_PADDW_PADDD_PADDQ.html
GENERAL	PADDSW	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW.html
GENERAL	VPADDSW	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW.html
GENERAL	PADDSB	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW.html
GENERAL	VPADDSB	Add Packed Signed Integers with Signed Saturation	PADDSB_PADDSW.html
GENERAL	PADDUSW	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW.html
GENERAL	VPADDUSW	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW.html
GENERAL	PADDUSB	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW.html
GENERAL	VPADDUSB	Add Packed Unsigned Integers with Unsigned Saturation	PADDUSB_PADDUSW.html
GENERAL	PALIGNR	Packed Align Right	PALIGNR.html
GENERAL	VPALIGNR	Packed Align Right	PALIGNR.html
GENERAL	VPANDD	Logical AND	PAND.html
GENERAL	PAND	Logical AND	PAND.html
GENERAL	VPANDQ	Logical AND	PAND.html
GENERAL	VPAND	Logical AND	PAND.html
GENERAL	PANDN	Logical AND NOT	PANDN.html
GENERAL	VPANDNQ	Logical AND NOT	PANDN.html
GENERAL	VPANDN	Logical AND NOT	PANDN.html
GENERAL	VPANDND	Logical AND NOT	PANDN.html
GENERAL	PAUSE	Spin Loop Hint	PAUSE.html
GENERAL	PAVGW	Average Packed Integers	PAVGB_PAVGW.html
GENERAL	PAVGB	Average Packed Integers	PAVGB_PAVGW.html
GENERAL	VPAVGW	Average Packed Integers	PAVGB_PAVGW.html
GENERAL	VPAVGB	Average Packed Integers	PAVGB_PAVGW.html
GENERAL	VPBLENDVB	Variable Blend Packed Bytes	PBLENDVB.html
GENERAL	PBLENDVB	Variable Blend Packed Bytes	PBLENDVB.html
GENERAL	VPBLENDW	Blend Packed Words	PBLENDW.html
GENERAL	PBLENDW	Blend Packed Words	PBLENDW.html
GENERAL	VPCLMULQDQ	Carry-Less Multiplication Quadword 	PCLMULQDQ.html
GENERAL	PCLMULQDQ	Carry-Less Multiplication Quadword 	PCLMULQDQ.html
GENERAL	VPCMPEQW	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	VPCMPEQD	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	VPCMPEQB	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	PCMPEQB	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	PCMPEQD	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	PCMPEQW	Compare Packed Data for Equal	PCMPEQB_PCMPEQW_PCMPEQD.html
GENERAL	PCMPEQQ	Compare Packed Qword Data for Equal	PCMPEQQ.html
GENERAL	VPCMPEQQ	Compare Packed Qword Data for Equal	PCMPEQQ.html
GENERAL	VPCMPESTRI	Packed Compare Explicit Length Strings, Return Index	PCMPESTRI.html
GENERAL	PCMPESTRI	Packed Compare Explicit Length Strings, Return Index	PCMPESTRI.html
GENERAL	VPCMPESTRM	Packed Compare Explicit Length Strings, Return Mask	PCMPESTRM.html
GENERAL	PCMPESTRM	Packed Compare Explicit Length Strings, Return Mask	PCMPESTRM.html
GENERAL	VPCMPGTW	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	VPCMPGTD	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	PCMPGTB	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	VPCMPGTB	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	PCMPGTD	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	PCMPGTW	Compare Packed Signed Integers for Greater Than	PCMPGTB_PCMPGTW_PCMPGTD.html
GENERAL	VPCMPGTQ	Compare Packed Data for Greater Than	PCMPGTQ.html
GENERAL	PCMPGTQ	Compare Packed Data for Greater Than	PCMPGTQ.html
GENERAL	VPCMPISTRI	Packed Compare Implicit Length Strings, Return Index	PCMPISTRI.html
GENERAL	PCMPISTRI	Packed Compare Implicit Length Strings, Return Index	PCMPISTRI.html
GENERAL	VPCMPISTRM	Packed Compare Implicit Length Strings, Return Mask	PCMPISTRM.html
GENERAL	PCMPISTRM	Packed Compare Implicit Length Strings, Return Mask	PCMPISTRM.html
GENERAL	PDEP	Parallel Bits Deposit	PDEP.html
GENERAL	PEXT	Parallel Bits Extract	PEXT.html
GENERAL	VPEXTRB	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	VPEXTRQ	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	PEXTRB	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	VPEXTRD	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	PEXTRQ	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	PEXTRD	Extract Byte/Dword/Qword	PEXTRB_PEXTRD_PEXTRQ.html
GENERAL	VPEXTRW	Extract Word	PEXTRW.html
GENERAL	PEXTRW	Extract Word	PEXTRW.html
GENERAL	PHADDSW	Packed Horizontal Add and Saturate	PHADDSW.html
GENERAL	VPHADDSW	Packed Horizontal Add and Saturate	PHADDSW.html
GENERAL	VPHADDD	Packed Horizontal Add	PHADDW_PHADDD.html
GENERAL	PHADDD	Packed Horizontal Add	PHADDW_PHADDD.html
GENERAL	VPHADDW	Packed Horizontal Add	PHADDW_PHADDD.html
GENERAL	PHADDW	Packed Horizontal Add	PHADDW_PHADDD.html
GENERAL	PHMINPOSUW	Packed Horizontal Word Minimum	PHMINPOSUW.html
GENERAL	VPHMINPOSUW	Packed Horizontal Word Minimum	PHMINPOSUW.html
GENERAL	PHSUBSW	Packed Horizontal Subtract and Saturate	PHSUBSW.html
GENERAL	VPHSUBSW	Packed Horizontal Subtract and Saturate	PHSUBSW.html
GENERAL	VPHSUBD	Packed Horizontal Subtract	PHSUBW_PHSUBD.html
GENERAL	VPHSUBW	Packed Horizontal Subtract	PHSUBW_PHSUBD.html
GENERAL	PHSUBD	Packed Horizontal Subtract	PHSUBW_PHSUBD.html
GENERAL	PHSUBW	Packed Horizontal Subtract	PHSUBW_PHSUBD.html
GENERAL	PINSRQ	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	VPINSRB	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	PINSRB	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	VPINSRD	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	VPINSRQ	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	PINSRD	Insert Byte/Dword/Qword	PINSRB_PINSRD_PINSRQ.html
GENERAL	VPINSRW	Insert Word	PINSRW.html
GENERAL	PINSRW	Insert Word	PINSRW.html
GENERAL	VPMADDUBSW	Multiply and Add Packed Signed and Unsigned Bytes	PMADDUBSW.html
GENERAL	PMADDUBSW	Multiply and Add Packed Signed and Unsigned Bytes	PMADDUBSW.html
GENERAL	PMADDWD	Multiply and Add Packed Integers	PMADDWD.html
GENERAL	VPMADDWD	Multiply and Add Packed Integers	PMADDWD.html
GENERAL	PMAXSW	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	VPMAXSW	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	PMAXSD	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	PMAXSB	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	VPMAXSD	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	PMAXSQ	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	VPMAXSB	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	VPMAXSQ	Maximum of Packed Signed Integers	PMAXSB_PMAXSW_PMAXSD_PMAXSQ.html
GENERAL	VPMAXUB	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW.html
GENERAL	PMAXUW	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW.html
GENERAL	VPMAXUW	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW.html
GENERAL	PMAXUB	Maximum of Packed Unsigned Integers	PMAXUB_PMAXUW.html
GENERAL	VPMAXUQ	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ.html
GENERAL	PMAXUD	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ.html
GENERAL	VPMAXUD	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ.html
GENERAL	PMAXUQ	Maximum of Packed Unsigned Integers	PMAXUD_PMAXUQ.html
GENERAL	PMINSB	Minimum of Packed Signed Integers	PMINSB_PMINSW.html
GENERAL	PMINSW	Minimum of Packed Signed Integers	PMINSB_PMINSW.html
GENERAL	VPMINSW	Minimum of Packed Signed Integers	PMINSB_PMINSW.html
GENERAL	VPMINSB	Minimum of Packed Signed Integers	PMINSB_PMINSW.html
GENERAL	PMINSD	Minimum of Packed Signed Integers	PMINSD_PMINSQ.html
GENERAL	VPMINSD	Minimum of Packed Signed Integers	PMINSD_PMINSQ.html
GENERAL	PMINSQ	Minimum of Packed Signed Integers	PMINSD_PMINSQ.html
GENERAL	VPMINSQ	Minimum of Packed Signed Integers	PMINSD_PMINSQ.html
GENERAL	VPMINUW	Minimum of Packed Unsigned Integers	PMINUB_PMINUW.html
GENERAL	PMINUW	Minimum of Packed Unsigned Integers	PMINUB_PMINUW.html
GENERAL	PMINUB	Minimum of Packed Unsigned Integers	PMINUB_PMINUW.html
GENERAL	VPMINUB	Minimum of Packed Unsigned Integers	PMINUB_PMINUW.html
GENERAL	PMINUD	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ.html
GENERAL	VPMINUD	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ.html
GENERAL	PMINUQ	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ.html
GENERAL	VPMINUQ	Minimum of Packed Unsigned Integers	PMINUD_PMINUQ.html
GENERAL	PMOVMSKB	Move Byte Mask	PMOVMSKB.html
GENERAL	VPMOVMSKB	Move Byte Mask	PMOVMSKB.html
GENERAL	VPMOVSXWD	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVSXDQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXBW	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVSXBQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXBD	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVSXWQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXBQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVSXBW	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXWD	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXDQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	PMOVSXWQ	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVSXBD	Packed Move with Sign Extend	PMOVSX.html
GENERAL	VPMOVZXWQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXBQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXBW	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMOVZXWD	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMOVZXDQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXBD	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMOVZXBQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMOVZXBD	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXWD	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXDQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMOVZXBW	Packed Move with Zero Extend	PMOVZX.html
GENERAL	PMOVZXWQ	Packed Move with Zero Extend	PMOVZX.html
GENERAL	VPMULDQ	Multiply Packed Doubleword Integers	PMULDQ.html
GENERAL	PMULDQ	Multiply Packed Doubleword Integers	PMULDQ.html
GENERAL	VPMULHRSW	Packed Multiply High with Round and Scale	PMULHRSW.html
GENERAL	PMULHRSW	Packed Multiply High with Round and Scale	PMULHRSW.html
GENERAL	VPMULHUW	Multiply Packed Unsigned Integers and Store High Result	PMULHUW.html
GENERAL	PMULHUW	Multiply Packed Unsigned Integers and Store High Result	PMULHUW.html
GENERAL	VPMULHW	Multiply Packed Signed Integers and Store High Result	PMULHW.html
GENERAL	PMULHW	Multiply Packed Signed Integers and Store High Result	PMULHW.html
GENERAL	PMULLQ	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ.html
GENERAL	VPMULLD	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ.html
GENERAL	PMULLD	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ.html
GENERAL	VPMULLQ	Multiply Packed Integers and Store Low Result	PMULLD_PMULLQ.html
GENERAL	VPMULLW	Multiply Packed Signed Integers and Store Low Result	PMULLW.html
GENERAL	PMULLW	Multiply Packed Signed Integers and Store Low Result	PMULLW.html
GENERAL	VPMULUDQ	Multiply Packed Unsigned Doubleword Integers	PMULUDQ.html
GENERAL	PMULUDQ	Multiply Packed Unsigned Doubleword Integers	PMULUDQ.html
GENERAL	POP	Pop a Value from the Stack	POP.html
GENERAL	POPA	Pop All General-Purpose Registers	POPA_POPAD.html
GENERAL	POPAD	Pop All General-Purpose Registers	POPA_POPAD.html
GENERAL	POPCNT	Return the Count of Number of Bits Set to 1	POPCNT.html
GENERAL	POPF	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ.html
GENERAL	POPFQ	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ.html
GENERAL	POPFD	Pop Stack into EFLAGS Register	POPF_POPFD_POPFQ.html
GENERAL	POR	Bitwise Logical OR	POR.html
GENERAL	VPORQ	Bitwise Logical OR	POR.html
GENERAL	VPOR	Bitwise Logical OR	POR.html
GENERAL	VPORD	Bitwise Logical OR	POR.html
GENERAL	PREFETCHT0	Prefetch Data Into Caches	PREFETCHh.html
GENERAL	PREFETCHT2	Prefetch Data Into Caches	PREFETCHh.html
GENERAL	PREFETCHT1	Prefetch Data Into Caches	PREFETCHh.html
GENERAL	PREFETCHNTA	Prefetch Data Into Caches	PREFETCHh.html
GENERAL	PREFETCHW	Prefetch Data into Caches in Anticipation of a Write	PREFETCHW.html
GENERAL	PREFETCHWT1	Prefetch Vector Data Into Caches with Intent to Write and T1 Hint	PREFETCHWT1.html
GENERAL	PROLVD	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	VPROLD	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	VPROLQ	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	VPROLVQ	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	PROLVQ	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	VPROLVD	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	PROLD	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	PROLQ	Bit Rotate Left	PROLD_PROLVD_PROLQ_PROLVQ.html
GENERAL	PRORQ	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	PRORD	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	VPRORVQ	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	VPRORD	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	VPRORQ	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	PRORVQ	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	VPRORVD	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	PRORVD	Bit Rotate  Right	PRORD_PRORVD_PRORQ_PRORVQ.html
GENERAL	PSADBW	Compute Sum of Absolute Differences	PSADBW.html
GENERAL	VPSADBW	Compute Sum of Absolute Differences	PSADBW.html
GENERAL	PSHUFB	Packed Shuffle Bytes	PSHUFB.html
GENERAL	VPSHUFB	Packed Shuffle Bytes	PSHUFB.html
GENERAL	PSHUFD	Shuffle Packed Doublewords	PSHUFD.html
GENERAL	VPSHUFD	Shuffle Packed Doublewords	PSHUFD.html
GENERAL	PSHUFHW	Shuffle Packed High Words	PSHUFHW.html
GENERAL	VPSHUFHW	Shuffle Packed High Words	PSHUFHW.html
GENERAL	PSHUFLW	Shuffle Packed Low Words	PSHUFLW.html
GENERAL	VPSHUFLW	Shuffle Packed Low Words	PSHUFLW.html
GENERAL	PSHUFW	Shuffle Packed Words	PSHUFW.html
GENERAL	PSIGND	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	PSIGNB	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	VPSIGND	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	VPSIGNB	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	PSIGNW	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	VPSIGNW	Packed SIGN	PSIGNB_PSIGNW_PSIGND.html
GENERAL	VPSLLDQ	Shift Double Quadword Left Logical	PSLLDQ.html
GENERAL	PSLLDQ	Shift Double Quadword Left Logical	PSLLDQ.html
GENERAL	PSLLQ	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	VPSLLW	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	VPSLLD	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	VPSLLQ	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	PSLLW	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	PSLLD	Shift Packed Data Left Logical	PSLLW_PSLLD_PSLLQ.html
GENERAL	VPSRAW	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	PSRAQ	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	VPSRAD	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	VPSRAQ	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	PSRAW	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	PSRAD	Shift Packed Data Right Arithmetic	PSRAW_PSRAD_PSRAQ.html
GENERAL	PSRLDQ	Shift Double Quadword Right Logical	PSRLDQ.html
GENERAL	VPSRLDQ	Shift Double Quadword Right Logical	PSRLDQ.html
GENERAL	VPSRLQ	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	PSRLW	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	PSRLD	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	PSRLQ	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	VPSRLW	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	VPSRLD	Shift Packed Data Right Logical	PSRLW_PSRLD_PSRLQ.html
GENERAL	VPSUBD	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	VPSUBB	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	PSUBW	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	PSUBD	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	PSUBB	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	VPSUBW	Subtract Packed Integers	PSUBB_PSUBW_PSUBD.html
GENERAL	VPSUBQ	Subtract Packed Quadword Integers	PSUBQ.html
GENERAL	PSUBQ	Subtract Packed Quadword Integers	PSUBQ.html
GENERAL	VPSUBSB	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW.html
GENERAL	PSUBSB	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW.html
GENERAL	PSUBSW	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW.html
GENERAL	VPSUBSW	Subtract Packed Signed Integers with Signed Saturation	PSUBSB_PSUBSW.html
GENERAL	PSUBUSB	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW.html
GENERAL	VPSUBUSW	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW.html
GENERAL	PSUBUSW	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW.html
GENERAL	VPSUBUSB	Subtract Packed Unsigned Integers with Unsigned Saturation	PSUBUSB_PSUBUSW.html
GENERAL	VPTEST	PTEST- Logical Compare 	PTEST.html
GENERAL	PTEST	PTEST- Logical Compare 	PTEST.html
GENERAL	PTWRITE	PTWRITE - Write Data to a Processor Trace Packet 	PTWRITE.html
GENERAL	PUNPCKHQDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	PUNPCKHWD	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	PUNPCKHDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	VPUNPCKHBW	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	PUNPCKHBW	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	VPUNPCKHQDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	VPUNPCKHWD	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	VPUNPCKHDQ	Unpack High Data	PUNPCKHBW_PUNPCKHWD_PUNPCKHDQ_PUNPCKHQDQ.html
GENERAL	VPUNPCKLQDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	PUNPCKLBW	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	PUNPCKLQDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	VPUNPCKLBW	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	PUNPCKLWD	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	PUNPCKLDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	VPUNPCKLWD	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	VPUNPCKLDQ	Unpack Low Data	PUNPCKLBW_PUNPCKLWD_PUNPCKLDQ_PUNPCKLQDQ.html
GENERAL	PUSH	Push Word, Doubleword or Quadword Onto the Stack	PUSH.html
GENERAL	PUSHAD	Push All General-Purpose Registers	PUSHA_PUSHAD.html
GENERAL	PUSHA	Push All General-Purpose Registers	PUSHA_PUSHAD.html
GENERAL	PUSHFD	Push EFLAGS Register onto the Stack	PUSHF_PUSHFD.html
GENERAL	PUSHF	Push EFLAGS Register onto the Stack	PUSHF_PUSHFD.html
GENERAL	VPXOR	Logical Exclusive OR	PXOR.html
GENERAL	VPXORQ	Logical Exclusive OR	PXOR.html
GENERAL	VPXORD	Logical Exclusive OR	PXOR.html
GENERAL	PXOR	Logical Exclusive OR	PXOR.html
GENERAL	RCR	Rotate	RCL_RCR_ROL_ROR.html
GENERAL	ROR	Rotate	RCL_RCR_ROL_ROR.html
GENERAL	RCL	Rotate	RCL_RCR_ROL_ROR.html
GENERAL	ROL	Rotate	RCL_RCR_ROL_ROR.html
GENERAL	RCPPS	Compute Reciprocals of Packed Single-Precision Floating-Point Values	RCPPS.html
GENERAL	VRCPPS	Compute Reciprocals of Packed Single-Precision Floating-Point Values	RCPPS.html
GENERAL	VRCPSS	Compute Reciprocal of Scalar Single-Precision Floating-Point Values	RCPSS.html
GENERAL	RCPSS	Compute Reciprocal of Scalar Single-Precision Floating-Point Values	RCPSS.html
GENERAL	RDFSBASE	Read FS/GS Segment Base	RDFSBASE_RDGSBASE.html
GENERAL	RDGSBASE	Read FS/GS Segment Base	RDFSBASE_RDGSBASE.html
GENERAL	RDMSR	Read from Model Specific Register	RDMSR.html
GENERAL	RDPID	Read Processor ID	RDPID.html
GENERAL	RDPKRU	Read Protection Key Rights for User Pages	RDPKRU.html
GENERAL	RDPMC	Read Performance-Monitoring Counters	RDPMC.html
GENERAL	RDRAND	Read Random Number	RDRAND.html
GENERAL	RDSEED	Read Random SEED	RDSEED.html
GENERAL	RDTSC	Read Time-Stamp Counter	RDTSC.html
GENERAL	RDTSCP	Read Time-Stamp Counter and Processor ID	RDTSCP.html
GENERAL	REP OUTS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REP STOS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REPNE CMPS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REPE CMPS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REPNE SCAS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REP MOVS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REP LODS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REPE SCAS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	REP INS	Repeat String Operation Prefix	REP_REPE_REPZ_REPNE_REPNZ.html
GENERAL	RET	Return from Procedure	RET.html
GENERAL	RORX	Rotate Right Logical Without Affecting Flags	RORX.html
GENERAL	ROUNDPD	Round Packed Double Precision Floating-Point Values	ROUNDPD.html
GENERAL	VROUNDPD	Round Packed Double Precision Floating-Point Values	ROUNDPD.html
GENERAL	ROUNDPS	Round Packed Single Precision Floating-Point Values	ROUNDPS.html
GENERAL	VROUNDPS	Round Packed Single Precision Floating-Point Values	ROUNDPS.html
GENERAL	VROUNDSD	Round Scalar Double Precision Floating-Point Values	ROUNDSD.html
GENERAL	ROUNDSD	Round Scalar Double Precision Floating-Point Values	ROUNDSD.html
GENERAL	VROUNDSS	Round Scalar Single Precision Floating-Point Values	ROUNDSS.html
GENERAL	ROUNDSS	Round Scalar Single Precision Floating-Point Values	ROUNDSS.html
GENERAL	RSM	Resume from System Management Mode	RSM.html
GENERAL	RSQRTPS	Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values	RSQRTPS.html
GENERAL	VRSQRTPS	Compute Reciprocals of Square Roots of Packed Single-Precision Floating-Point Values	RSQRTPS.html
GENERAL	VRSQRTSS	Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value	RSQRTSS.html
GENERAL	RSQRTSS	Compute Reciprocal of Square Root of Scalar Single-Precision Floating-Point Value	RSQRTSS.html
GENERAL	SAHF	Store AH into Flags	SAHF.html
GENERAL	SAR	Shift	SAL_SAR_SHL_SHR.html
GENERAL	SHL	Shift	SAL_SAR_SHL_SHR.html
GENERAL	SHR	Shift	SAL_SAR_SHL_SHR.html
GENERAL	SAL	Shift	SAL_SAR_SHL_SHR.html
GENERAL	SARX	Shift Without Affecting Flags	SARX_SHLX_SHRX.html
GENERAL	SHLX	Shift Without Affecting Flags	SARX_SHLX_SHRX.html
GENERAL	SHRX	Shift Without Affecting Flags	SARX_SHLX_SHRX.html
GENERAL	SBB	Integer Subtraction with Borrow	SBB.html
GENERAL	SCASB	Scan String	SCAS_SCASB_SCASW_SCASD.html
GENERAL	SCASD	Scan String	SCAS_SCASB_SCASW_SCASD.html
GENERAL	SCAS	Scan String	SCAS_SCASB_SCASW_SCASD.html
GENERAL	SCASW	Scan String	SCAS_SCASB_SCASW_SCASD.html
GENERAL	SETNB	Set Byte on Condition	SETcc.html
GENERAL	SETNC	Set Byte on Condition	SETcc.html
GENERAL	SETPE	Set Byte on Condition	SETcc.html
GENERAL	SETNA	Set Byte on Condition	SETcc.html
GENERAL	SETLE	Set Byte on Condition	SETcc.html
GENERAL	SETNG	Set Byte on Condition	SETcc.html
GENERAL	SETNE	Set Byte on Condition	SETcc.html
GENERAL	SETGE	Set Byte on Condition	SETcc.html
GENERAL	SETAE	Set Byte on Condition	SETcc.html
GENERAL	SETA	Set Byte on Condition	SETcc.html
GENERAL	SETNBE	Set Byte on Condition	SETcc.html
GENERAL	SETG	Set Byte on Condition	SETcc.html
GENERAL	SETE	Set Byte on Condition	SETcc.html
GENERAL	SETNAE	Set Byte on Condition	SETcc.html
GENERAL	SETB	Set Byte on Condition	SETcc.html
GENERAL	SETC	Set Byte on Condition	SETcc.html
GENERAL	SETP	Set Byte on Condition	SETcc.html
GENERAL	SETO	Set Byte on Condition	SETcc.html
GENERAL	SETL	Set Byte on Condition	SETcc.html
GENERAL	SETNO	Set Byte on Condition	SETcc.html
GENERAL	SETNL	Set Byte on Condition	SETcc.html
GENERAL	SETPO	Set Byte on Condition	SETcc.html
GENERAL	SETNS	Set Byte on Condition	SETcc.html
GENERAL	SETNP	Set Byte on Condition	SETcc.html
GENERAL	SETBE	Set Byte on Condition	SETcc.html
GENERAL	SETS	Set Byte on Condition	SETcc.html
GENERAL	SETNZ	Set Byte on Condition	SETcc.html
GENERAL	SETZ	Set Byte on Condition	SETcc.html
GENERAL	SETNGE	Set Byte on Condition	SETcc.html
GENERAL	SFENCE	Store Fence	SFENCE.html
GENERAL	SGDT	Store Global Descriptor Table Register	SGDT.html
GENERAL	SHA1MSG1	Perform an Intermediate Calculation for the Next Four SHA1 Message Dwords	SHA1MSG1.html
GENERAL	SHA1MSG2	Perform a Final Calculation for the Next Four SHA1 Message Dwords	SHA1MSG2.html
GENERAL	SHA1NEXTE	Calculate SHA1 State Variable E after Four Rounds	SHA1NEXTE.html
GENERAL	SHA1RNDS4	Perform Four Rounds of SHA1 Operation	SHA1RNDS4.html
GENERAL	SHA256MSG1	Perform an Intermediate Calculation for the Next Four SHA256 Message Dwords	SHA256MSG1.html
GENERAL	SHA256MSG2	Perform a Final Calculation for the Next Four SHA256 Message Dwords	SHA256MSG2.html
GENERAL	SHA256RNDS2	Perform Two Rounds of SHA256 Operation	SHA256RNDS2.html
GENERAL	SHLD	Double Precision Shift Left	SHLD.html
GENERAL	SHRD	Double Precision Shift Right	SHRD.html
GENERAL	VSHUFPD	Packed Interleave Shuffle of Pairs of Double-Precision Floating-Point Values	SHUFPD.html
GENERAL	SHUFPD	Packed Interleave Shuffle of Pairs of Double-Precision Floating-Point Values	SHUFPD.html
GENERAL	VSHUFPS	Packed Interleave Shuffle of Quadruplets of Single-Precision Floating-Point Values	SHUFPS.html
GENERAL	SHUFPS	Packed Interleave Shuffle of Quadruplets of Single-Precision Floating-Point Values	SHUFPS.html
GENERAL	SIDT	Store Interrupt Descriptor Table Register	SIDT.html
GENERAL	SLDT	Store Local Descriptor Table Register	SLDT.html
GENERAL	SMSW	Store Machine Status Word	SMSW.html
GENERAL	SQRTPD	Square Root of Double-Precision Floating-Point Values	SQRTPD.html
GENERAL	VSQRTPD	Square Root of Double-Precision Floating-Point Values	SQRTPD.html
GENERAL	SQRTPS	Square Root of Single-Precision Floating-Point Values	SQRTPS.html
GENERAL	VSQRTPS	Square Root of Single-Precision Floating-Point Values	SQRTPS.html
GENERAL	VSQRTSD	Compute Square Root of Scalar Double-Precision Floating-Point Value	SQRTSD.html
GENERAL	SQRTSD	Compute Square Root of Scalar Double-Precision Floating-Point Value	SQRTSD.html
GENERAL	VSQRTSS	Compute Square Root of Scalar Single-Precision Value	SQRTSS.html
GENERAL	SQRTSS	Compute Square Root of Scalar Single-Precision Value	SQRTSS.html
GENERAL	STAC	Set AC Flag in EFLAGS Register	STAC.html
GENERAL	STC	Set Carry Flag	STC.html
GENERAL	STD	Set Direction Flag	STD.html
GENERAL	STI	Set Interrupt Flag	STI.html
GENERAL	STMXCSR	Store MXCSR Register State	STMXCSR.html
GENERAL	VSTMXCSR	Store MXCSR Register State	STMXCSR.html
GENERAL	STOS	Store String	STOS_STOSB_STOSW_STOSD_STOSQ.html
GENERAL	STOSB	Store String	STOS_STOSB_STOSW_STOSD_STOSQ.html
GENERAL	STOSQ	Store String	STOS_STOSB_STOSW_STOSD_STOSQ.html
GENERAL	STOSD	Store String	STOS_STOSB_STOSW_STOSD_STOSQ.html
GENERAL	STOSW	Store String	STOS_STOSB_STOSW_STOSD_STOSQ.html
GENERAL	STR	Store Task Register	STR.html
GENERAL	style		style.css
GENERAL	SUB	Subtract	SUB.html
GENERAL	VSUBPD	Subtract Packed Double-Precision Floating-Point Values	SUBPD.html
GENERAL	SUBPD	Subtract Packed Double-Precision Floating-Point Values	SUBPD.html
GENERAL	VSUBPS	Subtract Packed Single-Precision Floating-Point Values	SUBPS.html
GENERAL	SUBPS	Subtract Packed Single-Precision Floating-Point Values	SUBPS.html
GENERAL	VSUBSD	Subtract Scalar Double-Precision Floating-Point Value	SUBSD.html
GENERAL	SUBSD	Subtract Scalar Double-Precision Floating-Point Value	SUBSD.html
GENERAL	VSUBSS	Subtract Scalar Single-Precision Floating-Point Value	SUBSS.html
GENERAL	SUBSS	Subtract Scalar Single-Precision Floating-Point Value	SUBSS.html
GENERAL	SWAPGS	Swap GS Base Register	SWAPGS.html
GENERAL	SYSCALL	Fast System Call	SYSCALL.html
GENERAL	SYSENTER	Fast System Call	SYSENTER.html
GENERAL	SYSEXIT	Fast Return from Fast System Call	SYSEXIT.html
GENERAL	SYSRET	Return From Fast System Call	SYSRET.html
GENERAL	TEST	Logical Compare	TEST.html
GENERAL	TZCNT	Count the Number of Trailing Zero Bits	TZCNT.html
GENERAL	VUCOMISD	Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS	UCOMISD.html
GENERAL	UCOMISD	Unordered Compare Scalar Double-Precision Floating-Point Values and Set EFLAGS	UCOMISD.html
GENERAL	UCOMISS	Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS	UCOMISS.html
GENERAL	VUCOMISS	Unordered Compare Scalar Single-Precision Floating-Point Values and Set EFLAGS	UCOMISS.html
GENERAL	UD2	Undefined Instruction	UD2.html
GENERAL	UNPCKHPD	Unpack and Interleave High Packed Double-Precision Floating-Point Values	UNPCKHPD.html
GENERAL	VUNPCKHPD	Unpack and Interleave High Packed Double-Precision Floating-Point Values	UNPCKHPD.html
GENERAL	UNPCKHPS	Unpack and Interleave High Packed Single-Precision Floating-Point Values	UNPCKHPS.html
GENERAL	VUNPCKHPS	Unpack and Interleave High Packed Single-Precision Floating-Point Values	UNPCKHPS.html
GENERAL	UNPCKLPD	Unpack and Interleave Low Packed Double-Precision Floating-Point Values	UNPCKLPD.html
GENERAL	VUNPCKLPD	Unpack and Interleave Low Packed Double-Precision Floating-Point Values	UNPCKLPD.html
GENERAL	UNPCKLPS	Unpack and Interleave Low Packed Single-Precision Floating-Point Values	UNPCKLPS.html
GENERAL	VUNPCKLPS	Unpack and Interleave Low Packed Single-Precision Floating-Point Values	UNPCKLPS.html
GENERAL	VALIGND	Align Doubleword/Quadword Vectors	VALIGND_VALIGNQ.html
GENERAL	VALIGNQ	Align Doubleword/Quadword Vectors	VALIGND_VALIGNQ.html
GENERAL	VBLENDMPD	Blend Float64/Float32 Vectors Using an OpMask Control	VBLENDMPD_VBLENDMPS.html
GENERAL	VBLENDMPS	Blend Float64/Float32 Vectors Using an OpMask Control	VBLENDMPD_VBLENDMPS.html
GENERAL	VBROADCASTSS	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF128	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF32X2	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF32X4	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF64X4	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF64X2	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTSD	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VBROADCASTF32X8	Load with Broadcast Floating-Point Data	VBROADCAST.html
GENERAL	VCOMPRESSPD	Store Sparse Packed Double-Precision Floating-Point Values into Dense Memory	VCOMPRESSPD.html
GENERAL	VCOMPRESSPS	Store Sparse Packed Single-Precision Floating-Point Values into Dense Memory	VCOMPRESSPS.html
GENERAL	VCVTPD2QQ	Convert Packed Double-Precision Floating-Point Values to Packed Quadword Integers	VCVTPD2QQ.html
GENERAL	VCVTPD2UDQ	Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers	VCVTPD2UDQ.html
GENERAL	VCVTPD2UQQ	Convert Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers	VCVTPD2UQQ.html
GENERAL	VCVTPH2PS	Convert 16-bit FP values to Single-Precision FP values	VCVTPH2PS.html
GENERAL	VCVTPS2PH	Convert Single-Precision FP value to 16-bit FP value	VCVTPS2PH.html
GENERAL	VCVTPS2QQ	Convert Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values	VCVTPS2QQ.html
GENERAL	VCVTPS2UDQ	Convert Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values	VCVTPS2UDQ.html
GENERAL	VCVTPS2UQQ	Convert Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values	VCVTPS2UQQ.html
GENERAL	VCVTQQ2PD	Convert Packed Quadword Integers to Packed Double-Precision Floating-Point Values	VCVTQQ2PD.html
GENERAL	VCVTQQ2PS	Convert Packed Quadword Integers to Packed Single-Precision Floating-Point Values	VCVTQQ2PS.html
GENERAL	VCVTSD2USI	Convert Scalar Double-Precision Floating-Point Value to Unsigned Doubleword Integer	VCVTSD2USI.html
GENERAL	VCVTSS2USI	Convert Scalar Single-Precision Floating-Point Value to Unsigned Doubleword Integer	VCVTSS2USI.html
GENERAL	VCVTTPD2QQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Quadword Integers	VCVTTPD2QQ.html
GENERAL	VCVTTPD2UDQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Doubleword Integers	VCVTTPD2UDQ.html
GENERAL	VCVTTPD2UQQ	Convert with Truncation Packed Double-Precision Floating-Point Values to Packed Unsigned Quadword Integers	VCVTTPD2UQQ.html
GENERAL	VCVTTPS2QQ	Convert with Truncation Packed Single Precision Floating-Point Values to Packed Singed Quadword Integer Values	VCVTTPS2QQ.html
GENERAL	VCVTTPS2UDQ	Convert with Truncation Packed Single-Precision Floating-Point Values to Packed Unsigned Doubleword Integer Values	VCVTTPS2UDQ.html
GENERAL	VCVTTPS2UQQ	Convert with Truncation Packed Single Precision Floating-Point Values to Packed Unsigned Quadword Integer Values	VCVTTPS2UQQ.html
GENERAL	VCVTTSD2USI	Convert with Truncation Scalar Double-Precision Floating-Point Value to Unsigned Integer	VCVTTSD2USI.html
GENERAL	VCVTTSS2USI	Convert with Truncation Scalar Single-Precision Floating-Point Value to Unsigned Integer	VCVTTSS2USI.html
GENERAL	VCVTUDQ2PD	Convert Packed Unsigned Doubleword Integers to Packed Double-Precision Floating-Point Values	VCVTUDQ2PD.html
GENERAL	VCVTUDQ2PS	Convert Packed Unsigned Doubleword Integers to Packed Single-Precision Floating-Point Values	VCVTUDQ2PS.html
GENERAL	VCVTUQQ2PD	Convert Packed Unsigned Quadword Integers to Packed Double-Precision Floating-Point Values	VCVTUQQ2PD.html
GENERAL	VCVTUQQ2PS	Convert Packed Unsigned Quadword Integers to Packed Single-Precision Floating-Point Values	VCVTUQQ2PS.html
GENERAL	VCVTUSI2SD	Convert Unsigned Integer to Scalar Double-Precision Floating-Point Value	VCVTUSI2SD.html
GENERAL	VCVTUSI2SS	Convert Unsigned Integer to Scalar Single-Precision Floating-Point Value	VCVTUSI2SS.html
GENERAL	VDBPSADBW	Double Block Packed Sum-Absolute-Differences (SAD) on Unsigned Bytes	VDBPSADBW.html
GENERAL	VERR	Verify a Segment for Reading or Writing	VERR_VERW.html
GENERAL	VERW	Verify a Segment for Reading or Writing	VERR_VERW.html
GENERAL	VEXP2PD	Approximation to the Exponential 2^x of Packed Double-Precision Floating-Point Values with Less Than 2^-23 Relative Error	VEXP2PD.html
GENERAL	VEXP2PS	Approximation to the Exponential 2^x of Packed Single-Precision Floating-Point Values with Less Than 2^-23 Relative Error	VEXP2PS.html
GENERAL	VEXPANDPD	Load Sparse Packed Double-Precision Floating-Point Values from Dense Memory	VEXPANDPD.html
GENERAL	VEXPANDPS	Load Sparse Packed Single-Precision Floating-Point Values from Dense Memory	VEXPANDPS.html
GENERAL	VEXTRACTF128	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4.html
GENERAL	VEXTRACTF64x2	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4.html
GENERAL	VEXTRACTF32x8	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4.html
GENERAL	VEXTRACTF64x4	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4.html
GENERAL	VEXTRACTF32x4	Extra ct Packed Floating-Point Values	VEXTRACTF128_VEXTRACTF32x4_VEXTRACTF64x2_VEXTRACTF32x8_VEXTRACTF64x4.html
GENERAL	VEXTRACTI32x4	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4.html
GENERAL	VEXTRACTI64x4	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4.html
GENERAL	VEXTRACTI32x8	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4.html
GENERAL	VEXTRACTI128	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4.html
GENERAL	VEXTRACTI64x2	Extract packed Integer Values	VEXTRACTI128_VEXTRACTI32x4_VEXTRACTI64x2_VEXTRACTI32x8_VEXTRACTI64x4.html
GENERAL	VFIXUPIMMPD	Fix Up Special Packed Float64 Values	VFIXUPIMMPD.html
GENERAL	VFIXUPIMMPS	Fix Up Special Packed Float32 Values	VFIXUPIMMPS.html
GENERAL	VFIXUPIMMSD	Fix Up Special Scalar Float64 Value	VFIXUPIMMSD.html
GENERAL	VFIXUPIMMSS	Fix Up Special Scalar Float32 Value	VFIXUPIMMSS.html
GENERAL	VFMADD231PD	Fused Multiply-Add of Packed Double-Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD.html
GENERAL	VFMADD132PD	Fused Multiply-Add of Packed Double-Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD.html
GENERAL	VFMADD213PD	Fused Multiply-Add of Packed Double-Precision Floating-Point Values	VFMADD132PD_VFMADD213PD_VFMADD231PD.html
GENERAL	VFMADD213PS	Fused Multiply-Add of Packed Single-Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS.html
GENERAL	VFMADD132PS	Fused Multiply-Add of Packed Single-Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS.html
GENERAL	VFMADD231PS	Fused Multiply-Add of Packed Single-Precision Floating-Point Values	VFMADD132PS_VFMADD213PS_VFMADD231PS.html
GENERAL	VFMADD213SD	Fused Multiply-Add of Scalar Double-Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD.html
GENERAL	VFMADD231SD	Fused Multiply-Add of Scalar Double-Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD.html
GENERAL	VFMADD132SD	Fused Multiply-Add of Scalar Double-Precision Floating-Point Values	VFMADD132SD_VFMADD213SD_VFMADD231SD.html
GENERAL	VFMADD213SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS.html
GENERAL	VFMADD231SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS.html
GENERAL	VFMADD132SS	Fused Multiply-Add of Scalar Single-Precision Floating-Point Values	VFMADD132SS_VFMADD213SS_VFMADD231SS.html
GENERAL	VFMADDSUB213PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD.html
GENERAL	VFMADDSUB231PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD.html
GENERAL	VFMADDSUB132PD	Fused Multiply-Alternating Add/Subtract of Packed Double-Precision Floating-Point Values	VFMADDSUB132PD_VFMADDSUB213PD_VFMADDSUB231PD.html
GENERAL	VFMADDSUB213PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS.html
GENERAL	VFMADDSUB132PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS.html
GENERAL	VFMADDSUB231PS	Fused Multiply-Alternating Add/Subtract of Packed Single-Precision Floating-Point Values	VFMADDSUB132PS_VFMADDSUB213PS_VFMADDSUB231PS.html
GENERAL	VFMSUB231PD	Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD.html
GENERAL	VFMSUB132PD	Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD.html
GENERAL	VFMSUB213PD	Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFMSUB132PD_VFMSUB213PD_VFMSUB231PD.html
GENERAL	VFMSUB231PS	Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS.html
GENERAL	VFMSUB132PS	Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS.html
GENERAL	VFMSUB213PS	Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFMSUB132PS_VFMSUB213PS_VFMSUB231PS.html
GENERAL	VFMSUB132SD	Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD.html
GENERAL	VFMSUB231SD	Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD.html
GENERAL	VFMSUB213SD	Fused Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFMSUB132SD_VFMSUB213SD_VFMSUB231SD.html
GENERAL	VFMSUB231SS	Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS.html
GENERAL	VFMSUB132SS	Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS.html
GENERAL	VFMSUB213SS	Fused Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFMSUB132SS_VFMSUB213SS_VFMSUB231SS.html
GENERAL	VFMSUBADD231PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD.html
GENERAL	VFMSUBADD213PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD.html
GENERAL	VFMSUBADD132PD	Fused Multiply-Alternating Subtract/Add of Packed Double-Precision Floating-Point Values	VFMSUBADD132PD_VFMSUBADD213PD_VFMSUBADD231PD.html
GENERAL	VFMSUBADD231PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS.html
GENERAL	VFMSUBADD132PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS.html
GENERAL	VFMSUBADD213PS	Fused Multiply-Alternating Subtract/Add of Packed Single-Precision Floating-Point Values	VFMSUBADD132PS_VFMSUBADD213PS_VFMSUBADD231PS.html
GENERAL	VFNMADD231PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD.html
GENERAL	VFNMADD132PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD.html
GENERAL	VFNMADD213PD	Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values	VFNMADD132PD_VFNMADD213PD_VFNMADD231PD.html
GENERAL	VFNMADD231PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS.html
GENERAL	VFNMADD213PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS.html
GENERAL	VFNMADD132PS	Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values	VFNMADD132PS_VFNMADD213PS_VFNMADD231PS.html
GENERAL	VFNMADD213SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD.html
GENERAL	VFNMADD132SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD.html
GENERAL	VFNMADD231SD	Fused Negative Multiply-Add of Scalar Double-Precision Floating-Point Values	VFNMADD132SD_VFNMADD213SD_VFNMADD231SD.html
GENERAL	VFNMADD213SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS.html
GENERAL	VFNMADD132SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS.html
GENERAL	VFNMADD231SS	Fused Negative Multiply-Add of Scalar Single-Precision Floating-Point Values	VFNMADD132SS_VFNMADD213SS_VFNMADD231SS.html
GENERAL	VFNMSUB213PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD.html
GENERAL	VFNMSUB231PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD.html
GENERAL	VFNMSUB132PD	Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point Values	VFNMSUB132PD_VFNMSUB213PD_VFNMSUB231PD.html
GENERAL	VFNMSUB213PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS.html
GENERAL	VFNMSUB231PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS.html
GENERAL	VFNMSUB132PS	Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Values	VFNMSUB132PS_VFNMSUB213PS_VFNMSUB231PS.html
GENERAL	VFNMSUB213SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD.html
GENERAL	VFNMSUB231SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD.html
GENERAL	VFNMSUB132SD	Fused Negative Multiply-Subtract of Scalar Double-Precision Floating-Point Values	VFNMSUB132SD_VFNMSUB213SD_VFNMSUB231SD.html
GENERAL	VFNMSUB213SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS.html
GENERAL	VFNMSUB132SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS.html
GENERAL	VFNMSUB231SS	Fused Negative Multiply-Subtract of Scalar Single-Precision Floating-Point Values	VFNMSUB132SS_VFNMSUB213SS_VFNMSUB231SS.html
GENERAL	VFPCLASSPD	Tests Types Of a Packed Float64 Values	VFPCLASSPD.html
GENERAL	VFPCLASSPS	Tests Types Of a Packed Float32 Values	VFPCLASSPS.html
GENERAL	VFPCLASSSD	Tests Types Of a Scalar Float64 Values	VFPCLASSSD.html
GENERAL	VFPCLASSSS	Tests Types Of a Scalar Float32 Values	VFPCLASSSS.html
GENERAL	VGATHERDPD	Gather Packed DP FP Values Using Signed Dword/Qword Indices	VGATHERDPD_VGATHERQPD.html
GENERAL	VGATHERQPD	Gather Packed DP FP Values Using Signed Dword/Qword Indices	VGATHERDPD_VGATHERQPD.html
GENERAL	VGATHERDPS	Gather Packed Single, Packed Double with Signed Dword	VGATHERDPS_VGATHERDPD.html
GENERAL	VGATHERDPD	Gather Packed Single, Packed Double with Signed Dword	VGATHERDPS_VGATHERDPD.html
GENERAL	VGATHERDPS	Gather Packed SP FP values Using Signed Dword/Qword Indices	VGATHERDPS_VGATHERQPS.html
GENERAL	VGATHERQPS	Gather Packed SP FP values Using Signed Dword/Qword Indices	VGATHERDPS_VGATHERQPS.html
GENERAL	VGATHERPF0QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD.html
GENERAL	VGATHERPF0QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD.html
GENERAL	VGATHERPF0DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD.html
GENERAL	VGATHERPF0DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint	VGATHERPF0DPS_VGATHERPF0QPS_VGATHERPF0DPD_VGATHERPF0QPD.html
GENERAL	VGATHERPF1QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD.html
GENERAL	VGATHERPF1QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD.html
GENERAL	VGATHERPF1DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD.html
GENERAL	VGATHERPF1DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint	VGATHERPF1DPS_VGATHERPF1QPS_VGATHERPF1DPD_VGATHERPF1QPD.html
GENERAL	VGATHERQPD	Gather Packed Single, Packed Double with Signed Qword Indices	VGATHERQPS_VGATHERQPD.html
GENERAL	VGATHERQPS	Gather Packed Single, Packed Double with Signed Qword Indices	VGATHERQPS_VGATHERQPD.html
GENERAL	VGETEXPPD	Convert Exponents of Packed DP FP Values to DP FP Values	VGETEXPPD.html
GENERAL	VGETEXPPS	Convert Exponents of Packed SP FP Values to SP FP Values	VGETEXPPS.html
GENERAL	VGETEXPSD	Convert Exponents of Scalar DP FP Values to DP FP Value	VGETEXPSD.html
GENERAL	VGETEXPSS	Convert Exponents of Scalar SP FP Values to SP FP Value	VGETEXPSS.html
GENERAL	VGETMANTPD	Extract Float64 Vector of Normalized Mantissas from Float64 Vector	VGETMANTPD.html
GENERAL	VGETMANTPS	Extract Float32 Vector of Normalized Mantissas from Float32 Vector	VGETMANTPS.html
GENERAL	VGETMANTSD	Extract Float64 of Normalized Mantissas from Float64 Scalar	VGETMANTSD.html
GENERAL	VGETMANTSS	Extract Float32 Vector of Normalized Mantissa from Float32 Vector	VGETMANTSS.html
GENERAL	VINSERTF64x2	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4.html
GENERAL	VINSERTF32x8	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4.html
GENERAL	VINSERTF128	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4.html
GENERAL	VINSERTF32x4	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4.html
GENERAL	VINSERTF64x4	Insert Packed Floating-Point Values	VINSERTF128_VINSERTF32x4_VINSERTF64x2_VINSERTF32x8_VINSERTF64x4.html
GENERAL	VINSERTI128	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4.html
GENERAL	VINSERTI32x8	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4.html
GENERAL	VINSERTI64x2	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4.html
GENERAL	VINSERTI32x4	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4.html
GENERAL	VINSERTI64x4	Insert Packed Integer Values	VINSERTI128_VINSERTI32x4_VINSERTI64x2_VINSERTI32x8_VINSERTI64x4.html
GENERAL	VMASKMOVPS	Conditional SIMD Packed Loads and Stores	VMASKMOV.html
GENERAL	VMASKMOVPD	Conditional SIMD Packed Loads and Stores	VMASKMOV.html
GENERAL	VPBLENDD	Blend Packed Dwords	VPBLENDD.html
GENERAL	VPBLENDMW	Blend Byte/Word Vectors Using an Opmask Control	VPBLENDMB_VPBLENDMW.html
GENERAL	VPBLENDMB	Blend Byte/Word Vectors Using an Opmask Control	VPBLENDMB_VPBLENDMW.html
GENERAL	VPBLENDMD	Blend Int32/Int64 Vectors Using an OpMask Control	VPBLENDMD_VPBLENDMQ.html
GENERAL	VPBLENDMQ	Blend Int32/Int64 Vectors Using an OpMask Control	VPBLENDMD_VPBLENDMQ.html
GENERAL	VPBROADCASTQ	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VPBROADCASTB	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VBROADCASTI32X8	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VPBROADCASTD	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VBROADCASTI32x2	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VBROADCASTI64X4	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VPBROADCASTW	Load Integer and Broadcast	VPBROADCAST.html
GENERAL	VPBROADCASTQ	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q.html
GENERAL	VPBROADCASTB	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q.html
GENERAL	VPBROADCASTD	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q.html
GENERAL	VPBROADCASTW	Load with Broadcast Integer Data from General Purpose Register	VPBROADCASTB_W_D_Q.html
GENERAL	VPBROADCASTMW2D	Broadcast Mask to Vector Register	VPBROADCASTM.html
GENERAL	VPBROADCASTMB2Q	Broadcast Mask to Vector Register	VPBROADCASTM.html
GENERAL	VPCMPB	Compare Packed Byte Values Into Mask	VPCMPB_VPCMPUB.html
GENERAL	VPCMPUB	Compare Packed Byte Values Into Mask	VPCMPB_VPCMPUB.html
GENERAL	VPCMPUD	Compare Packed Integer Values into Mask	VPCMPD_VPCMPUD.html
GENERAL	VPCMPD	Compare Packed Integer Values into Mask	VPCMPD_VPCMPUD.html
GENERAL	VPCMPQ	Compare Packed Integer Values into Mask	VPCMPQ_VPCMPUQ.html
GENERAL	VPCMPUQ	Compare Packed Integer Values into Mask	VPCMPQ_VPCMPUQ.html
GENERAL	VPCMPUW	Compare Packed Word Values Into Mask	VPCMPW_VPCMPUW.html
GENERAL	VPCMPW	Compare Packed Word Values Into Mask	VPCMPW_VPCMPUW.html
GENERAL	VPCOMPRESSD	Store Sparse Packed Doubleword Integer Values into Dense Memory/Register	VPCOMPRESSD.html
GENERAL	VPCOMPRESSQ	Store Sparse Packed Quadword Integer Values into Dense Memory/Register	VPCOMPRESSQ.html
GENERAL	VPCONFLICTQ	Detect Conflicts Within a Vector of Packed Dword/Qword Values into Dense Memory/ Register	VPCONFLICTD_Q.html
GENERAL	VPCONFLICTD	Detect Conflicts Within a Vector of Packed Dword/Qword Values into Dense Memory/ Register	VPCONFLICTD_Q.html
GENERAL	VPERM2F128	Permute Floating-Point Values	VPERM2F128.html
GENERAL	VPERM2I128	Permute Integer Values	VPERM2I128.html
GENERAL	VPERMD	Permute Packed Doublewords/Words Elements	VPERMD_VPERMW.html
GENERAL	VPERMW	Permute Packed Doublewords/Words Elements	VPERMD_VPERMW.html
GENERAL	VPERMI2W	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD.html
GENERAL	VPERMI2PD	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD.html
GENERAL	VPERMI2PS	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD.html
GENERAL	VPERMI2D	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD.html
GENERAL	VPERMI2Q	Full Permute From Two Tables Overwriting the Index	VPERMI2W_D_Q_PS_PD.html
GENERAL	VPERMILPD	Permute In-Lane of Pairs of Double-Precision Floating-Point Values	VPERMILPD.html
GENERAL	VPERMILPS	Permute In-Lane of Quadruples of Single-Precision Floating-Point Values	VPERMILPS.html
GENERAL	VPERMPD	Permute Double-Precision Floating-Point Elements	VPERMPD.html
GENERAL	VPERMPS	Permute Single-Precision Floating-Point Elements	VPERMPS.html
GENERAL	VPERMQ	Qwords Element Permutation	VPERMQ.html
GENERAL	VPEXPANDD	Load Sparse Packed Doubleword Integer Values from Dense Memory / Register	VPEXPANDD.html
GENERAL	VPEXPANDQ	Load Sparse Packed Quadword Integer Values from Dense Memory / Register	VPEXPANDQ.html
GENERAL	VPGATHERDD	Gather Packed Dword, Packed Qword with Signed Dword Indices	VPGATHERDD_VPGATHERDQ.html
GENERAL	VPGATHERDQ	Gather Packed Dword, Packed Qword with Signed Dword Indices	VPGATHERDD_VPGATHERDQ.html
GENERAL	VPGATHERDD	Gather Packed Dword Values Using Signed Dword/Qword Indices	VPGATHERDD_VPGATHERQD.html
GENERAL	VPGATHERQD	Gather Packed Dword Values Using Signed Dword/Qword Indices	VPGATHERDD_VPGATHERQD.html
GENERAL	VPGATHERQQ	Gather Packed Qword Values Using Signed Dword/Qword Indices	VPGATHERDQ_VPGATHERQQ.html
GENERAL	VPGATHERDQ	Gather Packed Qword Values Using Signed Dword/Qword Indices	VPGATHERDQ_VPGATHERQQ.html
GENERAL	VPGATHERQQ	Gather Packed Dword, Packed Qword with Signed Qword Indices	VPGATHERQD_VPGATHERQQ.html
GENERAL	VPGATHERQD	Gather Packed Dword, Packed Qword with Signed Qword Indices	VPGATHERQD_VPGATHERQQ.html
GENERAL	VPLZCNTD	Count the Number of Leading Zero Bits for Packed Dword, Packed Qword Values	VPLZCNTD_Q.html
GENERAL	VPLZCNTQ	Count the Number of Leading Zero Bits for Packed Dword, Packed Qword Values	VPLZCNTD_Q.html
GENERAL	VPMASKMOVD	Conditional SIMD Integer Packed Loads and Stores	VPMASKMOV.html
GENERAL	VPMASKMOVQ	Conditional SIMD Integer Packed Loads and Stores	VPMASKMOV.html
GENERAL	VPMOVW2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M.html
GENERAL	VPMOVD2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M.html
GENERAL	VPMOVQ2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M.html
GENERAL	VPMOVB2M	Convert a Vector Register to a Mask	VPMOVB2M_VPMOVW2M_VPMOVD2M_VPMOVQ2M.html
GENERAL	VPMOVDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB.html
GENERAL	VPMOVSDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB.html
GENERAL	VPMOVUSDB	Down Convert DWord to Byte	VPMOVDB_VPMOVSDB_VPMOVUSDB.html
GENERAL	VPMOVDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW.html
GENERAL	VPMOVSDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW.html
GENERAL	VPMOVUSDW	Down Convert DWord to Word	VPMOVDW_VPMOVSDW_VPMOVUSDW.html
GENERAL	VPMOVM2W	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q.html
GENERAL	VPMOVM2D	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q.html
GENERAL	VPMOVM2Q	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q.html
GENERAL	VPMOVM2B	Convert a Mask Register to a Vector Register	VPMOVM2B_VPMOVM2W_VPMOVM2D_VPMOVM2Q.html
GENERAL	VPMOVSQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB.html
GENERAL	VPMOVUSQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB.html
GENERAL	VPMOVQB	Down Convert QWord to Byte	VPMOVQB_VPMOVSQB_VPMOVUSQB.html
GENERAL	VPMOVQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD.html
GENERAL	VPMOVSQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD.html
GENERAL	VPMOVUSQD	Down Convert QWord to DWord	VPMOVQD_VPMOVSQD_VPMOVUSQD.html
GENERAL	VPMOVSQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW.html
GENERAL	VPMOVUSQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW.html
GENERAL	VPMOVQW	Down Convert QWord to Word	VPMOVQW_VPMOVSQW_VPMOVUSQW.html
GENERAL	VPMOVWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB.html
GENERAL	VPMOVSWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB.html
GENERAL	VPMOVUSWB	Down Convert Word to Byte	VPMOVWB_VPMOVSWB_VPMOVUSWB.html
GENERAL	VPSCATTERDQ	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ.html
GENERAL	VPSCATTERQD	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ.html
GENERAL	VPSCATTERDD	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ.html
GENERAL	VPSCATTERQQ	Scatter Packed Dword, Packed Qword with Signed Dword, Signed Qword Indices	VPSCATTERDD_VPSCATTERDQ_VPSCATTERQD_VPSCATTERQQ.html
GENERAL	VPSLLVQ	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ.html
GENERAL	VPSLLVD	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ.html
GENERAL	VPSLLVW	Variable Bit Shift Left Logical	VPSLLVW_VPSLLVD_VPSLLVQ.html
GENERAL	VPSRAVQ	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ.html
GENERAL	VPSRAVD	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ.html
GENERAL	VPSRAVW	Variable Bit Shift Right Arithmetic	VPSRAVW_VPSRAVD_VPSRAVQ.html
GENERAL	VPSRLVD	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ.html
GENERAL	VPSRLVW	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ.html
GENERAL	VPSRLVQ	Variable Bit Shift Right Logical	VPSRLVW_VPSRLVD_VPSRLVQ.html
GENERAL	VPTERNLOGD	Bitwise Ternary Logic	VPTERNLOGD_VPTERNLOGQ.html
GENERAL	VPTERNLOGQ	Bitwise Ternary Logic	VPTERNLOGD_VPTERNLOGQ.html
GENERAL	VPTESTMD	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ.html
GENERAL	VPTESTMQ	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ.html
GENERAL	VPTESTMB	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ.html
GENERAL	VPTESTMW	Logical AND and Set Mask	VPTESTMB_VPTESTMW_VPTESTMD_VPTESTMQ.html
GENERAL	VPTESTNMW	Logical NAND and Set	VPTESTNMB_W_D_Q.html
GENERAL	VPTESTNMD	Logical NAND and Set	VPTESTNMB_W_D_Q.html
GENERAL	VPTESTNMQ	Logical NAND and Set	VPTESTNMB_W_D_Q.html
GENERAL	VPTESTNMB	Logical NAND and Set	VPTESTNMB_W_D_Q.html
GENERAL	VRANGEPD	Range Restriction Calculation For Packed Pairs of Float64 Values	VRANGEPD.html
GENERAL	VRANGEPS	Range Restriction Calculation For Packed Pairs of Float32 Values	VRANGEPS.html
GENERAL	VRANGESD	Range Restriction Calculation From a pair of Scalar Float64 Values	VRANGESD.html
GENERAL	VRANGESS	Range Restriction Calculation From a Pair of Scalar Float32 Values	VRANGESS.html
GENERAL	VRCP14PD	Compute Approximate Reciprocals of Packed Float64 Values	VRCP14PD.html
GENERAL	VRCP14PS	Compute Approximate Reciprocals of Packed Float32 Values	VRCP14PS.html
GENERAL	VRCP14SD	Compute Approximate Reciprocal of Scalar Float64 Value	VRCP14SD.html
GENERAL	VRCP14SS	Compute Approximate Reciprocal of Scalar Float32 Value	VRCP14SS.html
GENERAL	VRCP28PD	Approximation to the Reciprocal of Packed Double-Precision Floating-Point Values with Less Than 2^-28 Relative Error	VRCP28PD.html
GENERAL	VRCP28PS	Approximation to the Reciprocal of Packed Single-Precision Floating-Point Values with Less Than 2^-28 Relative Error	VRCP28PS.html
GENERAL	VRCP28SD	Approximation to the Reciprocal of Scalar Double-Precision Floating-Point Value with Less Than 2^-28 Relative Error	VRCP28SD.html
GENERAL	VRCP28SS	Approximation to the Reciprocal of Scalar Single-Precision Floating-Point Value with Less Than 2^-28 Relative Error	VRCP28SS.html
GENERAL	VREDUCEPD	Perform Reduction Transformation on Packed Float64 Values	VREDUCEPD.html
GENERAL	VREDUCEPS	Perform Reduction Transformation on Packed Float32 Values	VREDUCEPS.html
GENERAL	VREDUCESD	Perform a Reduction Transformation on a Scalar Float64 Value	VREDUCESD.html
GENERAL	VREDUCESS	Perform a Reduction Transformation on a Scalar Float32 Value	VREDUCESS.html
GENERAL	VRNDSCALEPD	Round Packed Float64 Values To Include A Given Number Of Fraction Bits	VRNDSCALEPD.html
GENERAL	VRNDSCALEPS	Round Packed Float32 Values To Include A Given Number Of Fraction Bits	VRNDSCALEPS.html
GENERAL	VRNDSCALESD	Round Scalar Float64 Value To Include A Given Number Of Fraction Bits	VRNDSCALESD.html
GENERAL	VRNDSCALESS	Round Scalar Float32 Value To Include A Given Number Of Fraction Bits	VRNDSCALESS.html
GENERAL	VRSQRT14PD	Compute Approximate Reciprocals of Square Roots of Packed Float64 Values	VRSQRT14PD.html
GENERAL	VRSQRT14PS	Compute Approximate Reciprocals of Square Roots of Packed Float32 Values	VRSQRT14PS.html
GENERAL	VRSQRT14SD	Compute Approximate Reciprocal of Square Root of Scalar Float64 Value	VRSQRT14SD.html
GENERAL	VRSQRT14SS	Compute Approximate Reciprocal of Square Root of Scalar Float32 Value	VRSQRT14SS.html
GENERAL	VRSQRT28PD	Approximation to the Reciprocal Square Root of Packed Double-Precision Floating-Point Values with Less Than 2^-28 Relative Error	VRSQRT28PD.html
GENERAL	VRSQRT28PS	Approximation to the Reciprocal Square Root of Packed Single-Precision Floating-Point Values with Less Than 2^-28 Relative Error	VRSQRT28PS.html
GENERAL	VRSQRT28SD	Approximation to the Reciprocal Square Root of Scalar Double-Precision Floating-Point Value with Less Than 2^-28 Relative Error	VRSQRT28SD.html
GENERAL	VRSQRT28SS	Approximation to the Reciprocal Square Root of Scalar Single-Precision Floating-Point Value with Less Than 2^-28 Relative Error	VRSQRT28SS.html
GENERAL	VSCALEFPD	Scale Packed Float64 Values With Float64 Values	VSCALEFPD.html
GENERAL	VSCALEFPS	Scale Packed Float32 Values With Float32 Values	VSCALEFPS.html
GENERAL	VSCALEFSD	Scale Scalar Float64 Values With Float64 Values	VSCALEFSD.html
GENERAL	VSCALEFSS	Scale Scalar Float32 Value With Float32 Value	VSCALEFSS.html
GENERAL	VSCATTERDPS	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD.html
GENERAL	VSCATTERQPS	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD.html
GENERAL	VSCATTERQPD	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD.html
GENERAL	VSCATTERDPD	Scatter Packed Single, Packed Double with Signed Dword and Qword Indices	VSCATTERDPS_VSCATTERDPD_VSCATTERQPS_VSCATTERQPD.html
GENERAL	VSCATTERPF0DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD.html
GENERAL	VSCATTERPF0DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD.html
GENERAL	VSCATTERPF0QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD.html
GENERAL	VSCATTERPF0QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T0 Hint with Intent to Write	VSCATTERPF0DPS_VSCATTERPF0QPS_VSCATTERPF0DPD_VSCATTERPF0QPD.html
GENERAL	VSCATTERPF1DPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD.html
GENERAL	VSCATTERPF1DPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD.html
GENERAL	VSCATTERPF1QPD	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD.html
GENERAL	VSCATTERPF1QPS	Sparse Prefetch Packed SP/DP Data Values with Signed Dword, Signed Qword Indices Using T1 Hint with Intent to Write	VSCATTERPF1DPS_VSCATTERPF1QPS_VSCATTERPF1DPD_VSCATTERPF1QPD.html
GENERAL	VSHUFF32x4	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2.html
GENERAL	VSHUFI64x2	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2.html
GENERAL	VSHUFI32x4	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2.html
GENERAL	VSHUFF64x2	Shuffle Packed Values at 128-bit Granularity	VSHUFF32x4_VSHUFF64x2_VSHUFI32x4_VSHUFI64x2.html
GENERAL	VTESTPS	Packed Bit Test	VTESTPD_VTESTPS.html
GENERAL	VTESTPD	Packed Bit Test	VTESTPD_VTESTPS.html
GENERAL	VZEROALL	Zero All YMM Registers	VZEROALL.html
GENERAL	VZEROUPPER	Zero Upper Bits of YMM Registers	VZEROUPPER.html
GENERAL	FWAIT	Wait	WAIT_FWAIT.html
GENERAL	WAIT	Wait	WAIT_FWAIT.html
GENERAL	WBINVD	Write Back and Invalidate Cache	WBINVD.html
GENERAL	WRFSBASE	Write FS/GS Segment Base	WRFSBASE_WRGSBASE.html
GENERAL	WRGSBASE	Write FS/GS Segment Base	WRFSBASE_WRGSBASE.html
GENERAL	WRMSR	Write to Model Specific Register	WRMSR.html
GENERAL	WRPKRU	Write Data to User Page Key Register	WRPKRU.html
GENERAL	XABORT	Transactional Abort	XABORT.html
GENERAL	XACQUIRE	Hardware Lock Elision Prefix Hints	XACQUIRE_XRELEASE.html
GENERAL	XRELEASE	Hardware Lock Elision Prefix Hints	XACQUIRE_XRELEASE.html
GENERAL	XADD	Exchange and Add	XADD.html
GENERAL	XBEGIN	Transactional Begin	XBEGIN.html
GENERAL	XCHG	Exchange Register/Memory with Register	XCHG.html
GENERAL	XEND	Transactional End	XEND.html
GENERAL	XGETBV	Get Value of Extended Control Register	XGETBV.html
GENERAL	XLATB	Table Look-up Translation	XLAT_XLATB.html
GENERAL	XLAT	Table Look-up Translation	XLAT_XLATB.html
GENERAL	XOR	Logical Exclusive OR	XOR.html
GENERAL	XORPD	Bitwise Logical XOR of Packed Double Precision Floating-Point Values	XORPD.html
GENERAL	VXORPD	Bitwise Logical XOR of Packed Double Precision Floating-Point Values	XORPD.html
GENERAL	XORPS	Bitwise Logical XOR of Packed Single Precision Floating-Point Values	XORPS.html
GENERAL	VXORPS	Bitwise Logical XOR of Packed Single Precision Floating-Point Values	XORPS.html
GENERAL	XRSTOR	Restore Processor Extended States	XRSTOR.html
GENERAL	XRSTORS	Restore Processor Extended States Supervisor	XRSTORS.html
GENERAL	XSAVE	Save Processor Extended States	XSAVE.html
GENERAL	XSAVEC	Save Processor Extended States with Compaction	XSAVEC.html
GENERAL	XSAVEOPT	Save Processor Extended States Optimized	XSAVEOPT.html
GENERAL	XSAVES	Save Processor Extended States Supervisor	XSAVES.html
GENERAL	XSETBV	Set Extended Control Register	XSETBV.html
GENERAL	XTEST	Test If In Transactional Execution	XTEST.html
AAA			AAA 	ASCII adjust AL after addition.
AAD			AAD 	ASCII adjust AX before division.
AAD	IMM8		AAD IMM8	Adjust AX before division to number base imm8.
AAM			AAM 	ASCII adjust AX after multiply.
AAM	IMM8		AAM IMM8	Adjust AX after multiply to number base imm8.
AAS			AAS 	ASCII adjust AL after subtraction.
ADC	AL,IMM8		ADC AL,IMM8	Add with carry imm8 to AL.
ADC	AX,IMM16		ADC AX,IMM16	Add with carry imm16 to AX.
ADC	EAX,IMM32		ADC EAX,IMM32	Add with carry imm32 to EAX.
ADC	R/M16,IMM16		ADC R/M16,IMM16	Add with carry imm16 to r/m16.
ADC	R/M16,IMM8		ADC R/M16,IMM8	Add with CF sign-extended imm8 to r/m16.
ADC	R/M16,R16		ADC R/M16,R16	Add with carry r16 to r/m16.
ADC	R/M32,IMM32		ADC R/M32,IMM32	Add with CF imm32 to r/m32.
ADC	R/M32,IMM8		ADC R/M32,IMM8	Add with CF sign-extended imm8 into r/m32.
ADC	R/M32,R32		ADC R/M32,R32	Add with CF r32 to r/m32.
ADC	R/M64,IMM32		ADC R/M64,IMM32	Add with CF imm32 sign extended to 64-bits to r/m64.
ADC	R/M64,IMM8		ADC R/M64,IMM8	Add with CF sign-extended imm8 into r/m64.
ADC	R/M64,R64		ADC R/M64,R64	Add with CF r64 to r/m64.
ADC	R/M8,IMM8		ADC R/M8,IMM8	Add with carry imm8 to r/m8.
ADC	R/M8,R8		ADC R/M8,R8	Add with carry byte register to r/m8.
ADC	R16,R/M16		ADC R16,R/M16	Add with carry r/m16 to r16.
ADC	R32,R/M32		ADC R32,R/M32	Add with CF r/m32 to r32.
ADC	R64,R/M64		ADC R64,R/M64	Add with CF r/m64 to r64.
ADC	R8,R/M8		ADC R8,R/M8	Add with carry r/m8 to byte register.
ADC	RAX,IMM32		ADC RAX,IMM32	Add with carry imm32 sign extended to 64-bits to RAX.
ADCX	R32,R/M32	ADX	ADCX R32,R/M32	Unsigned addition of r32 with CF, r/m32 to r32, writes CF.
ADCX	R64,R/M64	ADX	ADCX R64,R/M64	Unsigned addition of r64 with CF, r/m64 to r64, writes CF.
ADD	AL,IMM8		ADD AL,IMM8	Add imm8 to AL.
ADD	AX,IMM16		ADD AX,IMM16	Add imm16 to AX.
ADD	EAX,IMM32		ADD EAX,IMM32	Add imm32 to EAX.
ADD	R/M16,IMM16		ADD R/M16,IMM16	Add imm16 to r/m16.
ADD	R/M16,IMM8		ADD R/M16,IMM8	Add sign-extended imm8 to r/m16.
ADD	R/M16,R16		ADD R/M16,R16	Add r16 to r/m16.
ADD	R/M32,IMM32		ADD R/M32,IMM32	Add imm32 to r/m32.
ADD	R/M32,IMM8		ADD R/M32,IMM8	Add sign-extended imm8 to r/m32.
ADD	R/M32,R32		ADD R/M32,R32	Add r32 to r/m32.
ADD	R/M64,IMM32		ADD R/M64,IMM32	Add imm32 sign-extended to 64-bits to r/m64.
ADD	R/M64,IMM8		ADD R/M64,IMM8	Add sign-extended imm8 to r/m64.
ADD	R/M64,R64		ADD R/M64,R64	Add r64 to r/m64.
ADD	R/M8,IMM8		ADD R/M8,IMM8	Add imm8 to r/m8.
ADD	R/M8,R8		ADD R/M8,R8	Add r8 to r/m8.
ADD	R16,R/M16		ADD R16,R/M16	Add r/m16 to r16.
ADD	R32,R/M32		ADD R32,R/M32	Add r/m32 to r32.
ADD	R64,R/M64		ADD R64,R/M64	Add r/m64 to r64.
ADD	R8,R/M8		ADD R8,R/M8	Add r/m8 to r8.
ADD	RAX,IMM32		ADD RAX,IMM32	Add imm32 sign-extended to 64-bits to RAX.
ADDPD	XMM,XMM/M128	SSE2	ADDPD XMM1,XMM2/M128	Add packed double-precision floating-point values from xmm2/mem to xmm1 and store result in xmm1.
ADDPS	XMM,XMM/M128	SSE	ADDPS XMM1,XMM2/M128	Add packed single-precision floating-point values from xmm2/m128 to xmm1 and store result in xmm1.
ADDSD	XMM,XMM/M64	SSE2	ADDSD XMM1,XMM2/M64	Add the low double-precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1.
ADDSS	XMM,XMM/M32	SSE	ADDSS XMM1,XMM2/M32	Add the low single-precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1.
ADDSUBPD	XMM,XMM/M128	SSE3	ADDSUBPD XMM1,XMM2/M128	Add/subtract double-precision floating-point values from xmm2/m128 to xmm1.
ADDSUBPS	XMM,XMM/M128	SSE3	ADDSUBPS XMM1,XMM2/M128	Add/subtract single-precision floating-point values from xmm2/m128 to xmm1.
ADOX	R32,R/M32	ADX	ADOX R32,R/M32	Unsigned addition of r32 with OF, r/m32 to r32, writes OF.
ADOX	R64,R/M64	ADX	ADOX R64,R/M64	Unsigned addition of r64 with OF, r/m64 to r64, writes OF.
AESDEC	XMM,XMM/M128	AES	AESDEC XMM1,XMM2/M128	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
AESDECLAST	XMM,XMM/M128	AES	AESDECLAST XMM1,XMM2/M128	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
AESENC	XMM,XMM/M128	AES	AESENC XMM1,XMM2/M128	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
AESENCLAST	XMM,XMM/M128	AES	AESENCLAST XMM1,XMM2/M128	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128.
AESIMC	XMM,XMM/M128	AES	AESIMC XMM1,XMM2/M128	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
AESKEYGENASSIST	XMM,XMM/M128,IMM8	AES	AESKEYGENASSIST XMM1,XMM2/M128,IMM8	Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
AND	AL,IMM8		AND AL,IMM8	AL AND imm8.
AND	AX,IMM16		AND AX,IMM16	AX AND imm16.
AND	EAX,IMM32		AND EAX,IMM32	EAX AND imm32.
AND	R/M16,IMM16		AND R/M16,IMM16	r/m16 AND imm16.
AND	R/M16,IMM8		AND R/M16,IMM8	r/m16 AND imm8 (sign-extended).
AND	R/M16,R16		AND R/M16,R16	r/m16 AND r16.
AND	R/M32,IMM32		AND R/M32,IMM32	r/m32 AND imm32.
AND	R/M32,IMM8		AND R/M32,IMM8	r/m32 AND imm8 (sign-extended).
AND	R/M32,R32		AND R/M32,R32	r/m32 AND r32.
AND	R/M64,IMM32		AND R/M64,IMM32	r/m64 AND imm32 sign extended to 64-bits.
AND	R/M64,IMM8		AND R/M64,IMM8	r/m64 AND imm8 (sign-extended).
AND	R/M64,R64		AND R/M64,R64	r/m64 AND r32.
AND	R/M8,IMM8		AND R/M8,IMM8	r/m8 AND imm8.
AND	R/M8,R8		AND R/M8,R8	r/m8 AND r8.
AND	R16,R/M16		AND R16,R/M16	r16 AND r/m16.
AND	R32,R/M32		AND R32,R/M32	r32 AND r/m32.
AND	R64,R/M64		AND R64,R/M64	r64 AND r/m64.
AND	R8,R/M8		AND R8,R/M8	r8 AND r/m8.
AND	RAX,IMM32		AND RAX,IMM32	RAX AND imm32 sign-extended to 64-bits.
ANDN	R32,R32,R/M32	BMI1	ANDN R32A,R32B,R/M32	Bitwise AND of inverted r32b with r/m32, store result in r32a.
ANDN	R64,R64,R/M64	BMI1	ANDN R64A,R64B,R/M64	Bitwise AND of inverted r64b with r/m64, store result in r64a.
ANDNPD	XMM,XMM/M128	SSE2	ANDNPD XMM1,XMM2/M128	Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm1 and xmm2/mem.
ANDNPS	XMM,XMM/M128	SSE	ANDNPS XMM1,XMM2/M128	Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm1 and xmm2/mem.
ANDPD	XMM,XMM/M128	SSE2	ANDPD XMM1,XMM2/M128	Return the bitwise logical AND of packed double-precision floating-point values in xmm1 and xmm2/mem.
ANDPS	XMM,XMM/M128	SSE	ANDPS XMM1,XMM2/M128	Return the bitwise logical AND of packed single-precision floating-point values in xmm1 and xmm2/mem.
ARPL	R/M16,R16		ARPL R/M16,R16	Adjust RPL of r/m16 to not less than RPL of r16.
BLENDPD	XMM,XMM/M128,IMM8	SSE4_1	BLENDPD XMM1,XMM2/M128,IMM8	Select packed DP-FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
BLENDPS	XMM,XMM/M128,IMM8	SSE4_1	BLENDPS XMM1,XMM2/M128,IMM8	Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
BLENDVPD	XMM,XMM/M128	SSE4_1	BLENDVPD XMM1,XMM2/M128	Select packed DP FP values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1.
BLENDVPS	XMM,XMM/M128,XMM_ZERO	SSE4_1	BLENDVPS XMM1,XMM2/M128,XMM_ZERO	Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1.
BLSI	R32,R/M32	BMI1	BLSI R32,R/M32	Extract lowest set bit from r/m32 and set that bit in r32.
BLSI	R64,R/M64	BMI1	BLSI R64,R/M64	Extract lowest set bit from r/m64, and set that bit in r64.
BLSMSK	R32,R/M32	BMI1	BLSMSK R32,R/M32	Set all lower bits in r32 to “1” starting from bit 0 to lowest set bit in r/m32.
BLSMSK	R64,R/M64	BMI1	BLSMSK R64,R/M64	Set all lower bits in r64 to “1” starting from bit 0 to lowest set bit in r/m64.
BLSR	R32,R/M32	BMI1	BLSR R32,R/M32	Reset lowest set bit of r/m32, keep all other bits of r/m32 and write result to r32.
BLSR	R64,R/M64	BMI1	BLSR R64,R/M64	Reset lowest set bit of r/m64, keep all other bits of r/m64 and write result to r64.
BNDCL	BND,R/M32	MPX	BNDCL BND,R/M32	Generate a #BR if the address in r/m32 is lower than the lower bound in bnd.LB.
BNDCL	BND,R/M64	MPX	BNDCL BND,R/M64	Generate a #BR if the address in r/m64 is lower than the lower bound in bnd.LB.
BNDCN	BND,R/M32	MPX	BNDCN BND,R/M32	Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form).
BNDCN	BND,R/M64	MPX	BNDCN BND,R/M64	Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form).
BNDCU	BND,R/M32	MPX	BNDCU BND,R/M32	Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form).
BNDCU	BND,R/M64	MPX	BNDCU BND,R/M64	Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form).
BNDLDX	BND,MIB	MPX	BNDLDX BND,MIB	Load the bounds stored in a bound table entry (BTE) into bnd with address translation using the base of mib and conditional on the index of mib matching the pointer value in the BTE.
BNDMK	BND,M32	MPX	BNDMK BND,M32	Make lower and upper bounds from m32 and store them in bnd.
BNDMK	BND,M64	MPX	BNDMK BND,M64	Make lower and upper bounds from m64 and store them in bnd.
BNDMOV	BND,BND/M128	MPX	BNDMOV BND1,BND2/M128	Move lower and upper bound from bnd2/m128 to bound register bnd1.
BNDMOV	BND,BND/M64	MPX	BNDMOV BND1,BND2/M64	Move lower and upper bound from bnd2/m64 to bound register bnd1.
BNDMOV	BND/M128,BND	MPX	BNDMOV BND1/M128,BND2	Move lower and upper bound from bnd2 to bound register bnd1/m128.
BNDMOV	BND/M64,BND	MPX	BNDMOV BND1/M64,BND2	Move lower and upper bound from bnd2 to bnd1/m64.
BNDSTX	MIB,BND	MPX	BNDSTX MIB,BND	Store the bounds in bnd and the pointer value in the index regis-ter of mib to a bound table entry (BTE) with address translation using the base of mib.
BOUND	R16,M16&16		BOUND R16,M16&16	Check if r16 (array index) is within bounds specified by m16&amp;16.
BOUND	R32,M32&32		BOUND R32,M32&32	Check if r32 (array index) is within bounds specified by m32&amp;32.
BSF	R16,R/M16		BSF R16,R/M16	Bit scan forward on r/m16.
BSF	R32,R/M32		BSF R32,R/M32	Bit scan forward on r/m32.
BSF	R64,R/M64		BSF R64,R/M64	Bit scan forward on r/m64.
BSR	R16,R/M16		BSR R16,R/M16	Bit scan reverse on r/m16.
BSR	R32,R/M32		BSR R32,R/M32	Bit scan reverse on r/m32.
BSR	R64,R/M64		BSR R64,R/M64	Bit scan reverse on r/m64.
BSWAP	R32		BSWAP R32	Reverses the byte order of a 32-bit register.
BSWAP	R64		BSWAP R64	Reverses the byte order of a 64-bit register.
BT	R/M16,IMM8		BT R/M16,IMM8	Store selected bit in CF flag.
BT	R/M16,R16		BT R/M16,R16	Store selected bit in CF flag.
BT	R/M32,IMM8		BT R/M32,IMM8	Store selected bit in CF flag.
BT	R/M32,R32		BT R/M32,R32	Store selected bit in CF flag.
BT	R/M64,IMM8		BT R/M64,IMM8	Store selected bit in CF flag.
BT	R/M64,R64		BT R/M64,R64	Store selected bit in CF flag.
BTC	R/M16,IMM8		BTC R/M16,IMM8	Store selected bit in CF flag and complement.
BTC	R/M16,R16		BTC R/M16,R16	Store selected bit in CF flag and complement.
BTC	R/M32,IMM8		BTC R/M32,IMM8	Store selected bit in CF flag and complement.
BTC	R/M32,R32		BTC R/M32,R32	Store selected bit in CF flag and complement.
BTC	R/M64,IMM8		BTC R/M64,IMM8	Store selected bit in CF flag and complement.
BTC	R/M64,R64		BTC R/M64,R64	Store selected bit in CF flag and complement.
BTR	R/M16,IMM8		BTR R/M16,IMM8	Store selected bit in CF flag and clear.
BTR	R/M16,R16		BTR R/M16,R16	Store selected bit in CF flag and clear.
BTR	R/M32,IMM8		BTR R/M32,IMM8	Store selected bit in CF flag and clear.
BTR	R/M32,R32		BTR R/M32,R32	Store selected bit in CF flag and clear.
BTR	R/M64,IMM8		BTR R/M64,IMM8	Store selected bit in CF flag and clear.
BTR	R/M64,R64		BTR R/M64,R64	Store selected bit in CF flag and clear.
BTS	R/M16,IMM8		BTS R/M16,IMM8	Store selected bit in CF flag and set.
BTS	R/M16,R16		BTS R/M16,R16	Store selected bit in CF flag and set.
BTS	R/M32,IMM8		BTS R/M32,IMM8	Store selected bit in CF flag and set.
BTS	R/M32,R32		BTS R/M32,R32	Store selected bit in CF flag and set.
BTS	R/M64,IMM8		BTS R/M64,IMM8	Store selected bit in CF flag and set.
BTS	R/M64,R64		BTS R/M64,R64	Store selected bit in CF flag and set.
CALL	M16:16		CALL M16:16	Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16-bit offset from far pointer referenced in the instruction.
CALL	M16:32		CALL M16:32	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction.
CALL	M16:64		CALL M16:64	In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction.
CALL	PTR16:16		CALL PTR16:16	Call far, absolute, address given in operand.
CALL	PTR16:32		CALL PTR16:32	Call far, absolute, address given in operand.
CALL	R/M16		CALL R/M16	Call near, absolute indirect, address given in r/m16.
CALL	R/M32		CALL R/M32	Call near, absolute indirect, address given in r/m32.
CALL	R/M64		CALL R/M64	Call near, absolute indirect, address given in r/m64.
CALL	REL16		CALL REL16	Call near, relative, displacement relative to next instruction.
CALL	REL32		CALL REL32	Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode.
CBW			CBW 	AX ← sign-extend of AL.
CDQ			CDQ 	EDX:EAX ← sign-extend of EAX.
CDQE			CDQE 	RAX ← sign-extend of EAX.
CLAC			CLAC 	Clear the AC flag in the EFLAGS register.
CLC			CLC 	Clear CF flag.
CLD			CLD 	Clear DF flag.
CLFLUSH	M8		CLFLUSH M8	Flushes cache line containing m8.
CLFLUSHOPT	M8		CLFLUSHOPT M8	Flushes cache line containing m8.
CLI			CLI 	Clear interrupt flag; interrupts disabled when interrupt flag cleared.
CLTS			CLTS 	Clears TS flag in CR0.
CMC			CMC 	Complement CF flag.
CMP	AL,IMM8		CMP AL,IMM8	Compare imm8 with AL.
CMP	AX,IMM16		CMP AX,IMM16	Compare imm16 with AX.
CMP	EAX,IMM32		CMP EAX,IMM32	Compare imm32 with EAX.
CMP	R/M16,IMM16		CMP R/M16,IMM16	Compare imm16 with r/m16.
CMP	R/M16,IMM8		CMP R/M16,IMM8	Compare imm8 with r/m16.
CMP	R/M16,R16		CMP R/M16,R16	Compare r16 with r/m16.
CMP	R/M32,IMM32		CMP R/M32,IMM32	Compare imm32 with r/m32.
CMP	R/M32,IMM8		CMP R/M32,IMM8	Compare imm8 with r/m32.
CMP	R/M32,R32		CMP R/M32,R32	Compare r32 with r/m32.
CMP	R/M64,IMM32		CMP R/M64,IMM32	Compare imm32 sign-extended to 64-bits with r/m64.
CMP	R/M64,IMM8		CMP R/M64,IMM8	Compare imm8 with r/m64.
CMP	R/M64,R64		CMP R/M64,R64	Compare r64 with r/m64.
CMP	R/M8,IMM8		CMP R/M8,IMM8	Compare imm8 with r/m8.
CMP	R/M8,R8		CMP R/M8,R8	Compare r8 with r/m8.
CMP	R16,R/M16		CMP R16,R/M16	Compare r/m16 with r16.
CMP	R32,R/M32		CMP R32,R/M32	Compare r/m32 with r32.
CMP	R64,R/M64		CMP R64,R/M64	Compare r/m64 with r64.
CMP	R8,R/M8		CMP R8,R/M8	Compare r/m8 with r8.
CMP	RAX,IMM32		CMP RAX,IMM32	Compare imm32 sign-extended to 64-bits with RAX.
CMPPD	XMM,XMM/M128,IMM8	SSE2	CMPPD XMM1,XMM2/M128,IMM8	Compare packed double-precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate.
CMPPS	XMM,XMM/M128,IMM8	SSE	CMPPS XMM1,XMM2/M128,IMM8	Compare packed single-precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate.
CMPS	M16,M16		CMPS M16,M16	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
CMPS	M32,M32		CMPS M32,M32	For legacy mode, compare dword at address DS:(E)SI at dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI at dword at address (R|E)DI. The status flags are set accordingly.
CMPS	M64,M64		CMPS M64,M64	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
CMPS	M8,M8		CMPS M8,M8	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI to byte at address (R|E)DI. The status flags are set accordingly.
CMPSB			CMPSB 	For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly.
CMPSD			CMPSD 	For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly.
CMPSD	XMM,XMM/M64,IMM8	SSE2	CMPSD XMM1,XMM2/M64,IMM8	Compare low double-precision floating-point value in xmm2/m64 and xmm1 using bits 2:0 of imm8 as comparison predicate.
CMPSQ			CMPSQ 	Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly.
CMPSS	XMM,XMM/M32,IMM8	SSE	CMPSS XMM1,XMM2/M32,IMM8	Compare low single-precision floating-point value in xmm2/m32 and xmm1 using bits 2:0 of imm8 as comparison predicate.
CMPSW			CMPSW 	For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly.
CMPXCHG	R/M16,R16		CMPXCHG R/M16,R16	Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX.
CMPXCHG	R/M32,R32		CMPXCHG R/M32,R32	Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX.
CMPXCHG	R/M64,R64		CMPXCHG R/M64,R64	Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX.
CMPXCHG	R/M8,R8		CMPXCHG R/M8,R8	Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL.
CMPXCHG16B	M128		CMPXCHG16B M128	Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX.
CMPXCHG8B	M64		CMPXCHG8B M64	Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX.
COMISD	XMM,XMM/M64	SSE2	COMISD XMM1,XMM2/M64	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
COMISS	XMM,XMM/M32	SSE	COMISS XMM1,XMM2/M32	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
CPUID			CPUID 	Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well).
CQO			CQO 	RDX:RAX← sign-extend of RAX.
CRC32	R32,R/M16		CRC32 R32,R/M16	Accumulate CRC32 on r/m16.
CRC32	R32,R/M32		CRC32 R32,R/M32	Accumulate CRC32 on r/m32.
CRC32	R32,R/M8		CRC32 R32,R/M8	Accumulate CRC32 on r/m8.
CRC32	R64,R/M64		CRC32 R64,R/M64	Accumulate CRC32 on r/m64.
CRC32	R64,R/M8		CRC32 R64,R/M8	Accumulate CRC32 on r/m8.
CVTDQ2PD	XMM,XMM/M64	SSE2	CVTDQ2PD XMM1,XMM2/M64	Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1.
CVTDQ2PS	XMM,XMM/M128	SSE2	CVTDQ2PS XMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1.
CVTPD2DQ	XMM,XMM/M128	SSE2	CVTPD2DQ XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1.
CVTPD2PI	MM,XMM/M128		CVTPD2PI MM,XMM/M128	Convert two packed double-precision floating-point values from xmm/m128 to two packed signed doubleword integers in mm.
CVTPD2PS	XMM,XMM/M128	SSE2	CVTPD2PS XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two single-precision floating-point values in xmm1.
CVTPI2PD	XMM,MM/M64		CVTPI2PD XMM,MM/M64	Convert two packed signed doubleword integers from mm/mem64 to two packed double-precision floating-point values in xmm.
CVTPI2PS	XMM,MM/M64		CVTPI2PS XMM,MM/M64	Convert two signed doubleword integers from mm/m64 to two single-precision floating-point values in xmm.
CVTPS2DQ	XMM,XMM/M128	SSE2	CVTPS2DQ XMM1,XMM2/M128	Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1.
CVTPS2PD	XMM,XMM/M64	SSE2	CVTPS2PD XMM1,XMM2/M64	Convert two packed single-precision floating-point values in xmm2/m64 to two packed double-precision floating-point values in xmm1.
CVTPS2PI	MM,XMM/M64		CVTPS2PI MM,XMM/M64	Convert two packed single-precision floating-point values from xmm/m64 to two packed signed doubleword integers in mm.
CVTSD2SI	R32,XMM/M64	SSE2	CVTSD2SI R32,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
CVTSD2SI	R64,XMM/M64	SSE2	CVTSD2SI R64,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer sign-extended into r64.
CVTSD2SS	XMM,XMM/M64	SSE2	CVTSD2SS XMM1,XMM2/M64	Convert one double-precision floating-point value in xmm2/m64 to one single-precision floating-point value in xmm1.
CVTSI2SD	XMM,R/M64	SSE2	CVTSI2SD XMM1,R/M64	Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
CVTSI2SD	XMM,R32/M32	SSE2	CVTSI2SD XMM1,R32/M32	Convert one signed doubleword integer from r32/m32 to one double-precision floating-point value in xmm1.
CVTSI2SS	XMM,R/M32	SSE	CVTSI2SS XMM1,R/M32	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
CVTSI2SS	XMM,R/M64	SSE	CVTSI2SS XMM1,R/M64	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
CVTSS2SD	XMM,XMM/M32	SSE2	CVTSS2SD XMM1,XMM2/M32	Convert one single-precision floating-point value in xmm2/m32 to one double-precision floating-point value in xmm1.
CVTSS2SI	R32,XMM/M32	SSE	CVTSS2SI R32,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
CVTSS2SI	R64,XMM/M32	SSE	CVTSS2SI R64,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
CVTTPD2DQ	XMM,XMM/M128	SSE2	CVTTPD2DQ XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
CVTTPD2PI	MM,XMM/M128		CVTTPD2PI MM,XMM/M128	Convert two packer double-precision floating-point values from xmm/m128 to two packed signed doubleword integers in mm using truncation.
CVTTPS2DQ	XMM,XMM/M128	SSE2	CVTTPS2DQ XMM1,XMM2/M128	Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
CVTTPS2PI	MM,XMM/M64		CVTTPS2PI MM,XMM/M64	Convert two single-precision floating-point values from xmm/m64 to two signed doubleword signed integers in mm using truncation.
CVTTSD2SI	R32,XMM/M64	SSE2	CVTTSD2SI R32,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
CVTTSD2SI	R64,XMM/M64	SSE2	CVTTSD2SI R64,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
CVTTSS2SI	R32,XMM/M32	SSE	CVTTSS2SI R32,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
CVTTSS2SI	R64,XMM/M32	SSE	CVTTSS2SI R64,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
CWD			CWD 	DX:AX ← sign-extend of AX.
CWDE			CWDE 	EAX ← sign-extend of AX.
DAA			DAA 	Decimal adjust AL after addition.
DAS			DAS 	Decimal adjust AL after subtraction.
DEC	R/M16		DEC R/M16	Decrement r/m16 by 1.
DEC	R/M32		DEC R/M32	Decrement r/m32 by 1.
DEC	R/M64		DEC R/M64	Decrement r/m64 by 1.
DEC	R/M8		DEC R/M8	Decrement r/m8 by 1.
DEC	R16		DEC R16	Decrement r16 by 1.
DEC	R32		DEC R32	Decrement r32 by 1.
DIV	R/M16		DIV R/M16	Unsigned divide DX:AX by r/m16, with result stored in AX ← Quotient, DX ← Remainder.
DIV	R/M32		DIV R/M32	Unsigned divide EDX:EAX by r/m32, with result stored in EAX ← Quotient, EDX ← Remainder.
DIV	R/M64		DIV R/M64	Unsigned divide RDX:RAX by r/m64, with result stored in RAX ← Quotient, RDX ← Remainder.
DIV	R/M8		DIV R/M8	Unsigned divide AX by r/m8, with result stored in AL ← Quotient, AH ← Remainder.
DIVPD	XMM,XMM/M128	SSE2	DIVPD XMM1,XMM2/M128	Divide packed double-precision floating-point values in xmm1 by packed double-precision floating-point values in xmm2/mem.
DIVPS	XMM,XMM/M128	SSE	DIVPS XMM1,XMM2/M128	Divide packed single-precision floating-point values in xmm1 by packed single-precision floating-point values in xmm2/mem.
DIVSD	XMM,XMM/M64	SSE2	DIVSD XMM1,XMM2/M64	Divide low double-precision floating-point value in xmm1 by low double-precision floating-point value in xmm2/m64.
DIVSS	XMM,XMM/M32	SSE	DIVSS XMM1,XMM2/M32	Divide low single-precision floating-point value in xmm1 by low single-precision floating-point value in xmm2/m32.
DPPD	XMM,XMM/M128,IMM8	SSE4_1	DPPD XMM1,XMM2/M128,IMM8	Selectively multiply packed DP floating-point values from xmm1 with packed DP floating-point values from xmm2, add and selectively store the packed DP floating-point values to xmm1.
DPPS	XMM,XMM/M128,IMM8	SSE4_1	DPPS XMM1,XMM2/M128,IMM8	Selectively multiply packed SP floating-point values from xmm1 with packed SP floating-point values from xmm2, add and selectively store the packed SP floating-point values or zero values to xmm1.
EMMS			EMMS 	Set the x87 FPU tag word to empty.
ENTER	IMM16,0		ENTER IMM16,0	Create a stack frame for a procedure.
ENTER	IMM16,1		ENTER IMM16,1	Create a stack frame with a nested pointer for a procedure.
ENTER	IMM16,IMM8		ENTER IMM16,IMM8	Create a stack frame with nested pointers for a procedure.
EXTRACTPS	REG/M32,XMM,IMM8	SSE4_1	EXTRACTPS REG/M32,XMM1,IMM8	Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
F2XM1			F2XM1 	Replace ST(0) with (2ST(0) – 1).
FABS			FABS 	Replace ST with its absolute value.
FADD	M32FP		FADD M32FP	Add m32fp to ST(0) and store result in ST(0).
FADD	M64FP		FADD M64FP	Add m64fp to ST(0) and store result in ST(0).
FADD	ST(0),ST(I)		FADD ST(0),ST(I)	Add ST(0) to ST(i) and store result in ST(0).
FADD	ST(I),ST(0)		FADD ST(I),ST(0)	Add ST(i) to ST(0) and store result in ST(i).
FADDP			FADDP 	Add ST(0) to ST(1), store result in ST(1), and pop the register stack.
FADDP	ST(I),ST(0)		FADDP ST(I),ST(0)	Add ST(0) to ST(i), store result in ST(i), and pop the register stack.
FBLD	M80DEC		FBLD M80DEC	Convert BCD value to floating-point and push onto the FPU stack.
FBSTP	M80BCD		FBSTP M80BCD	Store ST(0) in m80bcd and pop ST(0).
FCHS			FCHS 	Complements sign of ST(0).
FCLEX			FCLEX 	Clear floating-point exception flags after checking for pending unmasked floating-point exceptions.
FCMOVB	ST(0),ST(I)		FCMOVB ST(0),ST(I)	Move if below (CF=1).
FCMOVBE	ST(0),ST(I)		FCMOVBE ST(0),ST(I)	Move if below or equal (CF=1 or ZF=1).
FCMOVE	ST(0),ST(I)		FCMOVE ST(0),ST(I)	Move if equal (ZF=1).
FCMOVNB	ST(0),ST(I)		FCMOVNB ST(0),ST(I)	Move if not below (CF=0).
FCMOVNBE	ST(0),ST(I)		FCMOVNBE ST(0),ST(I)	Move if not below or equal (CF=0 and ZF=0).
FCMOVNE	ST(0),ST(I)		FCMOVNE ST(0),ST(I)	Move if not equal (ZF=0).
FCMOVU	ST(0),ST(I)		FCMOVU ST(0),ST(I)	Move if unordered (PF=1).
FCOM			FCOM 	Compare ST(0) with ST(1).
FCOM	M32FP		FCOM M32FP	Compare ST(0) with m32fp.
FCOM	M64FP		FCOM M64FP	Compare ST(0) with m64fp.
FCOM	ST(I)		FCOM ST(I)	Compare ST(0) with ST(i).
FCOMI	ST,ST(I)		FCOMI ST,ST(I)	Compare ST(0) with ST(i) and set status flags accordingly.
FCOMIP	ST,ST(I)		FCOMIP ST,ST(I)	Compare ST(0) with ST(i), set status flags accordingly, and pop register stack.
FCOMP			FCOMP 	Compare ST(0) with ST(1) and pop register stack.
FCOMP	M32FP		FCOMP M32FP	Compare ST(0) with m32fp and pop register stack.
FCOMP	M64FP		FCOMP M64FP	Compare ST(0) with m64fp and pop register stack.
FCOMP	ST(I)		FCOMP ST(I)	Compare ST(0) with ST(i) and pop register stack.
FCOMPP			FCOMPP 	Compare ST(0) with ST(1) and pop register stack twice.
FCOS			FCOS 	Replace ST(0) with its approximate cosine.
FDECSTP			FDECSTP 	Decrement TOP field in FPU status word.
FDIV	M32FP		FDIV M32FP	Divide ST(0) by m32fp and store result in ST(0).
FDIV	M64FP		FDIV M64FP	Divide ST(0) by m64fp and store result in ST(0).
FDIV	ST(0),ST(I)		FDIV ST(0),ST(I)	Divide ST(0) by ST(i) and store result in ST(0).
FDIV	ST(I),ST(0)		FDIV ST(I),ST(0)	Divide ST(i) by ST(0) and store result in ST(i).
FDIVP			FDIVP 	Divide ST(1) by ST(0), store result in ST(1), and pop the register stack.
FDIVP	ST(I),ST(0)		FDIVP ST(I),ST(0)	Divide ST(i) by ST(0), store result in ST(i), and pop the register stack.
FDIVR	M32FP		FDIVR M32FP	Divide m32fp by ST(0) and store result in ST(0).
FDIVR	M64FP		FDIVR M64FP	Divide m64fp by ST(0) and store result in ST(0).
FDIVR	ST(0),ST(I)		FDIVR ST(0),ST(I)	Divide ST(i) by ST(0) and store result in ST(0).
FDIVR	ST(I),ST(0)		FDIVR ST(I),ST(0)	Divide ST(0) by ST(i) and store result in ST(i).
FDIVRP			FDIVRP 	Divide ST(0) by ST(1), store result in ST(1), and pop the register stack.
FDIVRP	ST(I),ST(0)		FDIVRP ST(I),ST(0)	Divide ST(0) by ST(i), store result in ST(i), and pop the register stack.
FFREE	ST(I)		FFREE ST(I)	Sets tag for ST(i) to empty.
FIADD	M16INT		FIADD M16INT	Add m16int to ST(0) and store result in ST(0).
FIADD	M32INT		FIADD M32INT	Add m32int to ST(0) and store result in ST(0).
FICOM	M16INT		FICOM M16INT	Compare ST(0) with m16int.
FICOM	M32INT		FICOM M32INT	Compare ST(0) with m32int.
FICOMP	M16INT		FICOMP M16INT	Compare ST(0) with m16int and pop stack register.
FICOMP	M32INT		FICOMP M32INT	Compare ST(0) with m32int and pop stack register.
FIDIV	M16INT		FIDIV M16INT	Divide ST(0) by m16int and store result in ST(0).
FIDIV	M32INT		FIDIV M32INT	Divide ST(0) by m32int and store result in ST(0).
FIDIVR	M16INT		FIDIVR M16INT	Divide m16int by ST(0) and store result in ST(0).
FIDIVR	M32INT		FIDIVR M32INT	Divide m32int by ST(0) and store result in ST(0).
FILD	M16INT		FILD M16INT	Push m16int onto the FPU register stack.
FILD	M32INT		FILD M32INT	Push m32int onto the FPU register stack.
FILD	M64INT		FILD M64INT	Push m64int onto the FPU register stack.
FIMUL	M16INT		FIMUL M16INT	Multiply ST(0) by m16int and store result in ST(0).
FIMUL	M32INT		FIMUL M32INT	Multiply ST(0) by m32int and store result in ST(0).
FINCSTP			FINCSTP 	Increment the TOP field in the FPU status register.
FINIT			FINIT 	Initialize FPU after checking for pending unmasked floating-point exceptions.
FIST	M16INT		FIST M16INT	Store ST(0) in m16int.
FIST	M32INT		FIST M32INT	Store ST(0) in m32int.
FISTP	M16INT		FISTP M16INT	Store ST(0) in m16int and pop register stack.
FISTP	M32INT		FISTP M32INT	Store ST(0) in m32int and pop register stack.
FISTP	M64INT		FISTP M64INT	Store ST(0) in m64int and pop register stack.
FISTTP	M16INT		FISTTP M16INT	Store ST(0) in m16int with truncation.
FISTTP	M32INT		FISTTP M32INT	Store ST(0) in m32int with truncation.
FISTTP	M64INT		FISTTP M64INT	Store ST(0) in m64int with truncation.
FISUB	M16INT		FISUB M16INT	Subtract m16int from ST(0) and store result in ST(0).
FISUB	M32INT		FISUB M32INT	Subtract m32int from ST(0) and store result in ST(0).
FISUBR	M16INT		FISUBR M16INT	Subtract ST(0) from m16int and store result in ST(0).
FISUBR	M32INT		FISUBR M32INT	Subtract ST(0) from m32int and store result in ST(0).
FLD	M32FP		FLD M32FP	Push m32fp onto the FPU register stack.
FLD	M64FP		FLD M64FP	Push m64fp onto the FPU register stack.
FLD	M80FP		FLD M80FP	Push m80fp onto the FPU register stack.
FLD	ST(I)		FLD ST(I)	Push ST(i) onto the FPU register stack.
FLD1			FLD1 	Push +1.0 onto the FPU register stack.
FLDCW	M2BYTE		FLDCW M2BYTE	Load FPU control word from m2byte.
FLDENV	M14/28BYTE		FLDENV M14/28BYTE	Load FPU environment from m14byte or m28byte.
FLDL2E			FLDL2E 	Push log2e onto the FPU register stack.
FLDL2T			FLDL2T 	Push log210 onto the FPU register stack.
FLDLG2			FLDLG2 	Push log102 onto the FPU register stack.
FLDLN2			FLDLN2 	Push loge2 onto the FPU register stack.
FLDPI			FLDPI 	Push π onto the FPU register stack.
FLDZ			FLDZ 	Push +0.0 onto the FPU register stack.
FMUL	M32FP		FMUL M32FP	Multiply ST(0) by m32fp and store result in ST(0).
FMUL	M64FP		FMUL M64FP	Multiply ST(0) by m64fp and store result in ST(0).
FMUL	ST(0),ST(I)		FMUL ST(0),ST(I)	Multiply ST(0) by ST(i) and store result in ST(0).
FMUL	ST(I),ST(0)		FMUL ST(I),ST(0)	Multiply ST(i) by ST(0) and store result in ST(i).
FMULP			FMULP 	Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack.
FMULP	ST(I),ST(0)		FMULP ST(I),ST(0)	Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack.
FNCLEX			FNCLEX 	Clear floating-point exception flags without checking for pending unmasked floating-point exceptions.
FNINIT			FNINIT 	Initialize FPU without checking for pending unmasked floating-point exceptions.
FNOP			FNOP 	No operation is performed.
FNSAVE	M94/108BYTE		FNSAVE M94/108BYTE	Store FPU environment to m94byte or m108byte without checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
FNSTCW	M2BYTE		FNSTCW M2BYTE	Store FPU control word to m2byte without checking for pending unmasked floating-point exceptions.
FNSTENV	M14/28BYTE		FNSTENV M14/28BYTE	Store FPU environment to m14byte or m28byte without checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
FNSTSW	AX		FNSTSW AX	Store FPU status word in AX register without checking for pending unmasked floating-point exceptions.
FNSTSW	M2BYTE		FNSTSW M2BYTE	Store FPU status word at m2byte without checking for pending unmasked floating-point exceptions.
FPATAN			FPATAN 	Replace ST(1) with arctan(ST(1)/ST(0)) and pop the register stack.
FPREM			FPREM 	Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1).
FPREM1			FPREM1 	Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1).
FPTAN			FPTAN 	Replace ST(0) with its approximate tangent and push 1 onto the FPU stack.
FRNDINT			FRNDINT 	Round ST(0) to an integer.
FRSTOR	M94/108BYTE		FRSTOR M94/108BYTE	Load FPU state from m94byte or m108byte.
FSAVE	M94/108BYTE		FSAVE M94/108BYTE	Store FPU state to m94byte or m108byte after checking for pending unmasked floating-point exceptions. Then re-initialize the FPU.
FSCALE			FSCALE 	Scale ST(0) by ST(1).
FSIN			FSIN 	Replace ST(0) with the approximate of its sine.
FSINCOS			FSINCOS 	Compute the sine and cosine of ST(0); replace ST(0) with the approximate sine, and push the approximate cosine onto the register stack.
FSQRT			FSQRT 	Computes square root of ST(0) and stores the result in ST(0).
FST	M32FP		FST M32FP	Copy ST(0) to m32fp.
FST	M64FP		FST M64FP	Copy ST(0) to m64fp.
FST	ST(I)		FST ST(I)	Copy ST(0) to ST(i).
FSTCW	M2BYTE		FSTCW M2BYTE	Store FPU control word to m2byte after checking for pending unmasked floating-point exceptions.
FSTENV	M14/28BYTE		FSTENV M14/28BYTE	Store FPU environment to m14byte or m28byte after checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions.
FSTP	M32FP		FSTP M32FP	Copy ST(0) to m32fp and pop register stack.
FSTP	M64FP		FSTP M64FP	Copy ST(0) to m64fp and pop register stack.
FSTP	M80FP		FSTP M80FP	Copy ST(0) to m80fp and pop register stack.
FSTP	ST(I)		FSTP ST(I)	Copy ST(0) to ST(i) and pop register stack.
FSTSW	AX		FSTSW AX	Store FPU status word in AX register after checking for pending unmasked floating-point exceptions.
FSTSW	M2BYTE		FSTSW M2BYTE	Store FPU status word at m2byte after checking for pending unmasked floating-point exceptions.
FSUB	M32FP		FSUB M32FP	Subtract m32fp from ST(0) and store result in ST(0).
FSUB	M64FP		FSUB M64FP	Subtract m64fp from ST(0) and store result in ST(0).
FSUB	ST(0),ST(I)		FSUB ST(0),ST(I)	Subtract ST(i) from ST(0) and store result in ST(0).
FSUB	ST(I),ST(0)		FSUB ST(I),ST(0)	Subtract ST(0) from ST(i) and store result in ST(i).
FSUBP			FSUBP 	Subtract ST(0) from ST(1), store result in ST(1), and pop register stack.
FSUBP	ST(I),ST(0)		FSUBP ST(I),ST(0)	Subtract ST(0) from ST(i), store result in ST(i), and pop register stack.
FSUBR	M32FP		FSUBR M32FP	Subtract ST(0) from m32fp and store result in ST(0).
FSUBR	M64FP		FSUBR M64FP	Subtract ST(0) from m64fp and store result in ST(0).
FSUBR	ST(0),ST(I)		FSUBR ST(0),ST(I)	Subtract ST(0) from ST(i) and store result in ST(0).
FSUBR	ST(I),ST(0)		FSUBR ST(I),ST(0)	Subtract ST(i) from ST(0) and store result in ST(i).
FSUBRP			FSUBRP 	Subtract ST(1) from ST(0), store result in ST(1), and pop register stack.
FSUBRP	ST(I),ST(0)		FSUBRP ST(I),ST(0)	Subtract ST(i) from ST(0), store result in ST(i), and pop register stack.
FTST			FTST 	Compare ST(0) with 0.0.
FUCOM			FUCOM 	Compare ST(0) with ST(1).
FUCOM	ST(I)		FUCOM ST(I)	Compare ST(0) with ST(i).
FUCOMIP	ST,ST(I)		FUCOMIP ST,ST(I)	Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack.
FUCOMP			FUCOMP 	Compare ST(0) with ST(1) and pop register stack.
FUCOMP	ST(I)		FUCOMP ST(I)	Compare ST(0) with ST(i) and pop register stack.
FUCOMPP			FUCOMPP 	Compare ST(0) with ST(1) and pop register stack twice.
FWAIT			FWAIT 	Check pending unmasked floating-point exceptions.
FXAM			FXAM 	Classify value or number in ST(0).
FXCH			FXCH 	Exchange the contents of ST(0) and ST(1).
FXCH	ST(I)		FXCH ST(I)	Exchange the contents of ST(0) and ST(i).
FXRSTOR	M512BYTE		FXRSTOR M512BYTE	Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte.
FXSAVE	M512BYTE		FXSAVE M512BYTE	Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte.
FXTRACT			FXTRACT 	Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack.
FYL2X			FYL2X 	Replace ST(1) with (ST(1) ∗ log2ST(0)) and pop the register stack.
FYL2XP1			FYL2XP1 	Replace ST(1) with ST(1) ∗ log2(ST(0) + 1.0) and pop the register stack.
HADDPD	XMM,XMM/M128	SSE3	HADDPD XMM1,XMM2/M128	Horizontal add packed double-precision floating-point values from xmm2/m128 to xmm1.
HADDPS	XMM,XMM/M128	SSE3	HADDPS XMM1,XMM2/M128	Horizontal add packed single-precision floating-point values from xmm2/m128 to xmm1.
HLT			HLT 	Halt
HSUBPD	XMM,XMM/M128	SSE3	HSUBPD XMM1,XMM2/M128	Horizontal subtract packed double-precision floating-point values from xmm2/m128 to xmm1.
HSUBPS	XMM,XMM/M128	SSE3	HSUBPS XMM1,XMM2/M128	Horizontal subtract packed single-precision floating-point values from xmm2/m128 to xmm1.
IDIV	R/M16		IDIV R/M16	Signed divide DX:AX by r/m16, with result stored in AX ← Quotient, DX ← Remainder.
IDIV	R/M32		IDIV R/M32	Signed divide EDX:EAX by r/m32, with result stored in EAX ← Quotient, EDX ← Remainder.
IDIV	R/M64		IDIV R/M64	Signed divide RDX:RAX by r/m64, with result stored in RAX ← Quotient, RDX ← Remainder.
IDIV	R/M8		IDIV R/M8	Signed divide AX by r/m8, with result stored in: AL ← Quotient, AH ← Remainder.
IMUL	R/M16		IMUL R/M16	DX:AX ← AX ∗ r/m word.
IMUL	R/M32		IMUL R/M32	EDX:EAX ← EAX ∗ r/m32.
IMUL	R/M64		IMUL R/M64	RDX:RAX ← RAX ∗ r/m64.
IMUL	R/M8		IMUL R/M8	AX← AL ∗ r/m byte.
IMUL	R16,R/M16		IMUL R16,R/M16	word register ← word register ∗ r/m16.
IMUL	R16,R/M16,IMM16		IMUL R16,R/M16,IMM16	word register ← r/m16 ∗ immediate word.
IMUL	R16,R/M16,IMM8		IMUL R16,R/M16,IMM8	word register ← r/m16 ∗ sign-extended immediate byte.
IMUL	R32,R/M32		IMUL R32,R/M32	doubleword register ← doubleword register ∗ r/m32.
IMUL	R32,R/M32,IMM32		IMUL R32,R/M32,IMM32	doubleword register ← r/m32 ∗ immediate doubleword.
IMUL	R32,R/M32,IMM8		IMUL R32,R/M32,IMM8	doubleword register ← r/m32 ∗ sign-extended immediate byte.
IMUL	R64,R/M64		IMUL R64,R/M64	Quadword register ← Quadword register ∗ r/m64.
IMUL	R64,R/M64,IMM32		IMUL R64,R/M64,IMM32	Quadword register ← r/m64 ∗ immediate doubleword.
IMUL	R64,R/M64,IMM8		IMUL R64,R/M64,IMM8	Quadword register ← r/m64 ∗ sign-extended immediate byte.
IN	AL,DX		IN AL,DX	Input byte from I/O port in DX into AL.
IN	AL,IMM8		IN AL,IMM8	Input byte from imm8 I/O port address into AL.
IN	AX,DX		IN AX,DX	Input word from I/O port in DX into AX.
IN	AX,IMM8		IN AX,IMM8	Input word from imm8 I/O port address into AX.
IN	EAX,DX		IN EAX,DX	Input doubleword from I/O port in DX into EAX.
IN	EAX,IMM8		IN EAX,IMM8	Input dword from imm8 I/O port address into EAX.
INC	R/M16		INC R/M16	Increment r/m word by 1.
INC	R/M32		INC R/M32	Increment r/m doubleword by 1.
INC	R/M64		INC R/M64	Increment r/m quadword by 1.
INC	R/M8		INC R/M8	Increment r/m byte by 1.
INC	R16		INC R16	Increment word register by 1.
INC	R32		INC R32	Increment doubleword register by 1.
INS	M16,DX		INS M16,DX	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
INS	M32,DX		INS M32,DX	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
INS	M8,DX		INS M8,DX	Input byte from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.
INSB			INSB 	Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI.1
INSD			INSD 	Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
INSERTPS	XMM,XMM/M32,IMM8	SSE4_1	INSERTPS XMM1,XMM2/M32,IMM8	Insert a single-precision floating-point value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8.
INSW			INSW 	Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI.1
INTO			INTO 	Interrupt 4—if overflow flag is 1.
INVD			INVD 	Flush internal caches; initiate flushing of external caches.
INVLPG	MEM		INVLPG M	Invalidate TLB entries for page containing m.
INVPCID	R32,M128	INVPCID	INVPCID R32,M128	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descrip-tor in m128.
INVPCID	R64,M128	INVPCID	INVPCID R64,M128	Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descrip-tor in m128.
IRET			IRET 	Interrupt return (16-bit operand size).
IRETD			IRETD 	Interrupt return (32-bit operand size).

JCXZ	REL8		JCXZ REL8	Jump short if CX register is 0.
JECXZ	REL8		JECXZ REL8	Jump short if ECX register is 0.

JMP	M16	8086	JMP M16:16	Jump far, absolute indirect, address given in M16
JMP	M32	386	JMP M16:32	Jump far, absolute indirect, address given in M32.
JMP	M64	X64	JMP M16:64	Jump far, absolute indirect, address given in M64.

JMP	R/M16	8086	JMP R/M16	Jump near, absolute indirect, address = zero-extended r/m16. Not supported in 64-bit mode.
JMP	R/M32	386	JMP R/M32	Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode.
JMP	R/M64	X64	JMP R/M64	Jump near, absolute indirect, RIP = 64-Bit offset from register or memory

JMP	REL8	8086	JMP REL8	Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits
JMP	REL16	8086	JMP REL16	Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode.
JMP	REL32	386	JMP REL32	Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits
;
CMOVA	R16,R/M16	P6	CMOVA R16, r/m16	Move if above (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVAE	R16,R/M16	P6	CMOVAE R16, r/m16	Move if above or equal (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVB	R16,R/M16	P6	CMOVB R16, r/m16	Move if below (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVBE	R16,R/M16	P6	CMOVBE R16, r/m16	Move if below or equal (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVE	R16,R/M16	P6	CMOVE R16, r/m16	Move if equal (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html
CMOVG	R16,R/M16	P6	CMOVG R16, r/m16	Move if greater (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVGE	R16,R/M16	P6	CMOVGE R16, r/m16	Move if greater or equal (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVL	R16,R/M16	P6	CMOVL R16, r/m16	Move if less (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVLE	R16,R/M16	P6	CMOVLE R16, r/m16	Move if less or equal (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNA	R16,R/M16	P6	CMOVNA R16, r/m16	Move if not above (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVNAE	R16,R/M16	P6	CMOVNAE R16, r/m16	Move if not above or equal (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVNB	R16,R/M16	P6	CMOVNB R16, r/m16	Move if not below (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNBE	R16,R/M16	P6	CMOVNBE R16, r/m16	Move if not below or equal (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVNC	R16,R/M16	P6	CMOVNC R16, r/m16	Move if not carry (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNE	R16,R/M16	P6	CMOVNE R16, r/m16	Move if not equal (ZF=0) (CMOVNE=CMOVNZ)	CMOVcc.html
CMOVNG	R16,R/M16	P6	CMOVNG R16, r/m16	Move if not greater (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNGE	R16,R/M16	P6	CMOVNGE R16, r/m16	Move if not greater or equal (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVNL	R16,R/M16	P6	CMOVNL R16, r/m16	Move if not less (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVNLE	R16,R/M16	P6	CMOVNLE R16, r/m16	Move if not less or equal (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVNO	R16,R/M16	P6	CMOVNO R16, r/m16	Move if not overflow (OF=0)	CMOVcc.html
CMOVNP	R16,R/M16	P6	CMOVNP R16, r/m16	Move if not parity (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVNS	R16,R/M16	P6	CMOVNS R16, r/m16	Move if not sign (SF=0)	CMOVcc.html
CMOVNZ	R16,R/M16	P6	CMOVNZ R16, r/m16	Move if not zero (ZF=0 CMOVNE=CMOVNZ)	CMOVcc.html
CMOVO	R16,R/M16	P6	CMOVO R16, r/m16	Move if overflow (OF=1)	CMOVcc.html
CMOVP	R16,R/M16	P6	CMOVP R16, r/m16	Move if parity (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPE	R16,R/M16	P6	CMOVPE R16, r/m16	Move if parity even (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPO	R16,R/M16	P6	CMOVPO R16, r/m16	Move if parity odd (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVS	R16,R/M16	P6	CMOVS R16, r/m16	Move if sign (SF=1)	CMOVcc.html
CMOVZ	R16,R/M16	P6	CMOVZ R16, r/m16	Move if zero (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html
;
CMOVA	R32,R/M32	P6	CMOVA R32, R/M32	Move if above (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVAE	R32,R/M32	P6	CMOVAE R32, R/M32	Move if above or equal (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVB	R32,R/M32	P6	CMOVB R32, R/M32	Move if below (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVBE	R32,R/M32	P6	CMOVBE R32, R/M32	Move if below or equal (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVE	R32,R/M32	P6	CMOVE R32, R/M32	Move if equal (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html
CMOVG	R32,R/M32	P6	CMOVG R32, R/M32	Move if greater (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVGE	R32,R/M32	P6	CMOVGE R32, R/M32	Move if greater or equal (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVL	R32,R/M32	P6	CMOVL R32, R/M32	Move if less (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVLE	R32,R/M32	P6	CMOVLE R32, R/M32	Move if less or equal (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNA	R32,R/M32	P6	CMOVNA R32, R/M32	Move if not above (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVNAE	R32,R/M32	P6	CMOVNAE R32, R/M32	Move if not above or equal (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVNB	R32,R/M32	P6	CMOVNB R32, R/M32	Move if not below (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNBE	R32,R/M32	P6	CMOVNBE R32, R/M32	Move if not below or equal (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVNC	R32,R/M32	P6	CMOVNC R32, R/M32	Move if not carry (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNE	R32,R/M32	P6	CMOVNE R32, R/M32	Move if not equal (ZF=0) (CMOVNE=CMOVNZ)	CMOVcc.html
CMOVNG	R32,R/M32	P6	CMOVNG R32, R/M32	Move if not greater (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNGE	R32,R/M32	P6	CMOVNGE R32, R/M32	Move if not greater or equal (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVNL	R32,R/M32	P6	CMOVNL R32, R/M32	Move if not less (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVNLE	R32,R/M32	P6	CMOVNLE R32, R/M32	Move if not less or equal (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVNO	R32,R/M32	P6	CMOVNO R32, R/M32	Move if not overflow (OF=0)	CMOVcc.html
CMOVNP	R32,R/M32	P6	CMOVNP R32, R/M32	Move if not parity (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVNS	R32,R/M32	P6	CMOVNS R32, R/M32	Move if not sign (SF=0)	CMOVcc.html
CMOVNZ	R32,R/M32	P6	CMOVNZ R32, R/M32	Move if not zero (ZF=0 CMOVNE=CMOVNZ)	CMOVcc.html
CMOVO	R32,R/M32	P6	CMOVO R32, R/M32	Move if overflow (OF=1)	CMOVcc.html
CMOVP	R32,R/M32	P6	CMOVP R32, R/M32	Move if parity (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPE	R32,R/M32	P6	CMOVPE R32, R/M32	Move if parity even (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPO	R32,R/M32	P6	CMOVPO R32, R/M32	Move if parity odd (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVS	R32,R/M32	P6	CMOVS R32, R/M32	Move if sign (SF=1)	CMOVcc.html
CMOVZ	R32,R/M32	P6	CMOVZ R32, R/M32	Move if zero (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html
;
CMOVA	R64,R/M64	X64	CMOVA r64, r/m64	Move if above (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVAE	R64,R/M64	X64	CMOVAE r64, r/m64	Move if above or equal (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVB	R64,R/M64	X64	CMOVB r64, r/m64	Move if below (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVBE	R64,R/M64	X64	CMOVBE r64, r/m64	Move if below or equal (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVE	R64,R/M64	X64	CMOVE r64, r/m64	Move if equal (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html
CMOVG	R64,R/M64	X64	CMOVG r64, r/m64	Move if greater (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVGE	R64,R/M64	X64	CMOVGE r64, r/m64	Move if greater or equal (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVL	R64,R/M64	X64	CMOVL r64, r/m64	Move if less (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVLE	R64,R/M64	X64	CMOVLE r64, r/m64	Move if less or equal (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNA	R64,R/M64	X64	CMOVNA r64, r/m64	Move if not above (CF=1 or ZF=1) (CMOVBE=CMOVNA)	CMOVcc.html
CMOVNAE	R64,R/M64	X64	CMOVNAE r64, r/m64	Move if not above or equal (CF=1) (CMOVB=CMOVC=CMOVNAE)	CMOVcc.html
CMOVNB	R64,R/M64	X64	CMOVNB r64, r/m64	Move if not below (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNBE	R64,R/M64	X64	CMOVNBE r64, r/m64	Move if not below or equal (CF=0 and ZF=0) (CMOVA=CMOVNBE)	CMOVcc.html
CMOVNC	R64,R/M64	X64	CMOVNC r64, r/m64	Move if not carry (CF=0) (CMOVAE=CMOVNB=CMOVNC)	CMOVcc.html
CMOVNE	R64,R/M64	X64	CMOVNE r64, r/m64	Move if not equal (ZF=0) (CMOVNE=CMOVNZ)	CMOVcc.html
CMOVNG	R64,R/M64	X64	CMOVNG r64, r/m64	Move if not greater (ZF=1 or SF!=OF) (CMOVLE=CMOVNG)	CMOVcc.html
CMOVNGE	R64,R/M64	X64	CMOVNGE r64, r/m64	Move if not greater or equal (SF!=OF) (CMOVL=CMOVNGE)	CMOVcc.html
CMOVNL	R64,R/M64	X64	CMOVNL r64, r/m64	Move if not less (SF=OF) (CMOVGE=CMOVNL)	CMOVcc.html
CMOVNLE	R64,R/M64	X64	CMOVNLE r64, r/m64	Move if not less or equal (ZF=0 and SF=OF) (CMOVG=CMOVNLE)	CMOVcc.html
CMOVNO	R64,R/M64	X64	CMOVNO r64, r/m64	Move if not overflow (OF=0)	CMOVcc.html
CMOVNP	R64,R/M64	X64	CMOVNP r64, r/m64	Move if not parity (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVNS	R64,R/M64	X64	CMOVNS r64, r/m64	Move if not sign (SF=0)	CMOVcc.html
CMOVNZ	R64,R/M64	X64	CMOVNZ r64, r/m64	Move if not zero (ZF=0 CMOVNE=CMOVNZ)	CMOVcc.html
CMOVO	R64,R/M64	X64	CMOVO r64, r/m64	Move if overflow (OF=1)	CMOVcc.html
CMOVP	R64,R/M64	X64	CMOVP r64, r/m64	Move if parity (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPE	R64,R/M64	X64	CMOVPE r64, r/m64	Move if parity even (PF=1) (CMOVP=CMOVPE)	CMOVcc.html
CMOVPO	R64,R/M64	X64	CMOVPO r64, r/m64	Move if parity odd (PF=0) (CMOVNP=CMOVPO)	CMOVcc.html
CMOVS	R64,R/M64	X64	CMOVS r64, r/m64	Move if sign (SF=1)	CMOVcc.html
CMOVZ	R64,R/M64	X64	CMOVZ r64, r/m64	Move if zero (ZF=1) (CMOVE=CMOVZ)	CMOVcc.html

;
JA	IMM8	8086	JA IMM8	Jump if above (CF=0 and ZF=0) (JA=JNBE)
JAE	IMM8	8086	JAE IMM8	Jump if above or equal (CF=0) (JAE=JNB)
JB	IMM8	8086	JB IMM8	Jump if below (CF=1) (JB=JNAE)
JBE	IMM8	8086	JBE IMM8	Jump if below or equal (CF=1 or ZF=1) (JBE=JNA)
JC	IMM8	8086	JC IMM8	Jump if carry (CF=1)
JE	IMM8	8086	JE IMM8	Jump if equal (ZF=1) (JE=JZ)
JG	IMM8	8086	JG IMM8	Jump if greater (ZF=0 and SF=OF) (JG=JNLE)
JGE	IMM8	8086	JGE IMM8	Jump if greater or equal (SF=OF) (JGE=JNL)
JL	IMM8	8086	JL IMM8	Jump if less (SF!=OF) (JL=JNGE)
JLE	IMM8	8086	JLE IMM8	Jump if less or equal (ZF=1 or SF!=OF) (JLE=JNG)
JNA	IMM8	8086	JNA IMM8	Jump if not above (CF=1 or ZF=1) (JBE=JNA)
JNAE	IMM8	8086	JNAE IMM8	Jump if not above or equal (CF=1) (JB=JNAE)
JNB	IMM8	8086	JNB IMM8	Jump if not below (CF=0) (JAE=JNB)
JNBE	IMM8	8086	JNBE IMM8	Jump if not below or equal (CF=0 and ZF=0) (JA=JNBE)
JNC	IMM8	8086	JNC IMM8	Jump if not carry (CF=0)
JNE	IMM8	8086	JNE IMM8	Jump if not equal (ZF=0) (JNE=JNZ)
JNG	IMM8	8086	JNG IMM8	Jump if not greater (ZF=1 or SF!=OF) (JLE=JNG)
JNGE	IMM8	8086	JNGE IMM8	Jump if not greater or equal (SF!=OF) (JL=JNGE)
JNL	IMM8	8086	JNL IMM8	Jump if not less (SF=OF) (JGE=JNL)
JNLE	IMM8	8086	JNLE IMM8	Jump if not less or equal (ZF=0 and SF=OF) (JG=JNLE)
JNO	IMM8	8086	JNO IMM8	Jump if not overflow (OF=0)
JNP	IMM8	8086	JNP IMM8	Jump if not parity (PF=0) (JNP=JPO)
JNS	IMM8	8086	JNS IMM8	Jump if not sign (SF=0)
JNZ	IMM8	8086	JNZ IMM8	Jump if not zero (ZF=0) (JNE=JNZ)
JO	IMM8	8086	JO IMM8	Jump if overflow (OF=1)
JP	IMM8	8086	JP IMM8	Jump if parity (PF=1) (JP=JPE)
JPE	IMM8	8086	JPE IMM8	Jump if parity even (PF=1) (JP=JPE)
JPO	IMM8	8086	JPO IMM8	Jump if parity odd (PF=0) (JNP=JPO)
JS	IMM8	8086	JS IMM8	Jump if sign (SF=1)
JZ	IMM8	8086	JZ IMM8	Jump if zero (ZF=1) (JE=JZ)
;
JA	IMM	386	JA IMM	Jump if above (CF=0 and ZF=0) (JA=JNBE)
JAE	IMM	386	JAE IMM	Jump if above or equal (CF=0) (JAE=JNB)
JB	IMM	386	JB IMM	Jump if below (CF=1) (JB=JNAE)
JBE	IMM	386	JBE IMM	Jump if below or equal (CF=1 or ZF=1) (JBE=JNA)
JC	IMM	386	JC IMM	Jump if carry (CF=1)
JE	IMM	386	JE IMM	Jump if equal (ZF=1) (JE=JZ)
JG	IMM	386	JG IMM	Jump if greater (ZF=0 and SF=OF) (JG=JNLE)
JGE	IMM	386	JGE IMM	Jump if greater or equal (SF=OF) (JGE=JNL)
JL	IMM	386	JL IMM	Jump if less (SF!=OF) (JL=JNGE)
JLE	IMM	386	JLE IMM	Jump if less or equal (ZF=1 or SF!=OF) (JLE=JNG)
JNA	IMM	386	JNA IMM	Jump if not above (CF=1 or ZF=1) (JBE=JNA)
JNAE	IMM	386	JNAE IMM	Jump if not above or equal (CF=1) (JB=JNAE)
JNB	IMM	386	JNB IMM	Jump if not below (CF=0) (JAE=JNB)
JNBE	IMM	386	JNBE IMM	Jump if not below or equal (CF=0 and ZF=0) (JA=JNBE)
JNC	IMM	386	JNC IMM	Jump if not carry (CF=0)
JNE	IMM	386	JNE IMM	Jump if not equal (ZF=0) (JNE=JNZ)
JNG	IMM	386	JNG IMM	Jump if not greater (ZF=1 or SF!=OF) (JLE=JNG)
JNGE	IMM	386	JNGE IMM	Jump if not greater or equal (SF!=OF) (JL=JNGE)
JNL	IMM	386	JNL IMM	Jump if not less (SF=OF) (JGE=JNL)
JNLE	IMM	386	JNLE IMM	Jump if not less or equal (ZF=0 and SF=OF) (JG=JNLE)
JNO	IMM	386	JNO IMM	Jump if not overflow (OF=0)
JNP	IMM	386	JNP IMM	Jump if not parity (PF=0) (JNP=JPO)
JNS	IMM	386	JNS IMM	Jump if not sign (SF=0)
JNZ	IMM	386	JNZ IMM	Jump if not zero (ZF=0) (JNE=JNZ)
JO	IMM	386	JO IMM	Jump if overflow (OF=1)
JP	IMM	386	JP IMM	Jump if parity (PF=1) (JP=JPE)
JPE	IMM	386	JPE IMM	Jump if parity even (PF=1) (JP=JPE)
JPO	IMM	386	JPO IMM	Jump if parity odd (PF=0) (JNP=JPO)
JS	IMM	386	JS IMM	Jump if sign (SF=1)
JZ	IMM	386	JZ IMM	Jump if zero (ZF=1) (JE=JZ)
;
JA	IMM64	X64	JA IMM64	Jump if above (CF=0 and ZF=0) (JA=JNBE)
JAE	IMM64	X64	JAE IMM64	Jump if above or equal (CF=0) (JAE=JNB)
JB	IMM64	X64	JB IMM64	Jump if below (CF=1) (JB=JNAE)
JBE	IMM64	X64	JBE IMM64	Jump if below or equal (CF=1 or ZF=1) (JBE=JNA)
JC	IMM64	X64	JC IMM64	Jump if carry (CF=1)
JE	IMM64	X64	JE IMM64	Jump if equal (ZF=1) (JE=JZ)
JG	IMM64	X64	JG IMM64	Jump if greater (ZF=0 and SF=OF) (JG=JNLE)
JGE	IMM64	X64	JGE IMM64	Jump if greater or equal (SF=OF) (JGE=JNL)
JL	IMM64	X64	JL IMM64	Jump if less (SF!=OF) (JL=JNGE)
JLE	IMM64	X64	JLE IMM64	Jump if less or equal (ZF=1 or SF!=OF) (JLE=JNG)
JNA	IMM64	X64	JNA IMM64	Jump if not above (CF=1 or ZF=1) (JBE=JNA)
JNAE	IMM64	X64	JNAE IMM64	Jump if not above or equal (CF=1) (JB=JNAE)
JNB	IMM64	X64	JNB IMM64	Jump if not below (CF=0) (JAE=JNB)
JNBE	IMM64	X64	JNBE IMM64	Jump if not below or equal (CF=0 and ZF=0) (JA=JNBE)
JNC	IMM64	X64	JNC IMM64	Jump if not carry (CF=0)
JNE	IMM64	X64	JNE IMM64	Jump if not equal (ZF=0) (JNE=JNZ)
JNG	IMM64	X64	JNG IMM64	Jump if not greater (ZF=1 or SF!=OF) (JLE=JNG)
JNGE	IMM64	X64	JNGE IMM64	Jump if not greater or equal (SF!=OF) (JL=JNGE)
JNL	IMM64	X64	JNL IMM64	Jump if not less (SF=OF) (JGE=JNL)
JNLE	IMM64	X64	JNLE IMM64	Jump if not less or equal (ZF=0 and SF=OF) (JG=JNLE)
JNO	IMM64	X64	JNO IMM64	Jump if not overflow (OF=0)
JNP	IMM64	X64	JNP IMM64	Jump if not parity (PF=0) (JNP=JPO)
JNS	IMM64	X64	JNS IMM64	Jump if not sign (SF=0)
JNZ	IMM64	X64	JNZ IMM64	Jump if not zero (ZF=0) (JNE=JNZ)
JO	IMM64	X64	JO IMM64	Jump if overflow (OF=1)
JP	IMM64	X64	JP IMM64	Jump if parity (PF=1) (JP=JPE)
JPE	IMM64	X64	JPE IMM64	Jump if parity even (PF=1) (JP=JPE)
JPO	IMM64	X64	JPO IMM64	Jump if parity odd (PF=0) (JNP=JPO)
JS	IMM64	X64	JS IMM64	Jump if sign (SF=1)
JZ	IMM64	X64	JZ IMM64	Jump if zero (ZF=1) (JE=JZ)
;
SETA	R/M8	386	SETA r/m8	Set byte if above (CF=0 and ZF=0) (SETA=SETNBE)
SETAE	R/M8	386	SETAE r/m8	Set byte if above or equal (CF=0) (SETAE=SETNC=SETNB)
SETB	R/M8	386	SETB r/m8	Set byte if below (CF=1) (SETB=SETC=SETNAE)
SETBE	R/M8	386	SETBE r/m8	Set byte if below or equal (CF=1 or ZF=1) (SETBE=SETNA)
SETC	R/M8	386	SETC r/m8	Set byte if carry (CF=1 SETB=SETC=SETNAE)
SETE	R/M8	386	SETE r/m8	Set byte if equal (ZF=1) (SETE=SETZ)
SETG	R/M8	386	SETG r/m8	Set byte if greater (ZF=0 and SF=OF) (SETG=SETNLE)
SETGE	R/M8	386	SETGE r/m8	Set byte if greater or equal (SF=OF) (SETGE=SETNL)
SETL	R/M8	386	SETL r/m8	Set byte if less (SF!=OF) (SETL=SETNGE)
SETLE	R/M8	386	SETLE r/m8	Set byte if less or equal (ZF=1 or SF!=OF) (SETLE=SETNG)
SETNA	R/M8	386	SETNA r/m8	Set byte if not above (CF=1 or ZF=1) (SETBE=SETNA)
SETNAE	R/M8	386	SETNAE r/m8	Set byte if not above or equal (CF=1) (SETB=SETC=SETNAE)
SETNB	R/M8	386	SETNB r/m8	Set byte if not below (CF=0) (SETAE=SETNC=SETNB)
SETNBE	R/M8	386	SETNBE r/m8	Set byte if not below or equal (CF=0 and ZF=0) (SETA=SETNBE)
SETNC	R/M8	386	SETNC r/m8	Set byte if not carry (CF=0) (SETAE=SETNC=SETNB)
SETNE	R/M8	386	SETNE r/m8	Set byte if not equal (ZF=0) (SETNE=SETNZ)
SETNG	R/M8	386	SETNG r/m8	Set byte if not greater (ZF=1 or SF!=OF) (SETLE=SETNG)
SETNGE	R/M8	386	SETNGE r/m8	Set byte if not greater or equal (SF!=OF) (SETL=SETNGE)
SETNL	R/M8	386	SETNL r/m8	Set byte if not less (SF=OF) (SETGE=SETNL)
SETNLE	R/M8	386	SETNLE r/m8	Set byte if not less or equal (ZF=0 and SF=OF) (SETG=SETNLE)
SETNO	R/M8	386	SETNO r/m8	Set byte if not overflow (OF=0)
SETNP	R/M8	386	SETNP r/m8	Set byte if not parity (PF=0) (SETNP=SETPO)
SETNS	R/M8	386	SETNS r/m8	Set byte if not sign (SF=0)
SETNZ	R/M8	386	SETNZ r/m8	Set byte if not zero (ZF=0) (SETNE=SETNZ)
SETO	R/M8	386	SETO r/m8	Set byte if overflow (OF=1)
SETP	R/M8	386	SETP r/m8	Set byte if parity (PF=1) (SETP=SETPE)
SETPE	R/M8	386	SETPE r/m8	Set byte if parity even (PF=1) (SETP=SETPE)
SETPO	R/M8	386	SETPO r/m8	Set byte if parity odd (PF=0 SETNP=SETPO)
SETS	R/M8	386	SETS r/m8	Set byte if sign (SF=1)
SETZ	R/M8	386	SETZ r/m8	Set byte if zero (ZF=1) (SETE=SETZ)




JNA	REL8		JNA REL8	Jump short if not above (CF=1 or ZF=1).
JNAE	REL8		JNAE REL8	Jump short if not above or equal (CF=1).
JNB	REL8		JNB REL8	Jump short if not below (CF=0).
JNBE	REL8		JNBE REL8	Jump short if not below or equal (CF=0 and ZF=0).
JNC	REL8		JNC REL8	Jump short if not carry (CF=0).
JNE	REL8		JNE REL8	Jump short if not equal (ZF=0).
JNG	REL8		JNG REL8	Jump short if not greater (ZF=1 or SF≠ OF).
JNGE	REL8		JNGE REL8	Jump short if not greater or equal (SF≠ OF).
JNL	REL8		JNL REL8	Jump short if not less (SF=OF).
JNLE	REL8		JNLE REL8	Jump short if not less or equal (ZF=0 and SF=OF).
JNO	REL8		JNO REL8	Jump short if not overflow (OF=0).
JNP	REL8		JNP REL8	Jump short if not parity (PF=0).
JNS	REL8		JNS REL8	Jump short if not sign (SF=0).
JNZ	REL8		JNZ REL8	Jump short if not zero (ZF=0).
JO	REL8		JO REL8	Jump short if overflow (OF=1).
JP	REL8		JP REL8	Jump short if parity (PF=1).
JPE	REL8		JPE REL8	Jump short if parity even (PF=1).
JPO	REL8		JPO REL8	Jump short if parity odd (PF=0).
JRCXZ	REL8		JRCXZ REL8	Jump short if RCX register is 0.
JS	REL8		JS REL8	Jump short if sign (SF=1).
JZ	REL8		JZ REL8	Jump short if zero (ZF = 1).
KADDB	K,K,K	AVX512DQ	KADDB K1,K2,K3	Add 8 bits masks in k2 and k3 and place result in k1.
KADDD	K,K,K	AVX512BW	KADDD K1,K2,K3	Add 32 bits masks in k2 and k3 and place result in k1.
KADDQ	K,K,K	AVX512BW	KADDQ K1,K2,K3	Add 64 bits masks in k2 and k3 and place result in k1.
KADDW	K,K,K	AVX512DQ	KADDW K1,K2,K3	Add 16 bits masks in k2 and k3 and place result in k1.
KANDB	K,K,K	AVX512DQ	KANDB K1,K2,K3	Bitwise AND 8 bits masks k2 and k3 and place result in k1.
KANDD	K,K,K	AVX512BW	KANDD K1,K2,K3	Bitwise AND 32 bits masks k2 and k3 and place result in k1.
KANDNB	K,K,K	AVX512DQ	KANDNB K1,K2,K3	Bitwise AND NOT 8 bits masks k1 and k2 and place result in k1.
KANDND	K,K,K	AVX512BW	KANDND K1,K2,K3	Bitwise AND NOT 32 bits masks k2 and k3 and place result in k1.
KANDNQ	K,K,K	AVX512BW	KANDNQ K1,K2,K3	Bitwise AND NOT 64 bits masks k2 and k3 and place result in k1.
KANDNW	K,K,K	AVX512F	KANDNW K1,K2,K3	Bitwise AND NOT 16 bits masks k2 and k3 and place result in k1.
KANDQ	K,K,K	AVX512BW	KANDQ K1,K2,K3	Bitwise AND 64 bits masks k2 and k3 and place result in k1.
KANDW	K,K,K	AVX512F	KANDW K1,K2,K3	Bitwise AND 16 bits masks k2 and k3 and place result in k1.
KMOVB	K,K/M8	AVX512DQ	KMOVB K1,K2/M8	Move 8 bits mask from k2/m8 and store the result in k1.
KMOVB	K,R32	AVX512DQ	KMOVB K1,R32	Move 8 bits mask from r32 to k1.
KMOVB	M8,K	AVX512DQ	KMOVB M8,K1	Move 8 bits mask from k1 and store the result in m8.
KMOVB	R32,K	AVX512DQ	KMOVB R32,K1	Move 8 bits mask from k1 to r32.
KMOVD	K,K/M32	AVX512BW	KMOVD K1,K2/M32	Move 32 bits mask from k2/m32 and store the result in k1.
KMOVD	K,R32	AVX512BW	KMOVD K1,R32	Move 32 bits mask from r32 to k1.
KMOVD	M32,K	AVX512BW	KMOVD M32,K1	Move 32 bits mask from k1 and store the result in m32.
KMOVD	R32,K	AVX512BW	KMOVD R32,K1	Move 32 bits mask from k1 to r32.
KMOVQ	K,K/M64	AVX512BW	KMOVQ K1,K2/M64	Move 64 bits mask from k2/m64 and store the result in k1.
KMOVQ	K,R64	AVX512BW	KMOVQ K1,R64	Move 64 bits mask from r64 to k1.
KMOVQ	M64,K	AVX512BW	KMOVQ M64,K1	Move 64 bits mask from k1 and store the result in m64.
KMOVQ	R64,K	AVX512BW	KMOVQ R64,K1	Move 64 bits mask from k1 to r64.
KMOVW	K,K/M16	AVX512F	KMOVW K1,K2/M16	Move 16 bits mask from k2/m16 and store the result in k1.
KMOVW	K,R32	AVX512F	KMOVW K1,R32	Move 16 bits mask from r32 to k1.
KMOVW	M16,K	AVX512F	KMOVW M16,K1	Move 16 bits mask from k1 and store the result in m16.
KMOVW	R32,K	AVX512F	KMOVW R32,K1	Move 16 bits mask from k1 to r32.
KNOTB	K,K	AVX512DQ	KNOTB K1,K2	Bitwise NOT of 8 bits mask k2.
KNOTD	K,K	AVX512BW	KNOTD K1,K2	Bitwise NOT of 32 bits mask k2.
KNOTQ	K,K	AVX512BW	KNOTQ K1,K2	Bitwise NOT of 64 bits mask k2.
KNOTW	K,K	AVX512F	KNOTW K1,K2	Bitwise NOT of 16 bits mask k2.
KORB	K,K,K	AVX512DQ	KORB K1,K2,K3	Bitwise OR 8 bits masks k2 and k3 and place result in k1.
KORD	K,K,K	AVX512BW	KORD K1,K2,K3	Bitwise OR 32 bits masks k2 and k3 and place result in k1.
KORQ	K,K,K	AVX512BW	KORQ K1,K2,K3	Bitwise OR 64 bits masks k2 and k3 and place result in k1.
KORTESTB	K,K	AVX512DQ	KORTESTB K1,K2	Bitwise OR 8 bits masks k1 and k2 and update ZF and CF accordingly.
KORTESTD	K,K	AVX512BW	KORTESTD K1,K2	Bitwise OR 32 bits masks k1 and k2 and update ZF and CF accordingly.
KORTESTQ	K,K	AVX512BW	KORTESTQ K1,K2	Bitwise OR 64 bits masks k1 and k2 and update ZF and CF accordingly.
KORTESTW	K,K	AVX512F	KORTESTW K1,K2	Bitwise OR 16 bits masks k1 and k2 and update ZF and CF accordingly.
KORW	K,K,K	AVX512F	KORW K1,K2,K3	Bitwise OR 16 bits masks k2 and k3 and place result in k1.
KSHIFTLB	K,K,IMM8	AVX512DQ	KSHIFTLB K1,K2,IMM8	Shift left 8 bits in k2 by immediate and write result in k1.
KSHIFTLD	K,K,IMM8	AVX512BW	KSHIFTLD K1,K2,IMM8	Shift left 32 bits in k2 by immediate and write result in k1.
KSHIFTLQ	K,K,IMM8	AVX512BW	KSHIFTLQ K1,K2,IMM8	Shift left 64 bits in k2 by immediate and write result in k1.
KSHIFTLW	K,K,IMM8	AVX512F	KSHIFTLW K1,K2,IMM8	Shift left 16 bits in k2 by immediate and write result in k1.
KSHIFTRB	K,K,IMM8	AVX512DQ	KSHIFTRB K1,K2,IMM8	Shift right 8 bits in k2 by immediate and write result in k1.
KSHIFTRD	K,K,IMM8	AVX512BW	KSHIFTRD K1,K2,IMM8	Shift right 32 bits in k2 by immediate and write result in k1.
KSHIFTRQ	K,K,IMM8	AVX512BW	KSHIFTRQ K1,K2,IMM8	Shift right 64 bits in k2 by immediate and write result in k1.
KSHIFTRW	K,K,IMM8	AVX512F	KSHIFTRW K1,K2,IMM8	Shift right 16 bits in k2 by immediate and write result in k1.
KTESTB	K,K	AVX512DQ	KTESTB K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 8 bits mask register sources.
KTESTD	K,K	AVX512BW	KTESTD K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 32 bits mask register sources.
KTESTQ	K,K	AVX512BW	KTESTQ K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 64 bits mask register sources.
KTESTW	K,K	AVX512DQ	KTESTW K1,K2	Set ZF and CF depending on sign bit AND and ANDN of 16 bits mask register sources.
KUNPCKBW	K,K,K	AVX512F	KUNPCKBW K1,K2,K3	Unpack and interleave 8 bits masks in k2 and k3 and write word result in k1.
KUNPCKDQ	K,K,K	AVX512BW	KUNPCKDQ K1,K2,K3	Unpack and interleave 32 bits masks in k2 and k3 and write quadword result in k1.
KUNPCKWD	K,K,K	AVX512BW	KUNPCKWD K1,K2,K3	Unpack and interleave 16 bits in k2 and k3 and write double-word result in k1.
KXNORB	K,K,K	AVX512DQ	KXNORB K1,K2,K3	Bitwise XNOR 8 bits masks k2 and k3 and place result in k1.
KXNORD	K,K,K	AVX512BW	KXNORD K1,K2,K3	Bitwise XNOR 32 bits masks k2 and k3 and place result in k1.
KXNORQ	K,K,K	AVX512BW	KXNORQ K1,K2,K3	Bitwise XNOR 64 bits masks k2 and k3 and place result in k1.
KXNORW	K,K,K	AVX512F	KXNORW K1,K2,K3	Bitwise XNOR 16 bits masks k2 and k3 and place result in k1.
KXORB	K,K,K	AVX512DQ	KXORB K1,K2,K3	Bitwise XOR 8 bits masks k2 and k3 and place result in k1.
KXORD	K,K,K	AVX512BW	KXORD K1,K2,K3	Bitwise XOR 32 bits masks k2 and k3 and place result in k1.
KXORQ	K,K,K	AVX512BW	KXORQ K1,K2,K3	Bitwise XOR 64 bits masks k2 and k3 and place result in k1.
KXORW	K,K,K	AVX512F	KXORW K1,K2,K3	Bitwise XOR 16 bits masks k2 and k3 and place result in k1.
LAHF			LAHF 	Load: AH ← EFLAGS(SF:ZF:0:AF:0:PF:1:CF).
LAR	R16,R16/M16		LAR R16,R16/M16	r16 ← access rights referenced by r16/m16
LAR	REG,R32/M16		LAR REG,R32/M16	reg ← access rights referenced by r32/m16
LDDQU	XMM,MEM	SSE3	LDDQU XMM1,MEM	Load unaligned data from mem and return double quadword in xmm1.
LDMXCSR	M32	SSE	LDMXCSR M32	Load MXCSR register from m32.
LDS	R16,M16:16		LDS R16,M16:16	Load DS:r16 with far pointer from memory.
LDS	R32,M16:32		LDS R32,M16:32	Load DS:r32 with far pointer from memory.
LEA	R16,MEM		LEA R16,M	Store effective address for m in register r16.
LEA	R32,MEM		LEA R32,M	Store effective address for m in register r32.
LEA	R64,MEM		LEA R64,M	Store effective address for m in register r64.
LEAVE			LEAVE 	Set SP to BP, then pop BP.
LES	R16,M16:16		LES R16,M16:16	Load ES:r16 with far pointer from memory.
LES	R32,M16:32		LES R32,M16:32	Load ES:r32 with far pointer from memory.
LFENCE			LFENCE 	Serializes load operations.
LFS	R16,M16:16		LFS R16,M16:16	Load FS:r16 with far pointer from memory.
LFS	R32,M16:32		LFS R32,M16:32	Load FS:r32 with far pointer from memory.
LFS	R64,M16:64		LFS R64,M16:64	Load FS:r64 with far pointer from memory.
LGDT	M16&32		LGDT M16&32	Load m into GDTR.
LGDT	M16&64		LGDT M16&64	Load m into GDTR.
LGS	R16,M16:16		LGS R16,M16:16	Load GS:r16 with far pointer from memory.
LGS	R32,M16:32		LGS R32,M16:32	Load GS:r32 with far pointer from memory.
LGS	R64,M16:64		LGS R64,M16:64	Load GS:r64 with far pointer from memory.
LIDT	M16&32		LIDT M16&32	Load m into IDTR.
LIDT	M16&64		LIDT M16&64	Load m into IDTR.
LLDT	R/M16		LLDT R/M16	Load segment selector r/m16 into LDTR.
LMSW	R/M16		LMSW R/M16	Loads r/m16 in machine status word of CR0.
LOCK			LOCK 	Asserts LOCK# signal for duration of the accompanying instruction.
LODS	M16		LODS M16	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
LODS	M32		LODS M32	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
LODS	M64		LODS M64	Load qword at address (R)SI into RAX.
LODS	M8		LODS M8	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
LODSB			LODSB 	For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL.
LODSD			LODSD 	For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX.
LODSQ			LODSQ 	Load qword at address (R)SI into RAX.
LODSW			LODSW 	For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX.
LOOP	REL8		LOOP REL8	Decrement count; jump short if count ≠ 0.
LSL	R16,R16/M16		LSL R16,R16/M16	Load: r16 ← segment limit, selector r16/m16.
LSL	R32,R32/M16		LSL R32,R32/M16	Load: r32 ← segment limit, selector r32/m16.
LSL	R64,R32/M16		LSL R64,R32/M16	Load: r64 ← segment limit, selector r32/m16
LSS	R16,M16:16		LSS R16,M16:16	Load SS:r16 with far pointer from memory.
LSS	R32,M16:32		LSS R32,M16:32	Load SS:r32 with far pointer from memory.
LSS	R64,M16:64		LSS R64,M16:64	Load SS:r64 with far pointer from memory.
LTR	R/M16		LTR R/M16	Load r/m16 into task register.
LZCNT	R16,R/M16	LZCNT	LZCNT R16,R/M16	Count the number of leading zero bits in r/m16, return result in r16.
LZCNT	R32,R/M32	LZCNT	LZCNT R32,R/M32	Count the number of leading zero bits in r/m32, return result in r32.
LZCNT	R64,R/M64	LZCNT	LZCNT R64,R/M64	Count the number of leading zero bits in r/m64, return result in r64.
MASKMOVDQU	XMM,XMM	SSE2	MASKMOVDQU XMM1,XMM2	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
MASKMOVQ	MM,MM		MASKMOVQ MM1,MM2	Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI.
MAXPD	XMM,XMM/M128	SSE2	MAXPD XMM1,XMM2/M128	Return the maximum double-precision floating-point values between xmm1 and xmm2/m128.
MAXPS	XMM,XMM/M128	SSE	MAXPS XMM1,XMM2/M128	Return the maximum single-precision floating-point values between xmm1 and xmm2/mem.
MAXSD	XMM,XMM/M64	SSE2	MAXSD XMM1,XMM2/M64	Return the maximum scalar double-precision floating-point value between xmm2/m64 and xmm1.
MAXSS	XMM,XMM/M32	SSE	MAXSS XMM1,XMM2/M32	Return the maximum scalar single-precision floating-point value between xmm2/m32 and xmm1.
MFENCE			MFENCE 	Serializes load and store operations.
MINPD	XMM,XMM/M128	SSE2	MINPD XMM1,XMM2/M128	Return the minimum double-precision floating-point values between xmm1 and xmm2/mem
MINPS	XMM,XMM/M128	SSE	MINPS XMM1,XMM2/M128	Return the minimum single-precision floating-point values between xmm1 and xmm2/mem.
MINSD	XMM,XMM/M64	SSE2	MINSD XMM1,XMM2/M64	Return the minimum scalar double-precision floating-point value between xmm2/m64 and xmm1.
MINSS	XMM,XMM/M32	SSE	MINSS XMM1,XMM2/M32	Return the minimum scalar single-precision floating-point value between xmm2/m32 and xmm1.
MONITOR			MONITOR 	Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write-back memory caching type. The address is DS:EAX (DS:RAX in 64-bit mode).
MOV	DR0–DR7,R32		MOV DR0–DR7,R32	Move r32 to debug register.
MOV	DR0–DR7,R64		MOV DR0–DR7,R64	Move r64 to extended debug register.
MOV	R32,DR0–DR7		MOV R32,DR0–DR7	Move debug register to r32.
MOV	R64,DR0–DR7		MOV R64,DR0–DR7	Move extended debug register to r64.
MOVAPD	XMM,XMM/M128	SSE2	MOVAPD XMM1,XMM2/M128	Move aligned packed double-precision floating-point values from xmm2/mem to xmm1.
MOVAPD	XMM/M128,XMM	SSE2	MOVAPD XMM2/M128,XMM1	Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem.
MOVAPS	XMM,XMM/M128	SSE	MOVAPS XMM1,XMM2/M128	Move aligned packed single-precision floating-point values from xmm2/mem to xmm1.
MOVAPS	XMM/M128,XMM	SSE	MOVAPS XMM2/M128,XMM1	Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem.
MOVBE	M16,R16		MOVBE M16,R16	Reverse byte order in r16 and move to m16.
MOVBE	M32,R32		MOVBE M32,R32	Reverse byte order in r32 and move to m32.
MOVBE	M64,R64		MOVBE M64,R64	Reverse byte order in r64 and move to m64.
MOVBE	R16,M16		MOVBE R16,M16	Reverse byte order in m16 and move to r16.
MOVBE	R32,M32		MOVBE R32,M32	Reverse byte order in m32 and move to r32.
MOVBE	R64,M64		MOVBE R64,M64	Reverse byte order in m64 and move to r64.
MOVD	MM,R/M32	MMX	MOVD MM,R/M32	Move doubleword from r/m32 to mm.
MOVD	R/M32,MM	MMX	MOVD R/M32,MM	Move doubleword from mm to r/m32.
MOVD	R/M32,XMM	SSE2	MOVD R/M32,XMM	Move doubleword from xmm register to r/m32.
MOVD	XMM,R/M32	SSE2	MOVD XMM,R/M32	Move doubleword from r/m32 to xmm.
MOVDDUP	XMM,XMM/M64	SSE3	MOVDDUP XMM1,XMM2/M64	Move double-precision floating-point value from xmm2/m64 and duplicate into xmm1.
MOVDQ2Q	MM,XMM		MOVDQ2Q MM,XMM	Move low quadword from xmm to mmx register.
MOVDQA	XMM,XMM/M128	SSE2	MOVDQA XMM1,XMM2/M128	Move aligned packed integer values from xmm2/mem to xmm1.
MOVDQA	XMM/M128,XMM	SSE2	MOVDQA XMM2/M128,XMM1	Move aligned packed integer values from xmm1 to xmm2/mem.
MOVDQU	XMM,XMM/M128	SSE2	MOVDQU XMM1,XMM2/M128	Move unaligned packed integer values from xmm2/m128 to xmm1.
MOVDQU	XMM/M128,XMM	SSE2	MOVDQU XMM2/M128,XMM1	Move unaligned packed integer values from xmm1 to xmm2/m128.
MOVHLPS	XMM,XMM	SSE	MOVHLPS XMM1,XMM2	Move two packed single-precision floating-point values from high quadword of xmm2 to low quadword of xmm1.
MOVHPD	M64,XMM	SSE2	MOVHPD M64,XMM1	Move double-precision floating-point value from high quadword of xmm1 to m64.
MOVHPD	XMM,M64	SSE2	MOVHPD XMM1,M64	Move double-precision floating-point value from m64 to high quadword of xmm1.
MOVHPS	M64,XMM	SSE	MOVHPS M64,XMM1	Move two packed single-precision floating-point values from high quadword of xmm1 to m64.
MOVHPS	XMM,M64	SSE	MOVHPS XMM1,M64	Move two packed single-precision floating-point values from m64 to high quadword of xmm1.
MOVLHPS	XMM,XMM	SSE	MOVLHPS XMM1,XMM2	Move two packed single-precision floating-point values from low quadword of xmm2 to high quadword of xmm1.
MOVLPD	M64,XMM	SSE2	MOVLPD M64,XMM1	Move double-precision floating-point value from low quadword of xmm1 to m64.
MOVLPD	XMM,M64	SSE2	MOVLPD XMM1,M64	Move double-precision floating-point value from m64 to low quadword of xmm1.
MOVLPS	M64,XMM	SSE	MOVLPS M64,XMM1	Move two packed single-precision floating-point values from low quadword of xmm1 to m64.
MOVLPS	XMM,M64	SSE	MOVLPS XMM1,M64	Move two packed single-precision floating-point values from m64 to low quadword of xmm1.
MOVMSKPD	REG,XMM	SSE2	MOVMSKPD REG,XMM	Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
MOVMSKPS	REG,XMM	SSE	MOVMSKPS REG,XMM	Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros.
MOVNTDQ	M128,XMM	SSE2	MOVNTDQ M128,XMM1	Move packed integer values in xmm1 to m128 using non-temporal hint.
MOVNTDQA	XMM,M128	SSE4_1	MOVNTDQA XMM1,M128	Move double quadword from m128 to xmm1 using non-temporal hint if WC memory type.
MOVNTI	M32,R32		MOVNTI M32,R32	Move doubleword from r32 to m32 using non-temporal hint.
MOVNTI	M64,R64		MOVNTI M64,R64	Move quadword from r64 to m64 using non-temporal hint.
MOVNTPD	M128,XMM	SSE2	MOVNTPD M128,XMM1	Move packed double-precision values in xmm1 to m128 using non-temporal hint.
MOVNTPS	M128,XMM	SSE	MOVNTPS M128,XMM1	Move packed single-precision values xmm1 to mem using non-temporal hint.
MOVNTQ	M64,MM		MOVNTQ M64,MM	Move quadword from mm to m64 using non-temporal hint.
MOVQ	MM,MM/M64	MMX	MOVQ MM,MM/M64	Move quadword from mm/m64 to mm.
MOVQ	MM,R/M64	MMX	MOVQ MM,R/M64	Move quadword from r/m64 to mm.
MOVQ	MM/M64,MM	MMX	MOVQ MM/M64,MM	Move quadword from mm to mm/m64.
MOVQ	R/M64,MM	MMX	MOVQ R/M64,MM	Move quadword from mm to r/m64.
MOVQ	R/M64,XMM	SSE2	MOVQ R/M64,XMM	Move quadword from xmm register to r/m64.
MOVQ	XMM,R/M64	SSE2	MOVQ XMM,R/M64	Move quadword from r/m64 to xmm.
MOVQ	XMM,XMM/M64	SSE2	MOVQ XMM1,XMM2/M64	Move quadword from xmm2/mem64 to xmm1.
MOVQ	XMM/M64,XMM	SSE2	MOVQ XMM2/M64,XMM1	Move quadword from xmm1 to xmm2/mem64.
MOVQ2DQ	XMM,MM		MOVQ2DQ XMM,MM	Move quadword from mmx to low quadword of xmm.
MOVS	M16,M16		MOVS M16,M16	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
MOVS	M32,M32		MOVS M32,M32	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
MOVS	M64,M64		MOVS M64,M64	Move qword from address (R|E)SI to (R|E)DI.
MOVS	M8,M8		MOVS M8,M8	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
MOVSB			MOVSB 	For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI.
MOVSD			MOVSD 	For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI.
MOVSD	XMM,M64	SSE2	MOVSD XMM1,M64	Load scalar double-precision floating-point value from m64 to xmm1 register.
MOVSD	XMM,XMM	SSE2	MOVSD XMM1,XMM2	Move scalar double-precision floating-point value from xmm2 to xmm1 register.
MOVSD	XMM/M64,XMM	SSE2	MOVSD XMM1/M64,XMM2	Move scalar double-precision floating-point value from xmm2 register to xmm1/m64.
MOVSHDUP	XMM,XMM/M128	SSE3	MOVSHDUP XMM1,XMM2/M128	Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
MOVSLDUP	XMM,XMM/M128	SSE3	MOVSLDUP XMM1,XMM2/M128	Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
MOVSQ			MOVSQ 	Move qword from address (R|E)SI to (R|E)DI.
MOVSS	XMM,M32	SSE	MOVSS XMM1,M32	Load scalar single-precision floating-point value from m32 to xmm1 register.
MOVSS	XMM,XMM	SSE	MOVSS XMM1,XMM2	Merge scalar single-precision floating-point value from xmm2 to xmm1 register.
MOVSS	XMM/M32,XMM	SSE	MOVSS XMM2/M32,XMM1	Move scalar single-precision floating-point value from xmm1 register to xmm2/m32.
MOVSW			MOVSW 	For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI.
MOVSX	R16,R/M8		MOVSX R16,R/M8	Move byte to word with sign-extension.
MOVSX	R32,R/M16		MOVSX R32,R/M16	Move word to doubleword, with sign-extension.
MOVSX	R32,R/M8		MOVSX R32,R/M8	Move byte to doubleword with sign-extension.
MOVSX	R64,R/M16		MOVSX R64,R/M16	Move word to quadword with sign-extension.
MOVSX	R64,R/M8		MOVSX R64,R/M8	Move byte to quadword with sign-extension.
MOVSXD	R64,R/M32		MOVSXD R64,R/M32	Move doubleword to quadword with sign-extension.
MOVUPD	XMM,XMM/M128	SSE2	MOVUPD XMM1,XMM2/M128	Move unaligned packed double-precision floating-point from xmm2/mem to xmm1.
MOVUPD	XMM/M128,XMM	SSE2	MOVUPD XMM2/M128,XMM1	Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem.
MOVUPS	XMM,XMM/M128	SSE	MOVUPS XMM1,XMM2/M128	Move unaligned packed single-precision floating-point from xmm2/mem to xmm1.
MOVUPS	XMM/M128,XMM	SSE	MOVUPS XMM2/M128,XMM1	Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem.
MOVZX	R16,R/M8		MOVZX R16,R/M8	Move byte to word with zero-extension.
MOVZX	R32,R/M16		MOVZX R32,R/M16	Move word to doubleword, zero-extension.
MOVZX	R32,R/M8		MOVZX R32,R/M8	Move byte to doubleword, zero-extension.
MOVZX	R64,R/M16		MOVZX R64,R/M16	Move word to quadword, zero-extension.
MOVZX	R64,R/M8		MOVZX R64,R/M8	Move byte to quadword, zero-extension.
MPSADBW	XMM,XMM/M128,IMM8	SSE4_1	MPSADBW XMM1,XMM2/M128,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8.
MUL	R/M16		MUL R/M16	Unsigned multiply (DX:AX ← AX ∗ r/m16).
MUL	R/M32		MUL R/M32	Unsigned multiply (EDX:EAX ← EAX ∗ r/m32).
MUL	R/M64		MUL R/M64	Unsigned multiply (RDX:RAX ← RAX ∗ r/m64).
MUL	R/M8		MUL R/M8	Unsigned multiply (AX ← AL ∗ r/m8).
MULPD	XMM,XMM/M128	SSE2	MULPD XMM1,XMM2/M128	Multiply packed double-precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1.
MULPS	XMM,XMM/M128	SSE	MULPS XMM1,XMM2/M128	Multiply packed single-precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1.
MULSD	XMM,XMM/M64	SSE2	MULSD XMM1,XMM2/M64	Multiply the low double-precision floating-point value in xmm2/m64 by low double-precision floating-point value in xmm1.
MULSS	XMM,XMM/M32	SSE	MULSS XMM1,XMM2/M32	Multiply the low single-precision floating-point value in xmm2/m32 by the low single-precision floating-point value in xmm1.
MULX	R32,R32,R/M32	BMI2	MULX R32A,R32B,R/M32	Unsigned multiply of r/m32 with EDX without affecting arithmetic flags.
MULX	R64,R64,R/M64	BMI2	MULX R64A,R64B,R/M64	Unsigned multiply of r/m64 with RDX without affecting arithmetic flags.
MWAIT			MWAIT 	A hint that allow the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events.
NEG	R/M16		NEG R/M16	Two's complement negate r/m16.
NEG	R/M32		NEG R/M32	Two's complement negate r/m32.
NEG	R/M64		NEG R/M64	Two's complement negate r/m64.
NEG	R/M8		NEG R/M8	Two's complement negate r/m8.
NOP			NOP 	One byte no-operation instruction.
NOP	R/M16		NOP R/M16	Multi-byte no-operation instruction.
NOP	R/M32		NOP R/M32	Multi-byte no-operation instruction.
NOT	R/M16		NOT R/M16	Reverse each bit of r/m16.
NOT	R/M32		NOT R/M32	Reverse each bit of r/m32.
NOT	R/M64		NOT R/M64	Reverse each bit of r/m64.
NOT	R/M8		NOT R/M8	Reverse each bit of r/m8.
OR	AL,IMM8		OR AL,IMM8	AL OR imm8.
OR	AX,IMM16		OR AX,IMM16	AX OR imm16.
OR	EAX,IMM32		OR EAX,IMM32	EAX OR imm32.
OR	R/M16,IMM16		OR R/M16,IMM16	r/m16 OR imm16.
OR	R/M16,IMM8		OR R/M16,IMM8	r/m16 OR imm8 (sign-extended).
OR	R/M16,R16		OR R/M16,R16	r/m16 OR r16.
OR	R/M32,IMM32		OR R/M32,IMM32	r/m32 OR imm32.
OR	R/M32,IMM8		OR R/M32,IMM8	r/m32 OR imm8 (sign-extended).
OR	R/M32,R32		OR R/M32,R32	r/m32 OR r32.
OR	R/M64,IMM32		OR R/M64,IMM32	r/m64 OR imm32 (sign-extended).
OR	R/M64,IMM8		OR R/M64,IMM8	r/m64 OR imm8 (sign-extended).
OR	R/M64,R64		OR R/M64,R64	r/m64 OR r64.
OR	R/M8,IMM8		OR R/M8,IMM8	r/m8 OR imm8.
OR	R/M8,R8		OR R/M8,R8	r/m8 OR r8.
OR	R16,R/M16		OR R16,R/M16	r16 OR r/m16.
OR	R32,R/M32		OR R32,R/M32	r32 OR r/m32.
OR	R64,R/M64		OR R64,R/M64	r64 OR r/m64.
OR	R8,R/M8		OR R8,R/M8	r8 OR r/m8.
OR	RAX,IMM32		OR RAX,IMM32	RAX OR imm32 (sign-extended).
ORPD	XMM,XMM/M128	SSE2	ORPD XMM1,XMM2/M128	Return the bitwise logical OR of packed double-precision floating-point values in xmm1 and xmm2/mem
ORPS	XMM,XMM/M128	SSE	ORPS XMM1,XMM2/M128	Return the bitwise logical OR of packed single-precision floating-point values in xmm1 and xmm2/mem
OUT	DX,AL		OUT DX,AL	Output byte in AL to I/O port address in DX.
OUT	DX,AX		OUT DX,AX	Output word in AX to I/O port address in DX.
OUT	DX,EAX		OUT DX,EAX	Output doubleword in EAX to I/O port address in DX.
OUT	IMM8,AL		OUT IMM8,AL	Output byte in AL to I/O port address imm8.
OUT	IMM8,AX		OUT IMM8,AX	Output word in AX to I/O port address imm8.
OUT	IMM8,EAX		OUT IMM8,EAX	Output doubleword in EAX to I/O port address imm8.
OUTS	DX,M16		OUTS DX,M16	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
OUTS	DX,M32		OUTS DX,M32	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
OUTS	DX,M8		OUTS DX,M8	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
OUTSB			OUTSB 	Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
OUTSD			OUTSD 	Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
OUTSW			OUTSW 	Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX.
PABSB	MM,MM/M64	SSSE3	PABSB MM1,MM2/M64	Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1.
PABSB	XMM,XMM/M128	SSSE3	PABSB XMM1,XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
PABSD	MM,MM/M64	SSSE3	PABSD MM1,MM2/M64	Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1.
PABSD	XMM,XMM/M128	SSSE3	PABSD XMM1,XMM2/M128	Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
PABSW	MM,MM/M64	SSSE3	PABSW MM1,MM2/M64	Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1.
PABSW	XMM,XMM/M128	SSSE3	PABSW XMM1,XMM2/M128	Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
PACKSSDW	MM,MM/M64	MMX	PACKSSDW MM1,MM2/M64	Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation.
PACKSSDW	XMM,XMM/M128	SSE2	PACKSSDW XMM1,XMM2/M128	Converts 4 packed signed doubleword integers from xmm1 and from xxm2/m128 into 8 packed signed word integers in xxm1 using signed saturation.
PACKSSWB	MM,MM/M64	MMX	PACKSSWB MM1,MM2/M64	Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation.
PACKSSWB	XMM,XMM/M128	SSE2	PACKSSWB XMM1,XMM2/M128	Converts 8 packed signed word integers from xmm1 and from xxm2/m128 into 16 packed signed byte integers in xxm1 using signed saturation.
PACKUSDW	XMM,XMM/M128	SSE4_1	PACKUSDW XMM1,XMM2/M128	Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
PACKUSWB	MM,MM/M64	MMX	PACKUSWB MM,MM/M64	Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation.
PACKUSWB	XMM,XMM/M128	SSE2	PACKUSWB XMM1,XMM2/M128	Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
PADDB	MM,MM/M64	MMX	PADDB MM,MM/M64	Add packed byte integers from mm/m64 and mm.
PADDB	XMM,XMM/M128	SSE2	PADDB XMM1,XMM2/M128	Add packed byte integers from xmm2/m128 and xmm1.
PADDD	XMM,XMM/M128	SSE2	PADDD XMM1,XMM2/M128	Add packed doubleword integers from xmm2/m128 and xmm1.
PADDQ	XMM,XMM/M128	SSE2	PADDQ XMM1,XMM2/M128	Add packed quadword integers from xmm2/m128 and xmm1.
PADDSB	MM,MM/M64	MMX	PADDSB MM,MM/M64	Add packed signed byte integers from mm/m64 and mm and saturate the results.
PADDSB	XMM,XMM/M128	SSE2	PADDSB XMM1,XMM2/M128	Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results.
PADDSW	MM,MM/M64	MMX	PADDSW MM,MM/M64	Add packed signed word integers from mm/m64 and mm and saturate the results.
PADDSW	XMM,XMM/M128	SSE2	PADDSW XMM1,XMM2/M128	Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results.
PADDUSB	MM,MM/M64	MMX	PADDUSB MM,MM/M64	Add packed unsigned byte integers from mm/m64 and mm and saturate the results.
PADDUSB	XMM,XMM/M128	SSE2	PADDUSB XMM1,XMM2/M128	Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results.
PADDUSW	MM,MM/M64	MMX	PADDUSW MM,MM/M64	Add packed unsigned word integers from mm/m64 and mm and saturate the results.
PADDUSW	XMM,XMM/M128	SSE2	PADDUSW XMM1,XMM2/M128	Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results.
PADDW	MM,MM/M64	MMX	PADDW MM,MM/M64	Add packed word integers from mm/m64 and mm.
PADDW	XMM,XMM/M128	SSE2	PADDW XMM1,XMM2/M128	Add packed word integers from xmm2/m128 and xmm1.
PALIGNR	MM,MM/M64,IMM8	SSSE3	PALIGNR MM1,MM2/M64,IMM8	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1.
PALIGNR	XMM,XMM/M128,IMM8	SSSE3	PALIGNR XMM1,XMM2/M128,IMM8	Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1.
PAND	MM,MM/M64	MMX	PAND MM,MM/M64	Bitwise AND mm/m64 and mm.
PAND	XMM,XMM/M128	SSE2	PAND XMM1,XMM2/M128	Bitwise AND of xmm2/m128 and xmm1.
PANDN	MM,MM/M64	MMX	PANDN MM,MM/M64	Bitwise AND NOT of mm/m64 and mm.
PANDN	XMM,XMM/M128	SSE2	PANDN XMM1,XMM2/M128	Bitwise AND NOT of xmm2/m128 and xmm1.
PAUSE			PAUSE 	Gives hint to processor that improves performance of spin-wait loops.
PAVGB	MM,MM/M64	SSE	PAVGB MM1,MM2/M64	Average packed unsigned byte integers from mm2/m64 and mm1 with rounding.
PAVGB	XMM,XMM/M128	SSE2	PAVGB XMM1,XMM2/M128	Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding.
PAVGW	MM,MM/M64	SSE	PAVGW MM1,MM2/M64	Average packed unsigned word integers from mm2/m64 and mm1 with rounding.
PAVGW	XMM,XMM/M128	SSE2	PAVGW XMM1,XMM2/M128	Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding.
PBLENDVB	XMM,XMM/M128,XMM_ZERO	SSE4_1	PBLENDVB XMM1,XMM2/M128,XMM_ZERO	Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1.
PBLENDW	XMM,XMM/M128,IMM8	SSE4_1	PBLENDW XMM1,XMM2/M128,IMM8	Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1.
PCLMULQDQ	XMM,XMM/M128,IMM8		PCLMULQDQ XMM1,XMM2/M128,IMM8	Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The imme-diate is used to determine which quadwords of xmm1 and xmm2/m128 should be used.
PCMPEQB	MM,MM/M64	MMX	PCMPEQB MM,MM/M64	Compare packed bytes in mm/m64 and mm for equality.
PCMPEQB	XMM,XMM/M128	SSE2	PCMPEQB XMM1,XMM2/M128	Compare packed bytes in xmm2/m128 and xmm1 for equality.
PCMPEQD	MM,MM/M64	MMX	PCMPEQD MM,MM/M64	Compare packed doublewords in mm/m64 and mm for equality.
PCMPEQD	XMM,XMM/M128	SSE2	PCMPEQD XMM1,XMM2/M128	Compare packed doublewords in xmm2/m128 and xmm1 for equality.
PCMPEQQ	XMM,XMM/M128	SSE4_1	PCMPEQQ XMM1,XMM2/M128	Compare packed qwords in xmm2/m128 and xmm1 for equality.
PCMPEQW	MM,MM/M64	MMX	PCMPEQW MM,MM/M64	Compare packed words in mm/m64 and mm for equality.
PCMPEQW	XMM,XMM/M128	SSE2	PCMPEQW XMM1,XMM2/M128	Compare packed words in xmm2/m128 and xmm1 for equality.
PCMPESTRI	XMM,XMM/M128,IMM8	SSE4_2	PCMPESTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
PCMPESTRM	XMM,XMM/M128,IMM8	SSE4_2	PCMPESTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0
PCMPGTB	MM,MM/M64	MMX	PCMPGTB MM,MM/M64	Compare packed signed byte integers in mm and mm/m64 for greater than.
PCMPGTB	XMM,XMM/M128	SSE2	PCMPGTB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than.
PCMPGTD	MM,MM/M64	MMX	PCMPGTD MM,MM/M64	Compare packed signed doubleword integers in mm and mm/m64 for greater than.
PCMPGTD	XMM,XMM/M128	SSE2	PCMPGTD XMM1,XMM2/M128	Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than.
PCMPGTQ	XMM,XMM/M128	SSE4_2	PCMPGTQ XMM1,XMM2/M128	Compare packed signed qwords in  xmm2/m128 and xmm1 for greater than.
PCMPGTW	MM,MM/M64	MMX	PCMPGTW MM,MM/M64	Compare packed signed word integers in mm and mm/m64 for greater than.
PCMPGTW	XMM,XMM/M128	SSE2	PCMPGTW XMM1,XMM2/M128	Compare packed signed word integers in xmm1 and xmm2/m128 for greater than.
PCMPISTRI	XMM,XMM/M128,IMM8	SSE4_2	PCMPISTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
PCMPISTRM	XMM,XMM/M128,IMM8	SSE4_2	PCMPISTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0.
PDEP	R32,R32,R/M32	BMI2	PDEP R32A,R32B,R/M32	Parallel deposit of bits from r32b using mask in r/m32, result is written to r32a.
PDEP	R64,R64,R/M64	BMI2	PDEP R64A,R64B,R/M64	Parallel deposit of bits from r64b using mask in r/m64, result is written to r64a.
PEXT	R32,R32,R/M32	BMI2	PEXT R32A,R32B,R/M32	Parallel extract of bits from r32b using mask in r/m32, result is written to r32a.
PEXT	R64,R64,R/M64	BMI2	PEXT R64A,R64B,R/M64	Parallel extract of bits from r64b using mask in r/m64, result is written to r64a.
PEXTRB	REG/M8,XMM,IMM8	SSE4_1	PEXTRB REG/M8,XMM2,IMM8	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r32 or r64 are zeroed.
PEXTRD	R/M32,XMM,IMM8	SSE4_1	PEXTRD R/M32,XMM2,IMM8	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32.
PEXTRQ	R/M64,XMM,IMM8	SSE4_1	PEXTRQ R/M64,XMM2,IMM8	Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64.
PEXTRW	REG,MM,IMM8	SSE	PEXTRW REG,MM,IMM8	Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
PEXTRW	REG,XMM,IMM8	SSE2	PEXTRW REG,XMM,IMM8	Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed.
PEXTRW	REG/M16,XMM,IMM8	SSE4_1	PEXTRW REG/M16,XMM,IMM8	Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero-extend the result in the destination, r32 or r64.
PHADDD	MM,MM/M64	SSSE3	PHADDD MM1,MM2/M64	Add 32-bit integers horizontally, pack to mm1.
PHADDD	XMM,XMM/M128	SSSE3	PHADDD XMM1,XMM2/M128	Add 32-bit integers horizontally, pack to xmm1.
PHADDSW	MM,MM/M64	SSSE3	PHADDSW MM1,MM2/M64	Add 16-bit signed integers horizontally, pack saturated integers to mm1.
PHADDSW	XMM,XMM/M128	SSSE3	PHADDSW XMM1,XMM2/M128	Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
PHADDW	MM,MM/M64	SSSE3	PHADDW MM1,MM2/M64	Add 16-bit integers horizontally, pack to mm1.
PHADDW	XMM,XMM/M128	SSSE3	PHADDW XMM1,XMM2/M128	Add 16-bit integers horizontally, pack to xmm1.
PHMINPOSUW	XMM,XMM/M128	SSE4_1	PHMINPOSUW XMM1,XMM2/M128	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
PHSUBD	MM,MM/M64	SSSE3	PHSUBD MM1,MM2/M64	Subtract 32-bit signed integers horizontally, pack to mm1.
PHSUBD	XMM,XMM/M128	SSSE3	PHSUBD XMM1,XMM2/M128	Subtract 32-bit signed integers horizontally, pack to xmm1.
PHSUBSW	MM,MM/M64	SSSE3	PHSUBSW MM1,MM2/M64	Subtract 16-bit signed integer horizontally, pack saturated integers to mm1.
PHSUBSW	XMM,XMM/M128	SSSE3	PHSUBSW XMM1,XMM2/M128	Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
PHSUBW	MM,MM/M64	SSSE3	PHSUBW MM1,MM2/M64	Subtract 16-bit signed integers horizontally, pack to mm1.
PHSUBW	XMM,XMM/M128	SSSE3	PHSUBW XMM1,XMM2/M128	Subtract 16-bit signed integers horizontally, pack to xmm1.
PINSRB	XMM,R32/M8,IMM8	SSE4_1	PINSRB XMM1,R32/M8,IMM8	Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8.
PINSRD	XMM,R/M32,IMM8	SSE4_1	PINSRD XMM1,R/M32,IMM8	Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8.
PINSRQ	XMM,R/M64,IMM8	SSE4_1	PINSRQ XMM1,R/M64,IMM8	Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8.
PINSRW	MM,R32/M16,IMM8	SSE	PINSRW MM,R32/M16,IMM8	Insert the low word from r32 or from m16 into mm at the word position specified by imm8.
PINSRW	XMM,R32/M16,IMM8	SSE2	PINSRW XMM,R32/M16,IMM8	Move the low word of r32 or from m16 into xmm at the word position specified by imm8.
PMADDUBSW	MM,MM/M64	SSSE3	PMADDUBSW MM1,MM2/M64	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to mm1.
PMADDUBSW	XMM,XMM/M128	SSSE3	PMADDUBSW XMM1,XMM2/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
PMADDWD	MM,MM/M64	MMX	PMADDWD MM,MM/M64	Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm.
PMADDWD	XMM,XMM/M128	SSE2	PMADDWD XMM1,XMM2/M128	Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1.
PMAXSB	XMM,XMM/M128	SSE4_1	PMAXSB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
PMAXSD	XMM,XMM/M128	SSE4_1	PMAXSD XMM1,XMM2/M128	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
PMAXSW	MM,MM/M64	SSE	PMAXSW MM1,MM2/M64	Compare signed word integers in mm2/m64 and mm1 and return maximum values.
PMAXSW	XMM,XMM/M128	SSE2	PMAXSW XMM1,XMM2/M128	Compare packed signed word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1.
PMAXUB	MM,MM/M64	SSE	PMAXUB MM1,MM2/M64	Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values.
PMAXUB	XMM,XMM/M128	SSE2	PMAXUB XMM1,XMM2/M128	Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
PMAXUD	XMM,XMM/M128	SSE4_1	PMAXUD XMM1,XMM2/M128	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1.
PMAXUW	XMM,XMM/M128	SSE4_1	PMAXUW XMM1,XMM2/M128	Compare packed unsigned word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1.
PMINSB	XMM,XMM/M128	SSE4_1	PMINSB XMM1,XMM2/M128	Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
PMINSD	XMM,XMM/M128	SSE4_1	PMINSD XMM1,XMM2/M128	Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
PMINSW	MM,MM/M64	SSE	PMINSW MM1,MM2/M64	Compare signed word integers in mm2/m64 and mm1 and return minimum values.
PMINSW	XMM,XMM/M128	SSE2	PMINSW XMM1,XMM2/M128	Compare packed signed word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1.
PMINUB	MM,MM/M64	SSE	PMINUB MM1,MM2/M64	Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values.
PMINUB	XMM,XMM/M128	SSE2	PMINUB XMM1,XMM2/M128	Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
PMINUD	XMM,XMM/M128	SSE4_1	PMINUD XMM1,XMM2/M128	Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1.
PMINUW	XMM,XMM/M128	SSE4_1	PMINUW XMM1,XMM2/M128	Compare packed unsigned word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1.
PMOVMSKB	REG,MM	SSE	PMOVMSKB REG,MM	Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed
PMOVMSKB	REG,XMM	SSE2	PMOVMSKB REG,XMM	Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed
PMOVSXBD	XMM,XMM/M32	SSE4_1	PMOVSXBD XMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
PMOVSXBQ	XMM,XMM/M16	SSE4_1	PMOVSXBQ XMM1,XMM2/M16	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
PMOVSXBW	XMM,XMM/M64	SSE4_1	PMOVSXBW XMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
PMOVSXDQ	XMM,XMM/M64	SSE4_1	PMOVSXDQ XMM1,XMM2/M64	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
PMOVSXWD	XMM,XMM/M64	SSE4_1	PMOVSXWD XMM1,XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
PMOVSXWQ	XMM,XMM/M32	SSE4_1	PMOVSXWQ XMM1,XMM2/M32	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
PMOVZXBD	XMM,XMM/M32	SSE4_1	PMOVZXBD XMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
PMOVZXBQ	XMM,XMM/M16	SSE4_1	PMOVZXBQ XMM1,XMM2/M16	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
PMOVZXBW	XMM,XMM/M64	SSE4_1	PMOVZXBW XMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
PMOVZXDQ	XMM,XMM/M64	SSE4_1	PMOVZXDQ XMM1,XMM2/M64	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
PMOVZXWD	XMM,XMM/M64	SSE4_1	PMOVZXWD XMM1,XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
PMOVZXWQ	XMM,XMM/M32	SSE4_1	PMOVZXWQ XMM1,XMM2/M32	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
PMULDQ	XMM,XMM/M128	SSE4_1	PMULDQ XMM1,XMM2/M128	Multiply packed signed doubleword integers in xmm1 by packed signed doubleword integers in xmm2/m128, and store the quadword results in xmm1.
PMULHRSW	MM,MM/M64	SSSE3	PMULHRSW MM1,MM2/M64	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to mm1.
PMULHRSW	XMM,XMM/M128	SSSE3	PMULHRSW XMM1,XMM2/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
PMULHUW	MM,MM/M64	SSE	PMULHUW MM1,MM2/M64	Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
PMULHUW	XMM,XMM/M128	SSE2	PMULHUW XMM1,XMM2/M128	Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
PMULHW	MM,MM/M64	MMX	PMULHW MM,MM/M64	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1.
PMULHW	XMM,XMM/M128	SSE2	PMULHW XMM1,XMM2/M128	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1.
PMULLD	XMM,XMM/M128	SSE4_1	PMULLD XMM1,XMM2/M128	Multiply the packed dword signed integers in xmm1 and xmm2/m128 and store the low 32 bits of each product in xmm1.
PMULLW	MM,MM/M64	MMX	PMULLW MM,MM/M64	Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1.
PMULLW	XMM,XMM/M128	SSE2	PMULLW XMM1,XMM2/M128	Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1.
PMULUDQ	MM,MM/M64	SSE2	PMULUDQ MM1,MM2/M64	Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1.
PMULUDQ	XMM,XMM/M128	SSE2	PMULUDQ XMM1,XMM2/M128	Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1.
POP	DS		POP DS	Pop top of stack into DS; increment stack pointer.
POP	ES		POP ES	Pop top of stack into ES; increment stack pointer.
POP	FS		POP FS	Pop top of stack into FS; increment stack pointer by 16 bits.
POP	GS		POP GS	Pop top of stack into GS; increment stack pointer by 16 bits.
POP	R/M16		POP R/M16	Pop top of stack into m16; increment stack pointer.
POP	R/M32		POP R/M32	Pop top of stack into m32; increment stack pointer.
POP	R/M64		POP R/M64	Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size.
POP	R16		POP R16	Pop top of stack into r16; increment stack pointer.
POP	R32		POP R32	Pop top of stack into r32; increment stack pointer.
POP	R64		POP R64	Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size.
POP	SS		POP SS	Pop top of stack into SS; increment stack pointer.
POPA			POPA 	Pop DI, SI, BP, BX, DX, CX, and AX.
POPAD			POPAD 	Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX.
POPCNT	R16,R/M16		POPCNT R16,R/M16	POPCNT on r/m16
POPCNT	R32,R/M32		POPCNT R32,R/M32	POPCNT on r/m32
POPCNT	R64,R/M64		POPCNT R64,R/M64	POPCNT on r/m64
POPF			POPF 	Pop top of stack into lower 16 bits of EFLAGS.
POPFD			POPFD 	Pop top of stack into EFLAGS.
POPFQ			POPFQ 	Pop top of stack and zero-extend into RFLAGS.
POR	MM,MM/M64	MMX	POR MM,MM/M64	Bitwise OR of mm/m64 and mm.
POR	XMM,XMM/M128	SSE2	POR XMM1,XMM2/M128	Bitwise OR of xmm2/m128 and xmm1.
PREFETCHNTA	M8		PREFETCHNTA M8	Move data from m8 closer to the processor using NTA hint.
PREFETCHT0	M8		PREFETCHT0 M8	Move data from m8 closer to the processor using T0 hint.
PREFETCHT1	M8		PREFETCHT1 M8	Move data from m8 closer to the processor using T1 hint.
PREFETCHT2	M8		PREFETCHT2 M8	Move data from m8 closer to the processor using T2 hint.
PREFETCHW	M8	PRFCHW	PREFETCHW M8	Move data from m8 closer to the processor in anticipation of a write.
PREFETCHWT1	M8	PREFETCHWT1	PREFETCHWT1 M8	Move data from m8 closer to the processor using T1 hint with intent to write.
PSADBW	MM,MM/M64	SSE	PSADBW MM1,MM2/M64	Computes the absolute differences of the packed unsigned byte integers from mm2 /m64 and mm1; differences are then summed to produce an unsigned word integer result.
PSADBW	XMM,XMM/M128	SSE2	PSADBW XMM1,XMM2/M128	Computes the absolute differences of the packed unsigned byte integers from xmm2 /m128 and xmm1; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
PSHUFB	MM,MM/M64	SSSE3	PSHUFB MM1,MM2/M64	Shuffle bytes in mm1 according to contents of mm2/m64.
PSHUFB	XMM,XMM/M128	SSSE3	PSHUFB XMM1,XMM2/M128	Shuffle bytes in xmm1 according to contents of xmm2/m128.
PSHUFD	XMM,XMM/M128,IMM8	SSE2	PSHUFD XMM1,XMM2/M128,IMM8	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
PSHUFHW	XMM,XMM/M128,IMM8	SSE2	PSHUFHW XMM1,XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
PSHUFLW	XMM,XMM/M128,IMM8	SSE2	PSHUFLW XMM1,XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
PSHUFW	MM,MM/M64,IMM8		PSHUFW MM1,MM2/M64,IMM8	Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1.
PSIGNB	MM,MM/M64	SSSE3	PSIGNB MM1,MM2/M64	Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64.
PSIGNB	XMM,XMM/M128	SSSE3	PSIGNB XMM1,XMM2/M128	Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128.
PSIGND	MM,MM/M64	SSSE3	PSIGND MM1,MM2/M64	Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128.
PSIGND	XMM,XMM/M128	SSSE3	PSIGND XMM1,XMM2/M128	Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128.
PSIGNW	MM,MM/M64	SSSE3	PSIGNW MM1,MM2/M64	Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128.
PSIGNW	XMM,XMM/M128	SSSE3	PSIGNW XMM1,XMM2/M128	Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128.
PSLLD	MM,IMM8	MMX	PSLLD MM,IMM8	Shift doublewords in mm left by imm8 while shifting in 0s.
PSLLD	MM,MM/M64	MMX	PSLLD MM,MM/M64	Shift doublewords in mm left by mm/m64 while shifting in 0s.
PSLLD	XMM,IMM8	SSE2	PSLLD XMM1,IMM8	Shift doublewords in xmm1 left by imm8 while shifting in 0s.
PSLLD	XMM,XMM/M128	SSE2	PSLLD XMM1,XMM2/M128	Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s.
PSLLDQ	XMM,IMM8	SSE2	PSLLDQ XMM1,IMM8	Shift xmm1 left by imm8 bytes while shifting in 0s.
PSLLQ	MM,IMM8	MMX	PSLLQ MM,IMM8	Shift quadword in mm left by imm8 while shifting in 0s.
PSLLQ	MM,MM/M64	MMX	PSLLQ MM,MM/M64	Shift quadword in mm left by mm/m64 while shifting in 0s.
PSLLQ	XMM,IMM8	SSE2	PSLLQ XMM1,IMM8	Shift quadwords in xmm1 left by imm8 while shifting in 0s.
PSLLQ	XMM,XMM/M128	SSE2	PSLLQ XMM1,XMM2/M128	Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s.
PSLLW	MM,IMM8	MMX	PSLLW MM1,IMM8	Shift words in mm left by imm8 while shifting in 0s.
PSLLW	MM,MM/M64	MMX	PSLLW MM,MM/M64	Shift words in mm left mm/m64 while shifting in 0s.
PSLLW	XMM,IMM8	SSE2	PSLLW XMM1,IMM8	Shift words in xmm1 left by imm8 while shifting in 0s.
PSLLW	XMM,XMM/M128	SSE2	PSLLW XMM1,XMM2/M128	Shift words in xmm1 left by xmm2/m128 while shifting in 0s.
PSRAD	MM,IMM8	MMX	PSRAD MM,IMM8	Shift doublewords in mm right by imm8 while shifting in sign bits.
PSRAD	MM,MM/M64	MMX	PSRAD MM,MM/M64	Shift doublewords in mm right by mm/m64 while shifting in sign bits.
PSRAD	XMM,IMM8	SSE2	PSRAD XMM1,IMM8	Shift doublewords in xmm1 right by imm8 while shifting in sign bits.
PSRAD	XMM,XMM/M128	SSE2	PSRAD XMM1,XMM2/M128	Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits.
PSRAW	MM,IMM8	MMX	PSRAW MM,IMM8	Shift words in mm right by imm8 while shifting in sign bits
PSRAW	MM,MM/M64	MMX	PSRAW MM,MM/M64	Shift words in mm right by mm/m64 while shifting in sign bits.
PSRAW	XMM,IMM8	SSE2	PSRAW XMM1,IMM8	Shift words in xmm1 right by imm8 while shifting in sign bits
PSRAW	XMM,XMM/M128	SSE2	PSRAW XMM1,XMM2/M128	Shift words in xmm1 right by xmm2/m128 while shifting in sign bits.
PSRLD	MM,IMM8	MMX	PSRLD MM,IMM8	Shift doublewords in mm right by imm8 while shifting in 0s.
PSRLD	MM,MM/M64	MMX	PSRLD MM,MM/M64	Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s.
PSRLD	XMM,IMM8	SSE2	PSRLD XMM1,IMM8	Shift doublewords in xmm1 right by imm8 while shifting in 0s.
PSRLD	XMM,XMM/M128	SSE2	PSRLD XMM1,XMM2/M128	Shift doublewords in xmm1 right by amount specified in xmm2 /m128 while shifting in 0s.
PSRLDQ	XMM,IMM8	SSE2	PSRLDQ XMM1,IMM8	Shift xmm1 right by imm8 while shifting in 0s.
PSRLQ	MM,IMM8	MMX	PSRLQ MM,IMM8	Shift mm right by imm8 while shifting in 0s.
PSRLQ	MM,MM/M64	MMX	PSRLQ MM,MM/M64	Shift mm right by amount specified in mm/m64 while shifting in 0s.
PSRLQ	XMM,IMM8	SSE2	PSRLQ XMM1,IMM8	Shift quadwords in xmm1 right by imm8 while shifting in 0s.
PSRLQ	XMM,XMM/M128	SSE2	PSRLQ XMM1,XMM2/M128	Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
PSRLW	MM,IMM8	MMX	PSRLW MM,IMM8	Shift words in mm right by imm8 while shifting in 0s.
PSRLW	MM,MM/M64	MMX	PSRLW MM,MM/M64	Shift words in mm right by amount specified in mm/m64 while shifting in 0s.
PSRLW	XMM,IMM8	SSE2	PSRLW XMM1,IMM8	Shift words in xmm1 right by imm8 while shifting in 0s.
PSRLW	XMM,XMM/M128	SSE2	PSRLW XMM1,XMM2/M128	Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s.
PSUBB	MM,MM/M64	MMX	PSUBB MM,MM/M64	Subtract packed byte integers in mm/m64 from packed byte integers in mm.
PSUBB	XMM,XMM/M128	SSE2	PSUBB XMM1,XMM2/M128	Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1.
PSUBD	MM,MM/M64	MMX	PSUBD MM,MM/M64	Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm.
PSUBD	XMM,XMM/M128	SSE2	PSUBD XMM1,XMM2/M128	Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1.
PSUBQ	MM,MM/M64	SSE2	PSUBQ MM1,MM2/M64	Subtract quadword integer in mm1 from mm2 /m64.
PSUBQ	XMM,XMM/M128	SSE2	PSUBQ XMM1,XMM2/M128	Subtract packed quadword integers in xmm1 from xmm2 /m128.
PSUBSB	MM,MM/M64	MMX	PSUBSB MM,MM/M64	Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results.
PSUBSB	XMM,XMM/M128	SSE2	PSUBSB XMM1,XMM2/M128	Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results.
PSUBSW	MM,MM/M64	MMX	PSUBSW MM,MM/M64	Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results.
PSUBSW	XMM,XMM/M128	SSE2	PSUBSW XMM1,XMM2/M128	Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results.
PSUBUSB	MM,MM/M64	MMX	PSUBUSB MM,MM/M64	Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result.
PSUBUSB	XMM,XMM/M128	SSE2	PSUBUSB XMM1,XMM2/M128	Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result.
PSUBUSW	MM,MM/M64	MMX	PSUBUSW MM,MM/M64	Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result.
PSUBUSW	XMM,XMM/M128	SSE2	PSUBUSW XMM1,XMM2/M128	Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result.
PSUBW	MM,MM/M64	MMX	PSUBW MM,MM/M64	Subtract packed word integers in mm/m64 from packed word integers in mm.
PSUBW	XMM,XMM/M128	SSE2	PSUBW XMM1,XMM2/M128	Subtract packed word integers in xmm2/m128 from packed word integers in xmm1.
PTEST	XMM,XMM/M128	SSE4_1	PTEST XMM1,XMM2/M128	Set ZF if xmm2/m128 AND xmm1 result is all 0s. Set CF if xmm2/m128 AND NOT xmm1 result is all 0s.
PUNPCKHBW	MM,MM/M64	MMX	PUNPCKHBW MM,MM/M64	Unpack and interleave high-order bytes from mm and mm/m64 into mm.
PUNPCKHBW	XMM,XMM/M128	SSE2	PUNPCKHBW XMM1,XMM2/M128	Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1.
PUNPCKHDQ	MM,MM/M64	MMX	PUNPCKHDQ MM,MM/M64	Unpack and interleave high-order doublewords from mm and mm/m64 into mm.
PUNPCKHDQ	XMM,XMM/M128	SSE2	PUNPCKHDQ XMM1,XMM2/M128	Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1.
PUNPCKHQDQ	XMM,XMM/M128	SSE2	PUNPCKHQDQ XMM1,XMM2/M128	Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1.
PUNPCKHWD	MM,MM/M64	MMX	PUNPCKHWD MM,MM/M64	Unpack and interleave high-order words from mm and mm/m64 into mm.
PUNPCKHWD	XMM,XMM/M128	SSE2	PUNPCKHWD XMM1,XMM2/M128	Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1.
PUNPCKLBW	MM,MM/M32	MMX	PUNPCKLBW MM,MM/M32	Interleave low-order bytes from mm and mm/m32 into mm.
PUNPCKLBW	XMM,XMM/M128	SSE2	PUNPCKLBW XMM1,XMM2/M128	Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1.
PUNPCKLDQ	MM,MM/M32	MMX	PUNPCKLDQ MM,MM/M32	Interleave low-order doublewords from mm and mm/m32 into mm.
PUNPCKLDQ	XMM,XMM/M128	SSE2	PUNPCKLDQ XMM1,XMM2/M128	Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1.
PUNPCKLQDQ	XMM,XMM/M128	SSE2	PUNPCKLQDQ XMM1,XMM2/M128	Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register.
PUNPCKLWD	MM,MM/M32	MMX	PUNPCKLWD MM,MM/M32	Interleave low-order words from mm and mm/m32 into mm.
PUNPCKLWD	XMM,XMM/M128	SSE2	PUNPCKLWD XMM1,XMM2/M128	Interleave low-order words from xmm1 and xmm2/m128 into xmm1.
PUSH	CS		PUSH CS	Push CS.
PUSH	DS		PUSH DS	Push DS.
PUSH	ES		PUSH ES	Push ES.
PUSH	FS		PUSH FS	Push FS.
PUSH	GS		PUSH GS	Push GS.
PUSH	IMM32		PUSH IMM32	Push imm32.
PUSH	IMM16		PUSH IMM16	Push imm16.
PUSH	IMM8		PUSH IMM8	Push imm8.
PUSH	R/M16		PUSH R/M16	Push r/m16.
PUSH	R/M32		PUSH R/M32	Push r/m32.
PUSH	R/M64		PUSH R/M64	Push r/m64.
PUSH	R16		PUSH R16	Push r16.
PUSH	R32		PUSH R32	Push r32.
PUSH	R64		PUSH R64	Push r64.
PUSH	SS		PUSH SS	Push SS.
PUSHA			PUSHA 	Push AX, CX, DX, BX, original SP, BP, SI, and DI.
PUSHAD			PUSHAD 	Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI.
PUSHF			PUSHF 	Push lower 16 bits of EFLAGS.
PUSHFD			PUSHFD 	Push EFLAGS.
PXOR	MM,MM/M64	MMX	PXOR MM,MM/M64	Bitwise XOR of mm/m64 and mm.
PXOR	XMM,XMM/M128	SSE2	PXOR XMM1,XMM2/M128	Bitwise XOR of xmm2/m128 and xmm1.
RCL	R/M16,1		RCL R/M16,1	Rotate 17 bits (CF, r/m16) left once.
RCL	R/M16,CL		RCL R/M16,CL	Rotate 17 bits (CF, r/m16) left CL times.
RCL	R/M16,IMM8		RCL R/M16,IMM8	Rotate 17 bits (CF, r/m16) left imm8 times.
RCL	R/M32,1		RCL R/M32,1	Rotate 33 bits (CF, r/m32) left once.
RCL	R/M32,CL		RCL R/M32,CL	Rotate 33 bits (CF, r/m32) left CL times.
RCL	R/M32,IMM8		RCL R/M32,IMM8	Rotate 33 bits (CF, r/m32) left imm8 times.
RCL	R/M64,1		RCL R/M64,1	Rotate 65 bits (CF, r/m64) left once. Uses a 6 bit count.
RCL	R/M64,CL		RCL R/M64,CL	Rotate 65 bits (CF, r/m64) left CL times. Uses a 6 bit count.
RCL	R/M64,IMM8		RCL R/M64,IMM8	Rotate 65 bits (CF, r/m64) left imm8 times. Uses a 6 bit count.
RCL	R/M8,1		RCL R/M8,1	Rotate 9 bits (CF, r/m8) left once.
RCL	R/M8,CL		RCL R/M8,CL	Rotate 9 bits (CF, r/m8) left CL times.
RCL	R/M8,IMM8		RCL R/M8,IMM8	Rotate 9 bits (CF, r/m8) left imm8 times.
RCPPS	XMM,XMM/M128	SSE	RCPPS XMM1,XMM2/M128	Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
RCPSS	XMM,XMM/M32	SSE	RCPSS XMM1,XMM2/M32	Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm2/m32 and stores the result in xmm1.
RCR	R/M16,1		RCR R/M16,1	Rotate 17 bits (CF, r/m16) right once.
RCR	R/M16,CL		RCR R/M16,CL	Rotate 17 bits (CF, r/m16) right CL times.
RCR	R/M16,IMM8		RCR R/M16,IMM8	Rotate 17 bits (CF, r/m16) right imm8 times.
RCR	R/M32,1		RCR R/M32,1	Rotate 33 bits (CF, r/m32) right once. Uses a 6 bit count.
RCR	R/M32,CL		RCR R/M32,CL	Rotate 33 bits (CF, r/m32) right CL times.
RCR	R/M32,IMM8		RCR R/M32,IMM8	Rotate 33 bits (CF, r/m32) right imm8 times.
RCR	R/M64,1		RCR R/M64,1	Rotate 65 bits (CF, r/m64) right once. Uses a 6 bit count.
RCR	R/M64,CL		RCR R/M64,CL	Rotate 65 bits (CF, r/m64) right CL times. Uses a 6 bit count.
RCR	R/M64,IMM8		RCR R/M64,IMM8	Rotate 65 bits (CF, r/m64) right imm8 times. Uses a 6 bit count.
RCR	R/M8,1		RCR R/M8,1	Rotate 9 bits (CF, r/m8) right once.
RCR	R/M8,CL		RCR R/M8,CL	Rotate 9 bits (CF, r/m8) right CL times.
RCR	R/M8,IMM8		RCR R/M8,IMM8	Rotate 9 bits (CF, r/m8) right imm8 times.
RDFSBASE	R32	FSGSBASE	RDFSBASE R32	Load the 32-bit destination register with the FS base address.
RDFSBASE	R64	FSGSBASE	RDFSBASE R64	Load the 64-bit destination register with the FS base address.
RDGSBASE	R32	FSGSBASE	RDGSBASE R32	Load the 32-bit destination register with the GS base address.
RDGSBASE	R64	FSGSBASE	RDGSBASE R64	Load the 64-bit destination register with the GS base address.
RDMSR			RDMSR 	Read MSR specified by ECX into EDX:EAX.
RDPID	R32	RDPID	RDPID R32	Read IA32_TSC_AUX into r32.
RDPID	R64	RDPID	RDPID R64	Read IA32_TSC_AUX into r64.
RDPKRU			RDPKRU 	Reads PKRU into EAX.
RDPMC			RDPMC 	Read performance-monitoring counter specified by ECX into EDX:EAX.
RDRAND	R16	RDRAND	RDRAND R16	Read a 16-bit random number and store in the destination register.
RDRAND	R32	RDRAND	RDRAND R32	Read a 32-bit random number and store in the destination register.
RDRAND	R64	RDRAND	RDRAND R64	Read a 64-bit random number and store in the destination register.
RDSEED	R16	RDSEED	RDSEED R16	Read a 16-bit NIST SP800-90B &amp; C compliant random value and store in the destination register.
RDSEED	R32	RDSEED	RDSEED R32	Read a 32-bit NIST SP800-90B &amp; C compliant random value and store in the destination register.
RDSEED	R64	RDSEED	RDSEED R64	Read a 64-bit NIST SP800-90B &amp; C compliant random value and store in the destination register.
RDTSC			RDTSC 	Read time-stamp counter into EDX:EAX.
RDTSCP			RDTSCP 	Read 64-bit time-stamp counter and IA32_TSC_AUX value into EDX:EAX and ECX.
RET			RET 	Near return to calling procedure.
RET	IMM16		RET IMM16	Near return to calling procedure and pop imm16 bytes from stack.
ROL	R/M8,1		ROL R/M8,1	Rotate 8 bits r/m8 left once.
ROL	R/M8,CL		ROL R/M8,CL	Rotate 8 bits r/m8 left CL times.
ROL	R/M8,IMM8		ROL R/M8,IMM8	Rotate 8 bits r/m8 left imm8 times.
RORX	R32,R/M32,IMM8	BMI2	RORX R32,R/M32,IMM8	Rotate 32-bit r/m32 right imm8 times without affecting arithmetic flags.
RORX	R64,R/M64,IMM8	BMI2	RORX R64,R/M64,IMM8	Rotate 64-bit r/m64 right imm8 times without affecting arithmetic flags.
ROUNDPD	XMM,XMM/M128,IMM8	SSE4_1	ROUNDPD XMM1,XMM2/M128,IMM8	Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1.  The rounding mode is determined by imm8.
ROUNDPS	XMM,XMM/M128,IMM8	SSE4_1	ROUNDPS XMM1,XMM2/M128,IMM8	Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1.  The rounding mode is determined by imm8.
ROUNDSD	XMM,XMM/M64,IMM8	SSE4_1	ROUNDSD XMM1,XMM2/M64,IMM8	Round the low packed double precision floating-point value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8.
ROUNDSS	XMM,XMM/M32,IMM8	SSE4_1	ROUNDSS XMM1,XMM2/M32,IMM8	Round the low packed single precision floating-point value in xmm2/m32 and place the result in xmm1.  The rounding mode is determined by imm8.
RSM			RSM 	Resume operation of interrupted program.
RSQRTPS	XMM,XMM/M128	SSE	RSQRTPS XMM1,XMM2/M128	Computes the approximate reciprocals of the square roots of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1.
RSQRTSS	XMM,XMM/M32	SSE	RSQRTSS XMM1,XMM2/M32	Computes the approximate reciprocal of the square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1.
SAHF			SAHF 	Loads SF, ZF, AF, PF, and CF from AH into EFLAGS register.
SAL	R/M16,1		SAL R/M16,1	Multiply r/m16 by 2, once.
SAL	R/M16,CL		SAL R/M16,CL	Multiply r/m16 by 2, CL times.
SAL	R/M16,IMM8		SAL R/M16,IMM8	Multiply r/m16 by 2, imm8 times.
SAL	R/M32,1		SAL R/M32,1	Multiply r/m32 by 2, once.
SAL	R/M32,CL		SAL R/M32,CL	Multiply r/m32 by 2, CL times.
SAL	R/M32,IMM8		SAL R/M32,IMM8	Multiply r/m32 by 2, imm8 times.
SAL	R/M64,1		SAL R/M64,1	Multiply r/m64 by 2, once.
SAL	R/M64,CL		SAL R/M64,CL	Multiply r/m64 by 2, CL times.
SAL	R/M64,IMM8		SAL R/M64,IMM8	Multiply r/m64 by 2, imm8 times.
SAL	R/M8,1		SAL R/M8,1	Multiply r/m8 by 2, once.
SAL	R/M8,CL		SAL R/M8,CL	Multiply r/m8 by 2, CL times.
SAL	R/M8,IMM8		SAL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SAR	R/M16,1		SAR R/M16,1	Signed divide r/m16 by 2, once.
SAR	R/M16,CL		SAR R/M16,CL	Signed divide r/m16 by 2, CL times.
SAR	R/M16,IMM8		SAR R/M16,IMM8	Signed divide r/m16 by 2, imm8 times.
SAR	R/M32,1		SAR R/M32,1	Signed divide r/m32 by 2, once.
SAR	R/M32,CL		SAR R/M32,CL	Signed divide r/m32 by 2, CL times.
SAR	R/M32,IMM8		SAR R/M32,IMM8	Signed divide r/m32 by 2, imm8 times.
SAR	R/M64,1		SAR R/M64,1	Signed divide r/m64 by 2, once.
SAR	R/M64,CL		SAR R/M64,CL	Signed divide r/m64 by 2, CL times.
SAR	R/M64,IMM8		SAR R/M64,IMM8	Signed divide r/m64 by 2, imm8 times
SAR	R/M8,1		SAR R/M8,1	Signed divide r/m8 by 2, once.
SAR	R/M8,CL		SAR R/M8,CL	Signed divide r/m8 by 2, CL times.
SAR	R/M8,IMM8		SAR R/M8,IMM8	Signed divide r/m8 by 2, imm8 time.
SBB	AL,IMM8		SBB AL,IMM8	Subtract with borrow imm8 from AL.
SBB	AX,IMM16		SBB AX,IMM16	Subtract with borrow imm16 from AX.
SBB	EAX,IMM32		SBB EAX,IMM32	Subtract with borrow imm32 from EAX.
SBB	R/M16,IMM16		SBB R/M16,IMM16	Subtract with borrow imm16 from r/m16.
SBB	R/M16,IMM8		SBB R/M16,IMM8	Subtract with borrow sign-extended imm8 from r/m16.
SBB	R/M16,R16		SBB R/M16,R16	Subtract with borrow r16 from r/m16.
SBB	R/M32,IMM32		SBB R/M32,IMM32	Subtract with borrow imm32 from r/m32.
SBB	R/M32,IMM8		SBB R/M32,IMM8	Subtract with borrow sign-extended imm8 from r/m32.
SBB	R/M32,R32		SBB R/M32,R32	Subtract with borrow r32 from r/m32.
SBB	R/M64,IMM32		SBB R/M64,IMM32	Subtract with borrow sign-extended imm32 to 64-bits from r/m64.
SBB	R/M64,IMM8		SBB R/M64,IMM8	Subtract with borrow sign-extended imm8 from r/m64.
SBB	R/M64,R64		SBB R/M64,R64	Subtract with borrow r64 from r/m64.
SBB	R/M8,IMM8		SBB R/M8,IMM8	Subtract with borrow imm8 from r/m8.
SBB	R/M8,R8		SBB R/M8,R8	Subtract with borrow r8 from r/m8.
SBB	R16,R/M16		SBB R16,R/M16	Subtract with borrow r/m16 from r16.
SBB	R32,R/M32		SBB R32,R/M32	Subtract with borrow r/m32 from r32.
SBB	R64,R/M64		SBB R64,R/M64	Subtract with borrow r/m64 from r64.
SBB	R8,R/M8		SBB R8,R/M8	Subtract with borrow r/m8 from r8.
SBB	RAX,IMM32		SBB RAX,IMM32	Subtract with borrow sign-extended imm.32 to 64-bits from RAX.
SCAS	M16		SCAS M16	Compare AX with word at ES:(E)DI or RDI, then set status flags.
SCAS	M32		SCAS M32	Compare EAX with doubleword at ES(E)DI or RDI then set status flags.
SCAS	M64		SCAS M64	Compare RAX with quadword at RDI or EDI then set status flags.
SCAS	M8		SCAS M8	Compare AL with byte at ES:(E)DI or RDI, then set status flags.
SCASB			SCASB 	Compare AL with byte at ES:(E)DI or RDI then set status flags.
SCASD			SCASD 	Compare EAX with doubleword at ES:(E)DI or RDI then set status flags.
SCASW			SCASW 	Compare AX with word at ES:(E)DI or RDI then set status flags.
SFENCE			SFENCE 	Serializes store operations.
SGDT	MEM		SGDT M	Store GDTR to m.
SHA1MSG1	XMM,XMM/M128	SHA	SHA1MSG1 XMM1,XMM2/M128	Performs an intermediate calculation for the next four SHA1 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
SHA1MSG2	XMM,XMM/M128	SHA	SHA1MSG2 XMM1,XMM2/M128	Performs the final calculation for the next four SHA1 message dwords using intermediate results from xmm1 and the previous message dwords from xmm2/m128, storing the result in xmm1.
SHA1NEXTE	XMM,XMM/M128	SHA	SHA1NEXTE XMM1,XMM2/M128	Calculates SHA1 state variable E after four rounds of operation from the current SHA1 state variable A in xmm1. The calculated value of the SHA1 state variable E is added to the scheduled dwords in xmm2/m128, and stored with some of the scheduled dwords in xmm1.
SHA1RNDS4	XMM,XMM/M128,IMM8	SHA	SHA1RNDS4 XMM1,XMM2/M128,IMM8	Performs four rounds of SHA1 operation operating on SHA1 state (A, B, C, D) from xmm1, with a pre-computed sum of the next 4 round message dwords and state variable E from xmm2/m128. The immediate byte controls logic functions and round constants.
SHA256MSG1	XMM,XMM/M128	SHA	SHA256MSG1 XMM1,XMM2/M128	Performs an intermediate calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
SHA256MSG2	XMM,XMM/M128	SHA	SHA256MSG2 XMM1,XMM2/M128	Performs the final calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1.
SHA256RNDS2	XMM,XMM/M128,XMM_ZERO	SHA	SHA256RNDS2 XMM1,XMM2/M128,XMM_ZERO	Perform 2 rounds of SHA256 operation using an initial SHA256 state (C, D, G, H) from xmm1, an initial SHA256 state (A, B, E, F) from xmm2/m128, and a pre-computed sum of the next 2 round mes-sage dwords and the corresponding round constants from the implicit operand XMM0, storing the updated SHA256 state (A, B, E, F) result in xmm1.
SHL	R/M16,1		SHL R/M16,1	Multiply r/m16 by 2, once.
SHL	R/M16,CL		SHL R/M16,CL	Multiply r/m16 by 2, CL times.
SHL	R/M16,IMM8		SHL R/M16,IMM8	Multiply r/m16 by 2, imm8 times.
SHL	R/M32,1		SHL R/M32,1	Multiply r/m32 by 2, once.
SHL	R/M8,1		SHL R/M8,1	Multiply r/m8 by 2, once.
SHL	R/M8,CL		SHL R/M8,CL	Multiply r/m8 by 2, CL times.
SHL	R/M8,IMM8		SHL R/M8,IMM8	Multiply r/m8 by 2, imm8 times.
SHLD	R/M16,R16,CL		SHLD R/M16,R16,CL	Shift r/m16 to left CL places while shifting bits from r16 in from the right.
SHLD	R/M16,R16,IMM8		SHLD R/M16,R16,IMM8	Shift r/m16 to left imm8 places while shifting bits from r16 in from the right.
SHLD	R/M32,R32,CL		SHLD R/M32,R32,CL	Shift r/m32 to left CL places while shifting bits from r32 in from the right.
SHLD	R/M32,R32,IMM8		SHLD R/M32,R32,IMM8	Shift r/m32 to left imm8 places while shifting bits from r32 in from the right.
SHLD	R/M64,R64,CL		SHLD R/M64,R64,CL	Shift r/m64 to left CL places while shifting bits from r64 in from the right.
SHLD	R/M64,R64,IMM8		SHLD R/M64,R64,IMM8	Shift r/m64 to left imm8 places while shifting bits from r64 in from the right.
SHRD	R/M16,R16,CL		SHRD R/M16,R16,CL	Shift r/m16 to right CL places while shifting bits from r16 in from the left.
SHRD	R/M16,R16,IMM8		SHRD R/M16,R16,IMM8	Shift r/m16 to right imm8 places while shifting bits from r16 in from the left.
SHRD	R/M32,R32,CL		SHRD R/M32,R32,CL	Shift r/m32 to right CL places while shifting bits from r32 in from the left.
SHRD	R/M32,R32,IMM8		SHRD R/M32,R32,IMM8	Shift r/m32 to right imm8 places while shifting bits from r32 in from the left.
SHRD	R/M64,R64,CL		SHRD R/M64,R64,CL	Shift r/m64 to right CL places while shifting bits from r64 in from the left.
SHRD	R/M64,R64,IMM8		SHRD R/M64,R64,IMM8	Shift r/m64 to right imm8 places while shifting bits from r64 in from the left.
SHUFPD	XMM,XMM/M128,IMM8	SSE2	SHUFPD XMM1,XMM2/M128,IMM8	Shuffle two pairs of double-precision floating-point values from xmm1 and xmm2/m128 using imm8 to select from each pair, interleaved result is stored in xmm1.
SHUFPS	XMM,XMM/M128,IMM8	SSE	SHUFPS XMM1,XMM3/M128,IMM8	Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1.
SIDT	MEM		SIDT M	Store IDTR to m.
SLDT	R/M16		SLDT R/M16	Stores segment selector from LDTR in r/m16.
SLDT	R64/M16		SLDT R64/M16	Stores segment selector from LDTR in r64/m16.
SMSW	R/M16		SMSW R/M16	Store machine status word to r/m16.
SMSW	R32/M16		SMSW R32/M16	Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined.
SMSW	R64/M16		SMSW R64/M16	Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined.
SQRTPD	XMM,XMM/M128	SSE2	SQRTPD XMM1,XMM2/M128	Computes Square Roots of the packed double-precision floating-point values in xmm2/m128 and stores the result in xmm1.
SQRTPS	XMM,XMM/M128	SSE	SQRTPS XMM1,XMM2/M128	Computes Square Roots of the packed single-precision floating-point values in xmm2/m128 and stores the result in xmm1.
SQRTSD	XMM,XMM/M64	SSE2	SQRTSD XMM1,XMM2/M64	Computes square root of the low double-precision floating-point value in xmm2/m64 and stores the results in xmm1.
SQRTSS	XMM,XMM/M32	SSE	SQRTSS XMM1,XMM2/M32	Computes square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1.
STAC			STAC 	Set the AC flag in the EFLAGS register.
STC			STC 	Set CF flag.
STD			STD 	Set DF flag.
STI			STI 	Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction.
STMXCSR	M32	SSE	STMXCSR M32	Store contents of MXCSR register to m32.
STOS	M16		STOS M16	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
STOS	M32		STOS M32	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
STOS	M64		STOS M64	Store RAX at address RDI or EDI.
STOS	M8		STOS M8	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
STOSB			STOSB 	For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI.
STOSD			STOSD 	For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI.
STOSQ			STOSQ 	Store RAX at address RDI or EDI.
STOSW			STOSW 	For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI.
STR	R/M16		STR R/M16	Stores segment selector from TR in r/m16.
SUB	AL,IMM8		SUB AL,IMM8	Subtract imm8 from AL.
SUB	AX,IMM16		SUB AX,IMM16	Subtract imm16 from AX.
SUB	EAX,IMM32		SUB EAX,IMM32	Subtract imm32 from EAX.
SUB	R/M16,IMM16		SUB R/M16,IMM16	Subtract imm16 from r/m16.
SUB	R/M16,IMM8		SUB R/M16,IMM8	Subtract sign-extended imm8 from r/m16.
SUB	R/M16,R16		SUB R/M16,R16	Subtract r16 from r/m16.
SUB	R/M32,IMM32		SUB R/M32,IMM32	Subtract imm32 from r/m32.
SUB	R/M32,IMM8		SUB R/M32,IMM8	Subtract sign-extended imm8 from r/m32.
SUB	R/M32,R32		SUB R/M32,R32	Subtract r32 from r/m32.
SUB	R/M64,IMM32		SUB R/M64,IMM32	Subtract imm32 sign-extended to 64-bits from r/m64.
SUB	R/M64,IMM8		SUB R/M64,IMM8	Subtract sign-extended imm8 from r/m64.
SUB	R/M64,R64		SUB R/M64,R64	Subtract r64 from r/m64.
SUB	R/M8,IMM8		SUB R/M8,IMM8	Subtract imm8 from r/m8.
SUB	R/M8,R8		SUB R/M8,R8	Subtract r8 from r/m8.
SUB	R16,R/M16		SUB R16,R/M16	Subtract r/m16 from r16.
SUB	R32,R/M32		SUB R32,R/M32	Subtract r/m32 from r32.
SUB	R64,R/M64		SUB R64,R/M64	Subtract r/m64 from r64.
SUB	R8,R/M8		SUB R8,R/M8	Subtract r/m8 from r8.
SUB	RAX,IMM32		SUB RAX,IMM32	Subtract imm32 sign-extended to 64-bits from RAX.
SUBPD	XMM,XMM/M128	SSE2	SUBPD XMM1,XMM2/M128	Subtract packed double-precision floating-point values in xmm2/mem from xmm1 and store result in xmm1.
SUBPS	XMM,XMM/M128	SSE	SUBPS XMM1,XMM2/M128	Subtract packed single-precision floating-point values in xmm2/mem from xmm1 and store result in xmm1.
SUBSD	XMM,XMM/M64	SSE2	SUBSD XMM1,XMM2/M64	Subtract the low double-precision floating-point value in xmm2/m64 from xmm1 and store the result in xmm1.
SUBSS	XMM,XMM/M32	SSE	SUBSS XMM1,XMM2/M32	Subtract the low single-precision floating-point value in xmm2/m32 from xmm1 and store the result in xmm1.
SWAPGS			SWAPGS 	Exchanges the current GS base register value with the value contained in MSR address C0000102H.
SYSCALL			SYSCALL 	Fast call to privilege level 0 system procedures.
SYSENTER			SYSENTER 	Fast call to privilege level 0 system procedures.
SYSEXIT			SYSEXIT 	Fast return to privilege level 3 user code.
SYSRET			SYSRET 	Return to compatibility mode from fast system call
TEST	AL,IMM8		TEST AL,IMM8	AND imm8 with AL; set SF, ZF, PF according to result.
TEST	AX,IMM16		TEST AX,IMM16	AND imm16 with AX; set SF, ZF, PF according to result.
TEST	EAX,IMM32		TEST EAX,IMM32	AND imm32 with EAX; set SF, ZF, PF according to result.
TEST	R/M16,IMM16		TEST R/M16,IMM16	AND imm16 with r/m16; set SF, ZF, PF according to result.
TEST	R/M16,R16		TEST R/M16,R16	AND r16 with r/m16; set SF, ZF, PF according to result.
TEST	R/M32,IMM32		TEST R/M32,IMM32	AND imm32 with r/m32; set SF, ZF, PF according to result.
TEST	R/M32,R32		TEST R/M32,R32	AND r32 with r/m32; set SF, ZF, PF according to result.
TEST	R/M64,IMM32		TEST R/M64,IMM32	AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result.
TEST	R/M64,R64		TEST R/M64,R64	AND r64 with r/m64; set SF, ZF, PF according to result.
TEST	R/M8,IMM8		TEST R/M8,IMM8	AND imm8 with r/m8; set SF, ZF, PF according to result.
TEST	R/M8,R8		TEST R/M8,R8	AND r8 with r/m8; set SF, ZF, PF according to result.
TEST	RAX,IMM32		TEST RAX,IMM32	AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result.
TZCNT	R16,R/M16	BMI1	TZCNT R16,R/M16	Count the number of trailing zero bits in r/m16, return result in r16.
TZCNT	R32,R/M32	BMI1	TZCNT R32,R/M32	Count the number of trailing zero bits in r/m32, return result in r32.
TZCNT	R64,R/M64	BMI1	TZCNT R64,R/M64	Count the number of trailing zero bits in r/m64, return result in r64.
UCOMISD	XMM,XMM/M64	SSE2	UCOMISD XMM1,XMM2/M64	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
UCOMISS	XMM,XMM/M32	SSE	UCOMISS XMM1,XMM2/M32	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
UD2			UD2 	Raise invalid opcode exception.
VADDPD	XMM,XMM,XMM/M128	AVX	VADDPD XMM1,XMM2,XMM3/M128	Add packed double-precision floating-point values from xmm3/mem to xmm2 and store result in xmm1.
VADDPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VADDPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Add packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1.
VADDPD	YMM,YMM,YMM/M256	AVX	VADDPD YMM1,YMM2,YMM3/M256	Add packed double-precision floating-point values from ymm3/mem to ymm2 and store result in ymm1.
VADDPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VADDPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Add packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1.
VADDPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VADDPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Add packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1.
VADDPS	XMM,XMM,XMM/M128	AVX	VADDPS XMM1,XMM2,XMM3/M128	Add packed single-precision floating-point values from xmm3/m128 to xmm2 and store result in xmm1.
VADDPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VADDPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Add packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1 with writemask k1.
VADDPS	YMM,YMM,YMM/M256	AVX	VADDPS YMM1,YMM2,YMM3/M256	Add packed single-precision floating-point values from ymm3/m256 to ymm2 and store result in ymm1.
VADDPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VADDPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Add packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1 with writemask k1.
VADDPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VADDPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Add packed single-precision floating-point values from zmm3/m512/m32bcst to zmm2 and store result in zmm1 with writemask k1.
VADDSD	XMM,XMM,XMM/M64	AVX	VADDSD XMM1,XMM2,XMM3/M64	Add the low double-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
VADDSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VADDSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Add the low double-precision floating-point value from xmm3/m64 to xmm2 and store the result in xmm1 with writemask k1.
VADDSS	XMM,XMM,XMM/M32	AVX	VADDSS XMM1,XMM2,XMM3/M32	Add the low single-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1.
VADDSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VADDSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Add the low single-precision floating-point value from xmm3/m32 to xmm2 and store the result in xmm1with writemask k1.
VADDSUBPD	XMM,XMM,XMM/M128	AVX	VADDSUBPD XMM1,XMM2,XMM3/M128	Add/subtract packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VADDSUBPD	YMM,YMM,YMM/M256	AVX	VADDSUBPD YMM1,YMM2,YMM3/M256	Add / subtract packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
VADDSUBPS	XMM,XMM,XMM/M128	AVX	VADDSUBPS XMM1,XMM2,XMM3/M128	Add/subtract single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1.
VADDSUBPS	YMM,YMM,YMM/M256	AVX	VADDSUBPS YMM1,YMM2,YMM3/M256	Add / subtract single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1.
VAESDEC	XMM,XMM,XMM/M128	AES,AVX	VAESDEC XMM1,XMM2,XMM3/M128	Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
VAESDECLAST	XMM,XMM,XMM/M128	AES,AVX	VAESDECLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1.
VAESENC	XMM,XMM,XMM/M128	AES,AVX	VAESENC XMM1,XMM2,XMM3/M128	Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1.
VAESENCLAST	XMM,XMM,XMM/M128	AES,AVX	VAESENCLAST XMM1,XMM2,XMM3/M128	Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1.
VAESIMC	XMM,XMM/M128	AES,AVX	VAESIMC XMM1,XMM2/M128	Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1.
VAESKEYGENASSIST	XMM,XMM/M128,IMM8	AES,AVX	VAESKEYGENASSIST XMM1,XMM2/M128,IMM8	Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1.
VALIGND	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VALIGND XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Shift right and merge vectors xmm2 and xmm3/m128/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask.
VALIGND	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VALIGND YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shift right and merge vectors ymm2 and ymm3/m256/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask.
VALIGND	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VALIGND ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shift right and merge vectors zmm2 and zmm3/m512/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask.
VALIGNQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VALIGNQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Shift right and merge vectors xmm2 and xmm3/m128/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask.
VALIGNQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VALIGNQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shift right and merge vectors ymm2 and ymm3/m256/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask.
VALIGNQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VALIGNQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shift right and merge vectors zmm2 and zmm3/m512/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask.
VANDNPD	XMM,XMM,XMM/M128	AVX	VANDNPD XMM1,XMM2,XMM3/M128	Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/mem.
VANDNPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512DQ	VANDNPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VANDNPD	YMM,YMM,YMM/M256	AVX	VANDNPD YMM1,YMM2,YMM3/M256	Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/mem.
VANDNPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512DQ	VANDNPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VANDNPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512DQ	VANDNPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical AND NOT of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
VANDNPS	XMM,XMM,XMM/M128	AVX	VANDNPS XMM1,XMM2,XMM3/M128	Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm2 and xmm3/mem.
VANDNPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512DQ	VANDNPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VANDNPS	YMM,YMM,YMM/M256	AVX	VANDNPS YMM1,YMM2,YMM3/M256	Return the bitwise logical AND NOT of packed single-precision floating-point values in ymm2 and ymm3/mem.
VANDNPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512DQ	VANDNPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VANDNPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512DQ	VANDNPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
VANDPD	XMM,XMM,XMM/M128	AVX	VANDPD XMM1,XMM2,XMM3/M128	Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/mem.
VANDPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512DQ	VANDPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VANDPD	YMM,YMM,YMM/M256	AVX	VANDPD YMM1,YMM2,YMM3/M256	Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/mem.
VANDPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512DQ	VANDPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VANDPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512DQ	VANDPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical AND of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
VANDPS	XMM,XMM,XMM/M128	AVX	VANDPS XMM1,XMM2,XMM3/M128	Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/mem.
VANDPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512DQ	VANDPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VANDPS	YMM,YMM,YMM/M256	AVX	VANDPS YMM1,YMM2,YMM3/M256	Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/mem.
VANDPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512DQ	VANDPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VANDPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512DQ	VANDPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical AND of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
VBLENDMPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VBLENDMPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Blend double-precision vector xmm2 and double-precision vector xmm3/m128/m64bcst and store the result in xmm1, under control mask.
VBLENDMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VBLENDMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Blend double-precision vector ymm2 and double-precision vector ymm3/m256/m64bcst and store the result in ymm1, under control mask.
VBLENDMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VBLENDMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Blend double-precision vector zmm2 and double-precision vector zmm3/m512/m64bcst and store the result in zmm1, under control mask.
VBLENDMPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VBLENDMPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Blend single-precision vector xmm2 and single-precision vector xmm3/m128/m32bcst and store the result in xmm1, under control mask.
VBLENDMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VBLENDMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Blend single-precision vector ymm2 and single-precision vector ymm3/m256/m32bcst and store the result in ymm1, under control mask.
VBLENDMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VBLENDMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Blend single-precision vector zmm2 and single-precision vector zmm3/m512/m32bcst using k1 as select control and store the result in zmm1.
VBLENDPD	XMM,XMM,XMM/M128,IMM8	AVX	VBLENDPD XMM1,XMM2,XMM3/M128,IMM8	Select packed double-precision floating-point Values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VBLENDPD	YMM,YMM,YMM/M256,IMM8	AVX	VBLENDPD YMM1,YMM2,YMM3/M256,IMM8	Select packed double-precision floating-point Values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
VBLENDPS	XMM,XMM,XMM/M128,IMM8	AVX	VBLENDPS XMM1,XMM2,XMM3/M128,IMM8	Select packed single-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1.
VBLENDPS	YMM,YMM,YMM/M256,IMM8	AVX	VBLENDPS YMM1,YMM2,YMM3/M256,IMM8	Select packed single-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1.
VBLENDVPD	XMM,XMM,XMM/M128,XMM	AVX	VBLENDVPD XMM1,XMM2,XMM3/M128,XMM4	Conditionally copy double-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4.
VBLENDVPD	YMM,YMM,YMM/M256,YMM	AVX	VBLENDVPD YMM1,YMM2,YMM3/M256,YMM4	Conditionally copy double-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4.
VBLENDVPS	XMM,XMM,XMM/M128,XMM	AVX	VBLENDVPS XMM1,XMM2,XMM3/M128,XMM4	Conditionally copy single-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4.
VBLENDVPS	YMM,YMM,YMM/M256,YMM	AVX	VBLENDVPS YMM1,YMM2,YMM3/M256,YMM4	Conditionally copy single-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4.
VBROADCASTF128	YMM,M128	AVX	VBROADCASTF128 YMM1,M128	Broadcast 128 bits of floating-point data in mem to low and high 128-bits in ymm1.
VBROADCASTF32X2	YMM{K}{Z},XMM/M64	AVX512VL,AVX512DQ	VBROADCASTF32X2 YMM1{K1}{Z},XMM2/M64	Broadcast two single-precision floating-point elements in xmm2/m64 to locations in ymm1 using writemask k1.
VBROADCASTF32X2	ZMM{K}{Z},XMM/M64	AVX512DQ	VBROADCASTF32X2 ZMM1{K1}{Z},XMM2/M64	Broadcast two single-precision floating-point elements in xmm2/m64 to locations in zmm1 using writemask k1.
VBROADCASTF32X4	YMM{K}{Z},M128	AVX512VL,AVX512F	VBROADCASTF32X4 YMM1{K1}{Z},M128	Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in ymm1 using writemask k1.
VBROADCASTF32X4	ZMM{K}{Z},M128	AVX512F	VBROADCASTF32X4 ZMM1{K1}{Z},M128	Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in zmm1 using writemask k1.
VBROADCASTSD	YMM,M64	AVX	VBROADCASTSD YMM1,M64	Broadcast double-precision floating-point element in mem to four locations in ymm1.
VBROADCASTSD	YMM{K}{Z},XMM/M64	AVX512VL,AVX512F	VBROADCASTSD YMM1{K1}{Z},XMM2/M64	Broadcast low double-precision floating-point element in xmm2/m64 to four locations in ymm1 using writemask k1.
VBROADCASTSD	ZMM{K}{Z},XMM/M64	AVX512F	VBROADCASTSD ZMM1{K1}{Z},XMM2/M64	Broadcast low double-precision floating-point element in xmm2/m64 to eight locations in zmm1 using writemask k1.
VBROADCASTSS	XMM,M32	AVX	VBROADCASTSS XMM1,M32	Broadcast single-precision floating-point element in mem to four locations in xmm1.
VBROADCASTSS	XMM{K}{Z},XMM/M32	AVX512VL,AVX512F	VBROADCASTSS XMM1{K1}{Z},XMM2/M32	Broadcast low single-precision floating-point element in xmm2/m32 to all locations in xmm1 using writemask k1.
VBROADCASTSS	YMM,M32	AVX	VBROADCASTSS YMM1,M32	Broadcast single-precision floating-point element in mem to eight locations in ymm1.
VBROADCASTSS	YMM{K}{Z},XMM/M32	AVX512VL,AVX512F	VBROADCASTSS YMM1{K1}{Z},XMM2/M32	Broadcast low single-precision floating-point element in xmm2/m32 to all locations in ymm1 using writemask k1.
VBROADCASTSS	ZMM{K}{Z},XMM/M32	AVX512F	VBROADCASTSS ZMM1{K1}{Z},XMM2/M32	Broadcast low single-precision floating-point element in xmm2/m32 to all locations in zmm1 using writemask k1.
VCMPPD	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VCMPPD K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed double-precision floating-point values in xmm3/m128/m64bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPD	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VCMPPD K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed double-precision floating-point values in ymm3/m256/m64bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPD	K{K},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512F	VCMPPD K1{K2},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Compare packed double-precision floating-point values in zmm3/m512/m64bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPD	XMM,XMM,XMM/M128,IMM8	AVX	VCMPPD XMM1,XMM2,XMM3/M128,IMM8	Compare packed double-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPD	YMM,YMM,YMM/M256,IMM8	AVX	VCMPPD YMM1,YMM2,YMM3/M256,IMM8	Compare packed double-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPS	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VCMPPS K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed single-precision floating-point values in xmm3/m128/m32bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPS	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VCMPPS K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed single-precision floating-point values in ymm3/m256/m32bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPS	K{K},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512F	VCMPPS K1{K2},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Compare packed single-precision floating-point values in zmm3/m512/m32bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPPS	XMM,XMM,XMM/M128,IMM8	AVX	VCMPPS XMM1,XMM2,XMM3/M128,IMM8	Compare packed single-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPPS	YMM,YMM,YMM/M256,IMM8	AVX	VCMPPS YMM1,YMM2,YMM3/M256,IMM8	Compare packed single-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate.
VCMPSD	K{K},XMM,XMM/M64{SAE},IMM8	AVX512F	VCMPSD K1{K2},XMM2,XMM3/M64{SAE},IMM8	Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPSD	XMM,XMM,XMM/M64,IMM8	AVX	VCMPSD XMM1,XMM2,XMM3/M64,IMM8	Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate.
VCMPSS	K{K},XMM,XMM/M32{SAE},IMM8	AVX512F	VCMPSS K1{K2},XMM2,XMM3/M32{SAE},IMM8	Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1.
VCMPSS	XMM,XMM,XMM/M32,IMM8	AVX	VCMPSS XMM1,XMM2,XMM3/M32,IMM8	Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate.
VCOMISD	XMM,XMM/M64	AVX	VCOMISD XMM1,XMM2/M64	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VCOMISD	XMM,XMM/M64{SAE}	AVX512F	VCOMISD XMM1,XMM2/M64{SAE}	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VCOMISS	XMM,XMM/M32	AVX	VCOMISS XMM1,XMM2/M32	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VCOMISS	XMM,XMM/M32{SAE}	AVX512F	VCOMISS XMM1,XMM2/M32{SAE}	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VCOMPRESSPD	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VCOMPRESSPD XMM1/M128{K1}{Z},XMM2	Compress packed double-precision floating-point values from xmm2 to xmm1/m128 using writemask k1.
VCOMPRESSPD	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VCOMPRESSPD YMM1/M256{K1}{Z},YMM2	Compress packed double-precision floating-point values from ymm2 to ymm1/m256 using writemask k1.
VCOMPRESSPD	ZMM/M512{K}{Z},ZMM	AVX512F	VCOMPRESSPD ZMM1/M512{K1}{Z},ZMM2	Compress packed double-precision floating-point values from zmm2 using control mask k1 to zmm1/m512.
VCOMPRESSPS	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VCOMPRESSPS XMM1/M128{K1}{Z},XMM2	Compress packed single-precision floating-point values from xmm2 to xmm1/m128 using writemask k1.
VCOMPRESSPS	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VCOMPRESSPS YMM1/M256{K1}{Z},YMM2	Compress packed single-precision floating-point values from ymm2 to ymm1/m256 using writemask k1.
VCOMPRESSPS	ZMM/M512{K}{Z},ZMM	AVX512F	VCOMPRESSPS ZMM1/M512{K1}{Z},ZMM2	Compress packed single-precision floating-point values from zmm2 using control mask k1 to zmm1/m512.
VCVTDQ2PD	XMM,XMM/M64	AVX	VCVTDQ2PD XMM1,XMM2/M64	Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1.
VCVTDQ2PD	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTDQ2PD XMM1{K1}{Z},XMM2/M128/M32BCST	Convert 2 packed signed doubleword integers from xmm2/m128/m32bcst to eight packed double-precision floating-point values in xmm1 with writemask k1.
VCVTDQ2PD	YMM,XMM/M128	AVX	VCVTDQ2PD YMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed double-precision floating-point values in ymm1.
VCVTDQ2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTDQ2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert 4 packed signed doubleword integers from xmm2/m128/m32bcst to 4 packed double-precision floating-point values in ymm1 with writemask k1.
VCVTDQ2PD	ZMM{K}{Z},YMM/M256/M32BCST	AVX512F	VCVTDQ2PD ZMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1.
VCVTDQ2PS	XMM,XMM/M128	AVX	VCVTDQ2PS XMM1,XMM2/M128	Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1.
VCVTDQ2PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTDQ2PS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed signed doubleword integers from xmm2/m128/m32bcst to four packed single-precision floating-point values in xmm1with writemask k1.
VCVTDQ2PS	YMM,YMM/M256	AVX	VCVTDQ2PS YMM1,YMM2/M256	Convert eight packed signed doubleword integers from ymm2/mem to eight packed single-precision floating-point values in ymm1.
VCVTDQ2PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTDQ2PS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed single-precision floating-point values in ymm1with writemask k1.
VCVTDQ2PS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512F	VCVTDQ2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed signed doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1with writemask k1.
VCVTPD2DQ	XMM,XMM/M128	AVX	VCVTPD2DQ XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1.
VCVTPD2DQ	XMM,YMM/M256	AVX	VCVTPD2DQ XMM1,YMM2/M256	Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1.
VCVTPD2DQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VCVTPD2DQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 subject to writemask k1.
VCVTPD2DQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VCVTPD2DQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 subject to writemask k1.
VCVTPD2DQ	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512F	VCVTPD2DQ YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 subject to writemask k1.
VCVTPD2PS	XMM,XMM/M128	AVX	VCVTPD2PS XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two single-precision floating-point values in xmm1.
VCVTPD2PS	XMM,YMM/M256	AVX	VCVTPD2PS XMM1,YMM2/M256	Convert four packed double-precision floating-point values in ymm2/mem to four single-precision floating-point values in xmm1.
VCVTPD2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VCVTPD2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two single-precision floating-point values in xmm1with writemask k1.
VCVTPD2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VCVTPD2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four single-precision floating-point values in xmm1with writemask k1.
VCVTPD2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512F	VCVTPD2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight single-precision floating-point values in ymm1with writemask k1.
VCVTPD2QQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTPD2QQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values from xmm2/m128/m64bcst to two packed quadword integers in xmm1 with writemask k1.
VCVTPD2QQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTPD2QQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 with writemask k1.
VCVTPD2QQ	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTPD2QQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed double-precision floating-point values from zmm2/m512/m64bcst to eight packed quadword integers in zmm1 with writemask k1.
VCVTPD2UDQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VCVTPD2UDQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 subject to writemask k1.
VCVTPD2UDQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VCVTPD2UDQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 subject to writemask k1.
VCVTPD2UDQ	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512F	VCVTPD2UDQ YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 subject to writemask k1.
VCVTPD2UQQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTPD2UQQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values from xmm2/mem to two packed unsigned quadword integers in xmm1 with writemask k1.
VCVTPD2UQQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTPD2UQQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert fourth packed double-precision floating-point values from ymm2/mem to four packed unsigned quadword integers in ymm1 with writemask k1.
VCVTPD2UQQ	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTPD2UQQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed double-precision floating-point values from zmm2/mem to eight packed unsigned quadword integers in zmm1 with writemask k1.
VCVTPH2PS	XMM,XMM/M64	F16C	VCVTPH2PS XMM1,XMM2/M64	Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point value in xmm1.
VCVTPH2PS	XMM{K}{Z},XMM/M64	AVX512VL,AVX512F	VCVTPH2PS XMM1{K1}{Z},XMM2/M64	Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point values in xmm1.
VCVTPH2PS	YMM,XMM/M128	F16C	VCVTPH2PS YMM1,XMM2/M128	Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point value in ymm1.
VCVTPH2PS	YMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VCVTPH2PS YMM1{K1}{Z},XMM2/M128	Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point values in ymm1.
VCVTPH2PS	ZMM{K}{Z},YMM/M256{SAE}	AVX512F	VCVTPH2PS ZMM1{K1}{Z},YMM2/M256{SAE}	Convert sixteen packed half precision (16-bit) floating-point values in ymm2/m256 to packed single-precision floating-point values in zmm1.
VCVTPS2DQ	XMM,XMM/M128	AVX	VCVTPS2DQ XMM1,XMM2/M128	Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1.
VCVTPS2DQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTPS2DQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 subject to writemask k1.
VCVTPS2DQ	YMM,YMM/M256	AVX	VCVTPS2DQ YMM1,YMM2/M256	Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1.
VCVTPS2DQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTPS2DQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 subject to writemask k1.
VCVTPS2DQ	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512F	VCVTPS2DQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 subject to writemask k1.
VCVTPS2PD	XMM,XMM/M64	AVX	VCVTPS2PD XMM1,XMM2/M64	Convert two packed single-precision floating-point values in xmm2/m64 to two packed double-precision floating-point values in xmm1.
VCVTPS2PD	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512F	VCVTPS2PD XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single-precision floating-point values in xmm2/m64/m32bcst to packed double-precision floating-point values in xmm1 with writemask k1.
VCVTPS2PD	YMM,XMM/M128	AVX	VCVTPS2PD YMM1,XMM2/M128	Convert four packed single-precision floating-point values in xmm2/m128 to four packed double-precision floating-point values in ymm1.
VCVTPS2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL	VCVTPS2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single-precision floating-point values in xmm2/m128/m32bcst to packed double-precision floating-point values in ymm1 with writemask k1.
VCVTPS2PD	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512F	VCVTPS2PD ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed single-precision floating-point values in ymm2/m256/b32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1.
VCVTPS2PH	XMM/M128,YMM,IMM8	F16C	VCVTPS2PH XMM1/M128,YMM2,IMM8	Convert eight packed single-precision floating-point values in ymm2 to packed half-precision (16-bit) floating-point values in xmm1/m128. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M128{K}{Z},YMM,IMM8	AVX512VL,AVX512F	VCVTPS2PH XMM1/M128{K1}{Z},YMM2,IMM8	Convert eight packed single-precision floating-point values in ymm2 to packed half-precision (16-bit) floating-point values in xmm1/m128. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M64,XMM,IMM8	F16C	VCVTPS2PH XMM1/M64,XMM2,IMM8	Convert four packed single-precision floating-point values in xmm2 to packed half-precision (16-bit) floating-point values in xmm1/m64. Imm8 provides rounding controls.
VCVTPS2PH	XMM/M64{K}{Z},XMM,IMM8	AVX512VL,AVX512F	VCVTPS2PH XMM1/M64{K1}{Z},XMM2,IMM8	Convert four packed single-precision floating-point values in xmm2 to packed half-precision (16-bit) floating-point values in xmm1/m64. Imm8 provides rounding controls.
VCVTPS2PH	YMM/M256{K}{Z},ZMM{SAE},IMM8	AVX512F	VCVTPS2PH YMM1/M256{K1}{Z},ZMM2{SAE},IMM8	Convert sixteen packed single-precision floating-point values in zmm2 to packed half-precision (16-bit) floating-point values in ymm1/m256. Imm8 provides rounding controls.
VCVTPS2QQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512DQ	VCVTPS2QQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 subject to writemask k1.
VCVTPS2QQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512DQ	VCVTPS2QQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 subject to writemask k1.
VCVTPS2QQ	ZMM{K}{Z},YMM/M256/M32BCST{ER}	AVX512DQ	VCVTPS2QQ ZMM1{K1}{Z},YMM2/M256/M32BCST{ER}	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 subject to writemask k1.
VCVTPS2UDQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTPS2UDQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 subject to writemask k1.
VCVTPS2UDQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTPS2UDQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 subject to writemask k1.
VCVTPS2UDQ	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512F	VCVTPS2UDQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 subject to writemask k1.
VCVTPS2UQQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512DQ	VCVTPS2UQQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision floating-point values from zmm2/m64/m32bcst to two packed unsigned quadword values in zmm1 subject to writemask k1.
VCVTPS2UQQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512DQ	VCVTPS2UQQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 subject to writemask k1.
VCVTPS2UQQ	ZMM{K}{Z},YMM/M256/M32BCST{ER}	AVX512DQ	VCVTPS2UQQ ZMM1{K1}{Z},YMM2/M256/M32BCST{ER}	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 subject to writemask k1.
VCVTQQ2PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTQQ2PD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed quadword integers from xmm2/m128/m64bcst to packed double-precision floating-point values in xmm1 with writemask k1.
VCVTQQ2PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTQQ2PD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed quadword integers from ymm2/m256/m64bcst to packed double-precision floating-point values in ymm1 with writemask k1.
VCVTQQ2PD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTQQ2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed quadword integers from zmm2/m512/m64bcst to eight packed double-precision floating-point values in zmm1 with writemask k1.
VCVTQQ2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTQQ2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed quadword integers from xmm2/mem to packed single-precision floating-point values in xmm1 with writemask k1.
VCVTQQ2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTQQ2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed quadword integers from ymm2/mem to packed single-precision floating-point values in xmm1 with writemask k1.
VCVTQQ2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTQQ2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed quadword integers from zmm2/mem to eight packed single-precision floating-point values in ymm1 with writemask k1.
VCVTSD2SI	R32,XMM/M64	AVX	VCVTSD2SI R32,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
VCVTSD2SI	R32,XMM/M64{ER}	AVX512F	VCVTSD2SI R32,XMM1/M64{ER}	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
VCVTSD2SI	R64,XMM/M64	AVX	VCVTSD2SI R64,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer sign-extended into r64.
VCVTSD2SI	R64,XMM/M64{ER}	AVX512F	VCVTSD2SI R64,XMM1/M64{ER}	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer sign-extended into r64.
VCVTSD2SS	XMM,XMM,XMM/M64	AVX	VCVTSD2SS XMM1,XMM2,XMM3/M64	Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2.
VCVTSD2SS	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VCVTSD2SS XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2 under writemask k1.
VCVTSD2USI	R32,XMM/M64{ER}	AVX512F	VCVTSD2USI R32,XMM1/M64{ER}	Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r32.
VCVTSD2USI	R64,XMM/M64{ER}	AVX512F	VCVTSD2USI R64,XMM1/M64{ER}	Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zero-extended into r64.
VCVTSI2SD	XMM,XMM,R/M32	AVX	VCVTSI2SD XMM1,XMM2,R/M32	Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
VCVTSI2SD	XMM,XMM,R/M64	AVX	VCVTSI2SD XMM1,XMM2,R/M64	Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
VCVTSI2SD	XMM,XMM,R/M64{ER}	AVX512F	VCVTSI2SD XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
VCVTSI2SS	XMM,XMM,R/M32	AVX	VCVTSI2SS XMM1,XMM2,R/M32	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
VCVTSI2SS	XMM,XMM,R/M32{ER}	AVX512F	VCVTSI2SS XMM1,XMM2,R/M32{ER}	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
VCVTSI2SS	XMM,XMM,R/M64	AVX	VCVTSI2SS XMM1,XMM2,R/M64	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
VCVTSI2SS	XMM,XMM,R/M64{ER}	AVX512F	VCVTSI2SS XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
VCVTSS2SD	XMM,XMM,XMM/M32	AVX	VCVTSS2SD XMM1,XMM2,XMM3/M32	Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2.
VCVTSS2SD	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512F	VCVTSS2SD XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2 under writemask k1.
VCVTSS2SI	R32,XMM/M32	AVX	VCVTSS2SI R32,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
VCVTSS2SI	R32,XMM/M32{ER}	AVX512F	VCVTSS2SI R32,XMM1/M32{ER}	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
VCVTSS2SI	R64,XMM/M32	AVX	VCVTSS2SI R64,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
VCVTSS2SI	R64,XMM/M32{ER}	AVX512F	VCVTSS2SI R64,XMM1/M32{ER}	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
VCVTSS2USI	R32,XMM/M32{ER}	AVX512F	VCVTSS2USI R32,XMM1/M32{ER}	Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r32.
VCVTSS2USI	R64,XMM/M32{ER}	AVX512F	VCVTSS2USI R64,XMM1/M32{ER}	Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r64.
VCVTTPD2DQ	XMM,XMM/M128	AVX	VCVTTPD2DQ XMM1,XMM2/M128	Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation.
VCVTTPD2DQ	XMM,YMM/M256	AVX	VCVTTPD2DQ XMM1,YMM2/M256	Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation.
VCVTTPD2DQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VCVTTPD2DQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2DQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VCVTTPD2DQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2DQ	YMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512F	VCVTTPD2DQ YMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 using truncation subject to writemask k1.
VCVTTPD2QQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTTPD2QQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values from zmm2/m128/m64bcst to two packed quadword integers in zmm1 using truncation with writemask k1.
VCVTTPD2QQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTTPD2QQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 using truncation with writemask k1.
VCVTTPD2QQ	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512DQ	VCVTTPD2QQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed double-precision floating-point values from zmm2/m512 to eight packed quadword integers in zmm1 using truncation with writemask k1.
VCVTTPD2UDQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VCVTTPD2UDQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2UDQ	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VCVTTPD2UDQ XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 using truncation subject to writemask k1.
VCVTTPD2UDQ	YMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512F	VCVTTPD2UDQ YMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 using truncation subject to writemask k1.
VCVTTPD2UQQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTTPD2UQQ XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed double-precision floating-point values from xmm2/m128/m64bcst to two packed unsigned quadword integers in xmm1 using truncation with writemask k1.
VCVTTPD2UQQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTTPD2UQQ YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed unsigned quadword integers in ymm1 using truncation with writemask k1.
VCVTTPD2UQQ	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512DQ	VCVTTPD2UQQ ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert eight packed double-precision floating-point values from zmm2/mem to eight packed unsigned quadword integers in zmm1 using truncation with writemask k1.
VCVTTPS2DQ	XMM,XMM/M128	AVX	VCVTTPS2DQ XMM1,XMM2/M128	Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation.
VCVTTPS2DQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTTPS2DQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2DQ	YMM,YMM/M256	AVX	VCVTTPS2DQ YMM1,YMM2/M256	Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation.
VCVTTPS2DQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTTPS2DQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2DQ	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512F	VCVTTPS2DQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 using truncation subject to writemask k1.
VCVTTPS2QQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512DQ	VCVTTPS2QQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2QQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512DQ	VCVTTPS2QQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2QQ	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512DQ	VCVTTPS2QQ ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 using truncation subject to writemask k1.
VCVTTPS2UDQ	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTTPS2UDQ XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2UDQ	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTTPS2UDQ YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2UDQ	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512F	VCVTTPS2UDQ ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 using truncation subject to writemask k1.
VCVTTPS2UQQ	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512DQ	VCVTTPS2UQQ XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed unsigned quadword values in xmm1 using truncation subject to writemask k1.
VCVTTPS2UQQ	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512DQ	VCVTTPS2UQQ YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 using truncation subject to writemask k1.
VCVTTPS2UQQ	ZMM{K}{Z},YMM/M256/M32BCST{SAE}	AVX512DQ	VCVTTPS2UQQ ZMM1{K1}{Z},YMM2/M256/M32BCST{SAE}	Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 using truncation subject to writemask k1.
VCVTTSD2SI	R32,XMM/M64	AVX	VCVTTSD2SI R32,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VCVTTSD2SI	R32,XMM/M64{SAE}	AVX512F	VCVTTSD2SI R32,XMM1/M64{SAE}	Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
VCVTTSD2SI	R64,XMM/M64	AVX	VCVTTSD2SI R64,XMM1/M64	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
VCVTTSD2SI	R64,XMM/M64{SAE}	AVX512F	VCVTTSD2SI R64,XMM1/M64{SAE}	Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
VCVTTSD2USI	R32,XMM/M64{SAE}	AVX512F	VCVTTSD2USI R32,XMM1/M64{SAE}	Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r32 using truncation.
VCVTTSD2USI	R64,XMM/M64{SAE}	AVX512F	VCVTTSD2USI R64,XMM1/M64{SAE}	Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zero-extended into r64 using truncation.
VCVTTSS2SI	R32,XMM/M32	AVX	VCVTTSS2SI R32,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VCVTTSS2SI	R32,XMM/M32{SAE}	AVX512F	VCVTTSS2SI R32,XMM1/M32{SAE}	Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
VCVTTSS2SI	R64,XMM/M32	AVX	VCVTTSS2SI R64,XMM1/M32	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
VCVTTSS2SI	R64,XMM/M32{SAE}	AVX512F	VCVTTSS2SI R64,XMM1/M32{SAE}	Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
VCVTTSS2USI	R32,XMM/M32{SAE}	AVX512F	VCVTTSS2USI R32,XMM1/M32{SAE}	Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r32 using truncation.
VCVTTSS2USI	R64,XMM/M32{SAE}	AVX512F	VCVTTSS2USI R64,XMM1/M32{SAE}	Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r64 using truncation.
VCVTUDQ2PD	XMM{K}{Z},XMM/M64/M32BCST	AVX512VL,AVX512F	VCVTUDQ2PD XMM1{K1}{Z},XMM2/M64/M32BCST	Convert two packed unsigned doubleword integers from ymm2/m64/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1.
VCVTUDQ2PD	YMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTUDQ2PD YMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1.
VCVTUDQ2PD	ZMM{K}{Z},YMM/M256/M32BCST	AVX512F	VCVTUDQ2PD ZMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1.
VCVTUDQ2PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VCVTUDQ2PS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed single-precision floating-point values in xmm1 with writemask k1.
VCVTUDQ2PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VCVTUDQ2PS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to packed single-precision floating-point values in zmm1 with writemask k1.
VCVTUDQ2PS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512F	VCVTUDQ2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Convert sixteen packed unsigned doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1 with writemask k1.
VCVTUQQ2PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTUQQ2PD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to two packed double-precision floating-point values in xmm1 with writemask k1.
VCVTUQQ2PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTUQQ2PD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed double-precision floating-point values in ymm1 with writemask k1.
VCVTUQQ2PD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTUQQ2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed double-precision floating-point values in zmm1 with writemask k1.
VCVTUQQ2PS	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512DQ	VCVTUQQ2PS XMM1{K1}{Z},XMM2/M128/M64BCST	Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to packed single-precision floating-point values in zmm1 with writemask k1.
VCVTUQQ2PS	XMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512DQ	VCVTUQQ2PS XMM1{K1}{Z},YMM2/M256/M64BCST	Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed single-precision floating-point values in xmm1 with writemask k1.
VCVTUQQ2PS	YMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512DQ	VCVTUQQ2PS YMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed single-precision floating-point values in zmm1 with writemask k1.
VCVTUSI2SD	XMM,XMM,R/M32	AVX512F	VCVTUSI2SD XMM1,XMM2,R/M32	Convert one unsigned doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
VCVTUSI2SD	XMM,XMM,R/M64{ER}	AVX512F	VCVTUSI2SD XMM1,XMM2,R/M64{ER}	Convert one unsigned quadword integer from r/m64 to one double-precision floating-point value in xmm1.
VCVTUSI2SS	XMM,XMM,R/M32{ER}	AVX512F	VCVTUSI2SS XMM1,XMM2,R/M32{ER}	Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
VCVTUSI2SS	XMM,XMM,R/M64{ER}	AVX512F	VCVTUSI2SS XMM1,XMM2,R/M64{ER}	Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
VDBPSADBW	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VDBPSADBW XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Compute packed SAD word results of unsigned bytes in dword block from xmm2 with unsigned bytes of dword blocks transformed from xmm3/m128 using the shuffle controls in imm8. Results are written to xmm1 under the writemask k1.
VDBPSADBW	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VDBPSADBW YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Compute packed SAD word results of unsigned bytes in dword block from ymm2 with unsigned bytes of dword blocks transformed from ymm3/m256 using the shuffle controls in imm8. Results are written to ymm1 under the writemask k1.
VDBPSADBW	ZMM{K}{Z},ZMM,ZMM/M512,IMM8	AVX512BW	VDBPSADBW ZMM1{K1}{Z},ZMM2,ZMM3/M512,IMM8	Compute packed SAD word results of unsigned bytes in dword block from zmm2 with unsigned bytes of dword blocks transformed from zmm3/m512 using the shuffle controls in imm8. Results are written to zmm1 under the writemask k1.
VDIVPD	XMM,XMM,XMM/M128	AVX	VDIVPD XMM1,XMM2,XMM3/M128	Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem.
VDIVPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VDIVPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/m128/m64bcst and write results to xmm1 subject to writemask k1.
VDIVPD	YMM,YMM,YMM/M256	AVX	VDIVPD YMM1,YMM2,YMM3/M256	Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem.
VDIVPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VDIVPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/m256/m64bcst and write results to ymm1 subject to writemask k1.
VDIVPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VDIVPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Divide packed double-precision floating-point values in zmm2 by packed double-precision FP values in zmm3/m512/m64bcst and write results to zmm1 subject to writemask k1.
VDIVPS	XMM,XMM,XMM/M128	AVX	VDIVPS XMM1,XMM2,XMM3/M128	Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/mem.
VDIVPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VDIVPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/m128/m32bcst and write results to xmm1 subject to writemask k1.
VDIVPS	YMM,YMM,YMM/M256	AVX	VDIVPS YMM1,YMM2,YMM3/M256	Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/mem.
VDIVPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VDIVPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/m256/m32bcst and write results to ymm1 subject to writemask k1.
VDIVPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VDIVPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Divide packed single-precision floating-point values in zmm2 by packed single-precision floating-point values in zmm3/m512/m32bcst and write results to zmm1 subject to writemask k1.
VDIVSD	XMM,XMM,XMM/M64	AVX	VDIVSD XMM1,XMM2,XMM3/M64	Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64.
VDIVSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VDIVSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64.
VDIVSS	XMM,XMM,XMM/M32	AVX	VDIVSS XMM1,XMM2,XMM3/M32	Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32.
VDIVSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VDIVSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32.
VDPPD	XMM,XMM,XMM/M128,IMM8	AVX	VDPPD XMM1,XMM2,XMM3/M128,IMM8	Selectively multiply packed DP floating-point values from xmm2 with packed DP floating-point values from xmm3, add and selectively store the packed DP floating-point values to xmm1.
VDPPS	XMM,XMM,XMM/M128,IMM8	AVX	VDPPS XMM1,XMM2,XMM3/M128,IMM8	Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1.
VDPPS	YMM,YMM,YMM/M256,IMM8	AVX	VDPPS YMM1,YMM2,YMM3/M256,IMM8	Multiply packed single-precision floating-point values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1.
VERR	R/M16		VERR R/M16	Set ZF=1 if segment specified with r/m16 can be read.
VERW	R/M16		VERW R/M16	Set ZF=1 if segment specified with r/m16 can be written.
VEXP2PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512ER	VEXP2PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed double-precision floating-point values from zmm2/m512/m64bcst and stores the floating-point result in zmm1with writemask k1.
VEXP2PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512ER	VEXP2PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed single-precision floating-point values from zmm2/m512/m32bcst and stores the floating-point result in zmm1with writemask k1.
VEXPANDPD	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VEXPANDPD XMM1{K1}{Z},XMM2/M128	Expand packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1.
VEXPANDPD	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VEXPANDPD YMM1{K1}{Z},YMM2/M256	Expand packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1.
VEXPANDPD	ZMM{K}{Z},ZMM/M512	AVX512F	VEXPANDPD ZMM1{K1}{Z},ZMM2/M512	Expand packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VEXPANDPS	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VEXPANDPS XMM1{K1}{Z},XMM2/M128	Expand packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1.
VEXPANDPS	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VEXPANDPS YMM1{K1}{Z},YMM2/M256	Expand packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1.
VEXPANDPS	ZMM{K}{Z},ZMM/M512	AVX512F	VEXPANDPS ZMM1{K1}{Z},ZMM2/M512	Expand packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VEXTRACTF128	XMM/M128,YMM,IMM8	AVX	VEXTRACTF128 XMM1/M128,YMM2,IMM8	Extract 128 bits of packed floating-point values from ymm2 and store results in xmm1/m128.
VEXTRACTF32x4	XMM/M128{K}{Z},YMM,IMM8	AVX512VL,AVX512F	VEXTRACTF32x4 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of packed single-precision floating-point values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF32x4	XMM/M128{K}{Z},ZMM,IMM8	AVX512F	VEXTRACTF32x4 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of packed single-precision floating-point values from zmm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF32x8	YMM/M256{K}{Z},ZMM,IMM8	AVX512DQ	VEXTRACTF32x8 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of packed single-precision floating-point values from zmm2 and store results in ymm1/m256 subject to writemask k1.
VEXTRACTF64x2	XMM/M128{K}{Z},YMM,IMM8	AVX512VL,AVX512DQ	VEXTRACTF64x2 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of packed double-precision floating-point values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF64x2	XMM/M128{K}{Z},ZMM,IMM8	AVX512DQ	VEXTRACTF64x2 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of packed double-precision floating-point values from zmm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTF64x4	YMM/M256{K}{Z},ZMM,IMM8	AVX512F	VEXTRACTF64x4 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of packed double-precision floating-point values from zmm2 and store results in ymm1/m256 subject to writemask k1.
VEXTRACTI128	XMM/M128,YMM,IMM8	AVX2	VEXTRACTI128 XMM1/M128,YMM2,IMM8	Extract 128 bits of integer data from ymm2 and store results in xmm1/m128.
VEXTRACTI32x4	XMM/M128{K}{Z},YMM,IMM8	AVX512VL,AVX512F	VEXTRACTI32x4 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of double-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI32x4	XMM/M128{K}{Z},ZMM,IMM8	AVX512F	VEXTRACTI32x4 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of double-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI32x8	YMM/M256{K}{Z},ZMM,IMM8	AVX512DQ	VEXTRACTI32x8 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of double-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1.
VEXTRACTI64x2	XMM/M128{K}{Z},YMM,IMM8	AVX512VL,AVX512DQ	VEXTRACTI64x2 XMM1/M128{K1}{Z},YMM2,IMM8	Extract 128 bits of quad-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI64x2	XMM/M128{K}{Z},ZMM,IMM8	AVX512DQ	VEXTRACTI64x2 XMM1/M128{K1}{Z},ZMM2,IMM8	Extract 128 bits of quad-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1.
VEXTRACTI64x4	YMM/M256{K}{Z},ZMM,IMM8	AVX512F	VEXTRACTI64x4 YMM1/M256{K1}{Z},ZMM2,IMM8	Extract 256 bits of quad-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1.
VEXTRACTPS	REG/M32,XMM,IMM8	AVX	VEXTRACTPS REG/M32,XMM1,IMM8	Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable.
VFIXUPIMMPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VFIXUPIMMPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Fix up special numbers in float64 vector xmm1, float64 vector xmm2 and int64 vector xmm3/m128/m64bcst and store the result in xmm1, under writemask.
VFIXUPIMMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VFIXUPIMMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Fix up special numbers in float64 vector ymm1, float64 vector ymm2 and int64 vector ymm3/m256/m64bcst and store the result in ymm1, under writemask.
VFIXUPIMMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512F	VFIXUPIMMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Fix up elements of float64 vector in zmm2 using int64 vector table in zmm3/m512/m64bcst, combine with preserved elements from zmm1, and store the result in zmm1.
VFIXUPIMMPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VFIXUPIMMPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Fix up special numbers in float32 vector xmm1, float32 vector xmm2 and int32 vector xmm3/m128/m32bcst and store the result in xmm1, under writemask.
VFIXUPIMMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VFIXUPIMMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Fix up special numbers in float32 vector ymm1, float32 vector ymm2 and int32 vector ymm3/m256/m32bcst and store the result in ymm1, under writemask.
VFIXUPIMMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512F	VFIXUPIMMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Fix up elements of float32 vector in zmm2 using int32 vector table in zmm3/m512/m32bcst, combine with preserved elements from zmm1, and store the result in zmm1.
VFIXUPIMMSD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512F	VFIXUPIMMSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Fix up a float64 number in the low quadword element of xmm2 using scalar int32 table in xmm3/m64 and store the result in xmm1.
VFIXUPIMMSS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512F	VFIXUPIMMSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Fix up a float32 number in the low doubleword element in xmm2 using scalar int32 table in xmm3/m32 and store the result in xmm1.
VFMADD132PD	XMM,XMM,XMM/M128	FMA	VFMADD132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1.
VFMADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADD132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, add to xmm2 and put result in xmm1.
VFMADD132PD	YMM,YMM,YMM/M256	FMA	VFMADD132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1.
VFMADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, add to ymm2 and put result in ymm1.
VFMADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, add to zmm2 and put result in zmm1.
VFMADD132PS	XMM,XMM,XMM/M128	FMA	VFMADD132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1.
VFMADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add to xmm2 and put result in xmm1.
VFMADD132PS	YMM,YMM,YMM/M256	FMA	VFMADD132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1.
VFMADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add to ymm2 and put result in ymm1.
VFMADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add to zmm2 and put result in zmm1.
VFMADD132SD	XMM,XMM,XMM/M64	FMA	VFMADD132SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1.
VFMADD132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMADD132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1.
VFMADD132SS	XMM,XMM,XMM/M32	FMA	VFMADD132SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1.
VFMADD132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMADD132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1.
VFMADD213PD	XMM,XMM,XMM/M128	FMA	VFMADD213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1.
VFMADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m64bcst and put result in xmm1.
VFMADD213PD	YMM,YMM,YMM/M256	FMA	VFMADD213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1.
VFMADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m64bcst and put result in ymm1.
VFMADD213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMADD213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m64bcst and put result in zmm1.
VFMADD213PS	XMM,XMM,XMM/M128	FMA	VFMADD213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1.
VFMADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m32bcst and put result in xmm1.
VFMADD213PS	YMM,YMM,YMM/M256	FMA	VFMADD213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1.
VFMADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m32bcst and put result in ymm1.
VFMADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m32bcst and put result in zmm1.
VFMADD213SD	XMM,XMM,XMM/M64	FMA	VFMADD213SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1.
VFMADD213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMADD213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1.
VFMADD213SS	XMM,XMM,XMM/M32	FMA	VFMADD213SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1.
VFMADD213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMADD213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1.
VFMADD231PD	XMM,XMM,XMM/M128	FMA	VFMADD231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1.
VFMADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, add to xmm1 and put result in xmm1.
VFMADD231PD	YMM,YMM,YMM/M256	FMA	VFMADD231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1.
VFMADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, add to ymm1 and put result in ymm1.
VFMADD231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMADD231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, add to zmm1 and put result in zmm1.
VFMADD231PS	XMM,XMM,XMM/M128	FMA	VFMADD231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1.
VFMADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add to xmm1 and put result in xmm1.
VFMADD231PS	YMM,YMM,YMM/M256	FMA	VFMADD231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1.
VFMADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add to ymm1 and put result in ymm1.
VFMADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add to zmm1 and put result in zmm1.
VFMADD231SD	XMM,XMM,XMM/M64	FMA	VFMADD231SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1.
VFMADD231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMADD231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1.
VFMADD231SS	XMM,XMM,XMM/M32	FMA	VFMADD231SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1.
VFMADD231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMADD231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1.
VFMADDSUB132PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, add/subtract elements in xmm2 and put result in xmm1.
VFMADDSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADDSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, add/subtract elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMADDSUB132PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, add/subtract elements in ymm2 and put result in ymm1.
VFMADDSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADDSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMADDSUB132PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, add/subtract elements in xmm2 and put result in xmm1.
VFMADDSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADDSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add/subtract elements in zmm2 and put result in xmm1 subject to writemask k1.
VFMADDSUB132PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, add/subtract elements in ymm2 and put result in ymm1.
VFMADDSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADDSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMADDSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADDSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1.
VFMADDSUB213PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/mem and put result in xmm1.
VFMADDSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADDSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMADDSUB213PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/mem and put result in ymm1.
VFMADDSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADDSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMADDSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMADDSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1and zmm2, add/subtract elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1.
VFMADDSUB213PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/mem and put result in xmm1.
VFMADDSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADDSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1.
VFMADDSUB213PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/mem and put result in ymm1.
VFMADDSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADDSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1.
VFMADDSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADDSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, add/subtract elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1.
VFMADDSUB231PD	XMM,XMM,XMM/M128	FMA	VFMADDSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, add/subtract elements in xmm1 and put result in xmm1.
VFMADDSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMADDSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMADDSUB231PD	YMM,YMM,YMM/M256	FMA	VFMADDSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, add/subtract elements in ymm1 and put result in ymm1.
VFMADDSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMADDSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMADDSUB231PS	XMM,XMM,XMM/M128	FMA	VFMADDSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, add/subtract elements in xmm1 and put result in xmm1.
VFMADDSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMADDSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMADDSUB231PS	YMM,YMM,YMM/M256	FMA	VFMADDSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, add/subtract elements in ymm1 and put result in ymm1.
VFMADDSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMADDSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMADDSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMADDSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1.
VFMSUB132PD	XMM,XMM,XMM/M128	FMA	VFMSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1.
VFMSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, subtract xmm2 and put result in xmm1 subject to writemask k1.
VFMSUB132PD	YMM,YMM,YMM/M256	FMA	VFMSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1.
VFMSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, subtract ymm2 and put result in ymm1 subject to writemask k1.
VFMSUB132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMSUB132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, subtract zmm2 and put result in zmm1 subject to writemask k1.
VFMSUB132PS	XMM,XMM,XMM/M128	FMA	VFMSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1.
VFMSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract xmm2 and put result in xmm1.
VFMSUB132PS	YMM,YMM,YMM/M256	FMA	VFMSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1.
VFMSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract ymm2 and put result in ymm1.
VFMSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract zmm2 and put result in zmm1.
VFMSUB132SD	XMM,XMM,XMM/M64	FMA	VFMSUB132SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1.
VFMSUB132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMSUB132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1.
VFMSUB132SS	XMM,XMM,XMM/M32	FMA	VFMSUB132SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1.
VFMSUB132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMSUB132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1.
VFMSUB213PD	XMM,XMM,XMM/M128	FMA	VFMSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1.
VFMSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract   xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMSUB213PD	YMM,YMM,YMM/M256	FMA	VFMSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1.
VFMSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1.
VFMSUB213PS	XMM,XMM,XMM/M128	FMA	VFMSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1.
VFMSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract xmm3/m128/m32bcst and put result in xmm1.
VFMSUB213PS	YMM,YMM,YMM/M256	FMA	VFMSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1.
VFMSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m32bcst and put result in ymm1.
VFMSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m32bcst and put result in zmm1.
VFMSUB213SD	XMM,XMM,XMM/M64	FMA	VFMSUB213SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1.
VFMSUB213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMSUB213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1.
VFMSUB213SS	XMM,XMM,XMM/M32	FMA	VFMSUB213SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1.
VFMSUB213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMSUB213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1.
VFMSUB231PD	XMM,XMM,XMM/M128	FMA	VFMSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1.
VFMSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, subtract   xmm1 and put result in xmm1 subject to writemask k1.
VFMSUB231PD	YMM,YMM,YMM/M256	FMA	VFMSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1.S
VFMSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, subtract ymm1 and put result in ymm1 subject to writemask k1.
VFMSUB231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMSUB231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, subtract zmm1 and put result in zmm1 subject to writemask k1.
VFMSUB231PS	XMM,XMM,XMM/M128	FMA	VFMSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1.
VFMSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract xmm1 and put result in xmm1.
VFMSUB231PS	YMM,YMM,YMM/M256	FMA	VFMSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1.
VFMSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract ymm1 and put result in ymm1.
VFMSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract zmm1 and put result in zmm1.
VFMSUB231SD	XMM,XMM,XMM/M64	FMA	VFMSUB231SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1.
VFMSUB231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFMSUB231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1.
VFMSUB231SS	XMM,XMM,XMM/M32	FMA	VFMSUB231SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1.
VFMSUB231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFMSUB231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1.
VFMSUBADD132PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, subtract/add elements in xmm2 and put result in xmm1.
VFMSUBADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUBADD132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMSUBADD132PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, subtract/add elements in ymm2 and put result in ymm1.
VFMSUBADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUBADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMSUBADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFMSUBADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1.
VFMSUBADD132PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, subtract/add elements in xmm2 and put result in xmm1.
VFMSUBADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUBADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1.
VFMSUBADD132PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, subtract/add elements in ymm2 and put result in ymm1.
VFMSUBADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUBADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1.
VFMSUBADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUBADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1.
VFMSUBADD213PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/mem and put result in xmm1.
VFMSUBADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUBADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1.
VFMSUBADD213PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/mem and put result in ymm1.
VFMSUBADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUBADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1.
VFMSUBADD213PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/mem and put result in xmm1.
VFMSUBADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUBADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1.
VFMSUBADD213PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/mem and put result in ymm1.
VFMSUBADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUBADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1.
VFMSUBADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUBADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1.
VFMSUBADD231PD	XMM,XMM,XMM/M128	FMA	VFMSUBADD231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, subtract/add elements in xmm1 and put result in xmm1.
VFMSUBADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFMSUBADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMSUBADD231PD	YMM,YMM,YMM/M256	FMA	VFMSUBADD231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, subtract/add elements in ymm1 and put result in ymm1.
VFMSUBADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFMSUBADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMSUBADD231PS	XMM,XMM,XMM/M128	FMA	VFMSUBADD231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, subtract/add elements in xmm1 and put result in xmm1.
VFMSUBADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFMSUBADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1.
VFMSUBADD231PS	YMM,YMM,YMM/M256	FMA	VFMSUBADD231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, subtract/add elements in ymm1 and put result in ymm1.
VFMSUBADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFMSUBADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1.
VFMSUBADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFMSUBADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1.
VFNMADD132PD	XMM,XMM,XMM/M128	FMA	VFNMADD132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMADD132PD XMM0{K1}{Z},XMM1,XMM2/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PD	YMM,YMM,YMM/M256	FMA	VFNMADD132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMADD132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFNMADD132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm2 and put result in zmm1.
VFNMADD132PS	XMM,XMM,XMM/M128	FMA	VFNMADD132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMADD132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132PS	YMM,YMM,YMM/M256	FMA	VFNMADD132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMADD132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm2 and put result in ymm1.
VFNMADD132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512VL,AVX512F	VFNMADD132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm2 and put result in zmm1.
VFNMADD132SD	XMM,XMM,XMM/M64	FMA	VFNMADD132SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMADD132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132SS	XMM,XMM,XMM/M32	FMA	VFNMADD132SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMADD132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1.
VFNMADD213PD	XMM,XMM,XMM/M128	FMA	VFNMADD213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMADD213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m64bcst and put result in xmm1.
VFNMADD213PD	YMM,YMM,YMM/M256	FMA	VFNMADD213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1.
VFNMADD213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMADD213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m64bcst and put result in ymm1.
VFNMADD213PS	XMM,XMM,XMM/M128	FMA	VFNMADD213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMADD213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m32bcst and put result in xmm1.
VFNMADD213PS	YMM,YMM,YMM/M256	FMA	VFNMADD213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1.
VFNMADD213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMADD213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m32bcst and put result in ymm1.
VFNMADD213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFNMADD213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m32bcst and put result in zmm1.
VFNMADD213SD	XMM,XMM,XMM/M64	FMA	VFNMADD213SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1.
VFNMADD213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMADD213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m64 and put result in xmm1.
VFNMADD213SS	XMM,XMM,XMM/M32	FMA	VFNMADD213SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1.
VFNMADD213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMADD213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1.
VFNMADD231PD	XMM,XMM,XMM/M128	FMA	VFNMADD231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMADD231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PD	YMM,YMM,YMM/M256	FMA	VFNMADD231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMADD231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PS	XMM,XMM,XMM/M128	FMA	VFNMADD231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMADD231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231PS	YMM,YMM,YMM/M256	FMA	VFNMADD231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMADD231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm1 and put result in ymm1.
VFNMADD231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFNMADD231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm1 and put result in zmm1.
VFNMADD231SD	XMM,XMM,XMM/M64	FMA	VFNMADD231SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMADD231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231SS	XMM,XMM,XMM/M32	FMA	VFNMADD231SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMADD231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMADD231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1.
VFNMSUB132PD	XMM,XMM,XMM/M128	FMA	VFNMSUB132PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMSUB132PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PD	YMM,YMM,YMM/M256	FMA	VFNMSUB132PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMSUB132PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFNMSUB132PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm2 and put result in zmm1.
VFNMSUB132PS	XMM,XMM,XMM/M128	FMA	VFNMSUB132PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMSUB132PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132PS	YMM,YMM,YMM/M256	FMA	VFNMSUB132PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMSUB132PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and subtract ymm2 and put result in ymm1.
VFNMSUB132PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFNMSUB132PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and subtract zmm2 and put result in zmm1.
VFNMSUB132SD	XMM,XMM,XMM/M64	FMA	VFNMSUB132SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMSUB132SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132SS	XMM,XMM,XMM/M32	FMA	VFNMSUB132SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB132SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMSUB132SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1.
VFNMSUB213PD	XMM,XMM,XMM/M128	FMA	VFNMSUB213PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMSUB213PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m64bcst and put result in xmm1.
VFNMSUB213PD	YMM,YMM,YMM/M256	FMA	VFNMSUB213PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1.
VFNMSUB213PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMSUB213PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m64bcst and put result in ymm1.
VFNMSUB213PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFNMSUB213PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m64bcst and put result in zmm1.
VFNMSUB213PS	XMM,XMM,XMM/M128	FMA	VFNMSUB213PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMSUB213PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m32bcst and put result in xmm1.
VFNMSUB213PS	YMM,YMM,YMM/M256	FMA	VFNMSUB213PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1.
VFNMSUB213PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMSUB213PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m32bcst and put result in ymm1.
VFNMSUB213PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFNMSUB213PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m32bcst and put result in zmm1.
VFNMSUB213SD	XMM,XMM,XMM/M64	FMA	VFNMSUB213SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1.
VFNMSUB213SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMSUB213SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m64 and put result in xmm1.
VFNMSUB213SS	XMM,XMM,XMM/M32	FMA	VFNMSUB213SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1.
VFNMSUB213SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMSUB213SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1.
VFNMSUB231PD	XMM,XMM,XMM/M128	FMA	VFNMSUB231PD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VFNMSUB231PD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PD	YMM,YMM,YMM/M256	FMA	VFNMSUB231PD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VFNMSUB231PD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VFNMSUB231PD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm1 and put result in zmm1.
VFNMSUB231PS	XMM,XMM,XMM/M128	FMA	VFNMSUB231PS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VFNMSUB231PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result subtract add to xmm1 and put result in xmm1.
VFNMSUB231PS	YMM,YMM,YMM/M256	FMA	VFNMSUB231PS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1.
VFNMSUB231PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VFNMSUB231PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result subtract add to ymm1 and put result in ymm1.
VFNMSUB231PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VFNMSUB231PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result subtract add to zmm1 and put result in zmm1.
VFNMSUB231SD	XMM,XMM,XMM/M64	FMA	VFNMSUB231SD XMM1,XMM2,XMM3/M64	Multiply scalar double-precision floating-point value from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231SD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VFNMSUB231SD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231SS	XMM,XMM,XMM/M32	FMA	VFNMSUB231SS XMM1,XMM2,XMM3/M32	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFNMSUB231SS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VFNMSUB231SS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1.
VFPCLASSPD	K{K},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512DQ	VFPCLASSPD K2{K1},XMM2/M128/M64BCST,IMM8	NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPD	K{K},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512DQ	VFPCLASSPD K2{K1},YMM2/M256/M64BCST,IMM8	NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPD	K{K},ZMM/M512/M64BCST,IMM8	AVX512DQ	VFPCLASSPD K2{K1},ZMM2/M512/M64BCST,IMM8	NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPS	K{K},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512DQ	VFPCLASSPS K2{K1},XMM2/M128/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPS	K{K},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512DQ	VFPCLASSPS K2{K1},YMM2/M256/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSPS	K{K},ZMM/M512/M32BCST,IMM8	AVX512DQ	VFPCLASSPS K2{K1},ZMM2/M512/M32BCST,IMM8	Tests the input for the following categories:  NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative.  The immediate field provides a mask bit for each of these category tests.  The masked test results are OR-ed together to form a mask result.
VFPCLASSSD	K{K},XMM/M64,IMM8	AVX512DQ	VFPCLASSSD K2{K1},XMM2/M64,IMM8	Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result.
VFPCLASSSS	K{K},XMM/M32,IMM8	AVX512DQ	VFPCLASSSS K2{K1},XMM2/M32,IMM8	Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result.
VGATHERDPD	XMM,VM32X,XMM	AVX2	VGATHERDPD XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERDPD	XMM{K},VM32X	AVX512VL,AVX512F	VGATHERDPD XMM1{K1},VM32X	Using signed dword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask.
VGATHERDPD	YMM,VM32X,YMM	AVX2	VGATHERDPD YMM1,VM32X,YMM2	Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1.
VGATHERDPD	YMM{K},VM32X	AVX512VL,AVX512F	VGATHERDPD YMM1{K1},VM32X	Using signed dword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask.
VGATHERDPD	ZMM{K},VM32Y	AVX512F	VGATHERDPD ZMM1{K1},VM32Y	Using signed dword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask.
VGATHERDPS	XMM,VM32X,XMM	AVX2	VGATHERDPS XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERDPS	XMM{K},VM32X	AVX512VL,AVX512F	VGATHERDPS XMM1{K1},VM32X	Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGATHERDPS	YMM,VM32Y,YMM	AVX2	VGATHERDPS YMM1,VM32Y,YMM2	Using dword indices specified in vm32y, gather single-preci-sion FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VGATHERDPS	YMM{K},VM32Y	AVX512VL,AVX512F	VGATHERDPS YMM1{K1},VM32Y	Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGATHERDPS	ZMM{K},VM32Z	AVX512F	VGATHERDPS ZMM1{K1},VM32Z	Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGATHERPF0DPD	VM32Y{K}	AVX512PF	VGATHERPF0DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T0 hint.
VGATHERPF0DPS	VM32Z{K}	AVX512PF	VGATHERPF0DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T0 hint.
VGATHERPF0QPD	VM64Z{K}	AVX512PF	VGATHERPF0QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T0 hint.
VGATHERPF0QPS	VM64Z{K}	AVX512PF	VGATHERPF0QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T0 hint.
VGATHERPF1DPD	VM32Y{K}	AVX512PF	VGATHERPF1DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T1 hint.
VGATHERPF1DPS	VM32Z{K}	AVX512PF	VGATHERPF1DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T1 hint.
VGATHERPF1QPD	VM64Z{K}	AVX512PF	VGATHERPF1QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T1 hint.
VGATHERPF1QPS	VM64Z{K}	AVX512PF	VGATHERPF1QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T1 hint.
VGATHERQPD	XMM,VM64X,XMM	AVX2	VGATHERQPD XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERQPD	XMM{K},VM64X	AVX512VL,AVX512F	VGATHERQPD XMM1{K1},VM64X	Using signed qword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask.
VGATHERQPD	YMM,VM64Y,YMM	AVX2	VGATHERQPD YMM1,VM64Y,YMM2	Using qword indices specified in vm64y, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1.
VGATHERQPD	YMM{K},VM64Y	AVX512VL,AVX512F	VGATHERQPD YMM1{K1},VM64Y	Using signed qword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask.
VGATHERQPD	ZMM{K},VM64Z	AVX512F	VGATHERQPD ZMM1{K1},VM64Z	Using signed qword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask.
VGATHERQPS	XMM,VM64X,XMM	AVX2	VGATHERQPS XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERQPS	XMM,VM64Y,XMM	AVX2	VGATHERQPS XMM1,VM64Y,XMM2	Using qword indices specified in vm64y, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VGATHERQPS	XMM{K},VM64X	AVX512VL,AVX512F	VGATHERQPS XMM1{K1},VM64X	Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGATHERQPS	XMM{K},VM64Y	AVX512VL,AVX512F	VGATHERQPS XMM1{K1},VM64Y	Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGATHERQPS	YMM{K},VM64Z	AVX512F	VGATHERQPS YMM1{K1},VM64Z	Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask.
VGETEXPPD	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VGETEXPPD XMM1{K1}{Z},XMM2/M128/M64BCST	Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPD	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VGETEXPPD YMM1{K1}{Z},YMM2/M256/M64BCST	Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512F	VGETEXPPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination under writemask k1.
VGETEXPPS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VGETEXPPS XMM1{K1}{Z},XMM2/M128/M32BCST	Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VGETEXPPS YMM1{K1}{Z},YMM2/M256/M32BCST	Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512F	VGETEXPPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register.
VGETEXPSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512F	VGETEXPSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Convert the biased exponent (bits 62:52) of the low double-precision floating-point value in xmm3/m64 to a DP FP value representing unbiased integer exponent. Stores the result to the low 64-bit of xmm1 under the writemask k1 and merge with the other elements of xmm2.
VGETEXPSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512F	VGETEXPSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Convert the biased exponent (bits 30:23) of the low single-precision floating-point value in xmm3/m32 to a SP FP value representing unbiased integer exponent. Stores the result to xmm1 under the writemask k1 and merge with the other elements of xmm2.
VGETMANTPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VGETMANTPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Get Normalized Mantissa from float64 vector xmm2/m128/m64bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VGETMANTPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Get Normalized Mantissa from float64 vector ymm2/m256/m64bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512F	VGETMANTPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Get Normalized Mantissa from float64 vector zmm2/m512/m64bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VGETMANTPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Get normalized mantissa from float32 vector xmm2/m128/m32bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VGETMANTPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Get normalized mantissa from float32 vector ymm2/m256/m32bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512F	VGETMANTPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Get normalized mantissa from float32 vector zmm2/m512/m32bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask.
VGETMANTSD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512F	VGETMANTSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Extract the normalized mantissa of the low float64 element in xmm3/m64 using imm8 for sign control and mantissa interval normalization. Store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2.
VGETMANTSS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512F	VGETMANTSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Extract the normalized mantissa from the low float32 element of xmm3/m32 using imm8 for sign control and mantissa interval normalization, store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2.
VHADDPD	XMM,XMM,XMM/M128	AVX	VHADDPD XMM1,XMM2,XMM3/M128	Horizontal add packed double-precision floating-point values from xmm2 and xmm3/mem.
VHADDPD	YMM,YMM,YMM/M256	AVX	VHADDPD YMM1,YMM2,YMM3/M256	Horizontal add packed double-precision floating-point values from ymm2 and ymm3/mem.
VHADDPS	XMM,XMM,XMM/M128	AVX	VHADDPS XMM1,XMM2,XMM3/M128	Horizontal add packed single-precision floating-point values from xmm2 and xmm3/mem.
VHADDPS	YMM,YMM,YMM/M256	AVX	VHADDPS YMM1,YMM2,YMM3/M256	Horizontal add packed single-precision floating-point values from ymm2 and ymm3/mem.
VHSUBPD	XMM,XMM,XMM/M128	AVX	VHSUBPD XMM1,XMM2,XMM3/M128	Horizontal subtract packed double-precision floating-point values from xmm2 and xmm3/mem.
VHSUBPD	YMM,YMM,YMM/M256	AVX	VHSUBPD YMM1,YMM2,YMM3/M256	Horizontal subtract packed double-precision floating-point values from ymm2 and ymm3/mem.
VHSUBPS	XMM,XMM,XMM/M128	AVX	VHSUBPS XMM1,XMM2,XMM3/M128	Horizontal subtract packed single-precision floating-point values from xmm2 and xmm3/mem.
VHSUBPS	YMM,YMM,YMM/M256	AVX	VHSUBPS YMM1,YMM2,YMM3/M256	Horizontal subtract packed single-precision floating-point values from ymm2 and ymm3/mem.
VINSERTF128	YMM,YMM,XMM/M128,IMM8	AVX	VINSERTF128 YMM1,YMM2,XMM3/M128,IMM8	Insert 128 bits of packed floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1.
VINSERTF32x4	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512VL,AVX512F	VINSERTF32x4 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTF32x4	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512F	VINSERTF32x4 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTF32x8	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512DQ	VINSERTF32x8 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed single-precision floating-point values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTF64x2	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512VL,AVX512DQ	VINSERTF64x2 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed double-precision floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTF64x2	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512DQ	VINSERTF64x2 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed double-precision floating-point values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTF64x4	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512F	VINSERTF64x4 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed double-precision floating-point values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTI128	YMM,YMM,XMM/M128,IMM8	AVX2	VINSERTI128 YMM1,YMM2,XMM3/M128,IMM8	Insert 128 bits of integer data from xmm3/m128 and the remaining values from ymm2 into ymm1.
VINSERTI32x4	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512VL,AVX512F	VINSERTI32x4 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTI32x4	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512F	VINSERTI32x4 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTI32x8	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512DQ	VINSERTI32x8 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed doubleword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTI64x2	YMM{K}{Z},YMM,XMM/M128,IMM8	AVX512VL,AVX512DQ	VINSERTI64x2 YMM1{K1}{Z},YMM2,XMM3/M128,IMM8	Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1.
VINSERTI64x2	ZMM{K}{Z},ZMM,XMM/M128,IMM8	AVX512DQ	VINSERTI64x2 ZMM1{K1}{Z},ZMM2,XMM3/M128,IMM8	Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTI64x4	ZMM{K}{Z},ZMM,YMM/M256,IMM8	AVX512F	VINSERTI64x4 ZMM1{K1}{Z},ZMM2,YMM3/M256,IMM8	Insert 256 bits of packed quadword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1.
VINSERTPS	XMM,XMM,XMM/M32,IMM8	AVX	VINSERTPS XMM1,XMM2,XMM3/M32,IMM8	Insert a single-precision floating-point value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8.
VLDDQU	XMM,M128	AVX	VLDDQU XMM1,M128	Load unaligned packed integer values from mem to xmm1.
VLDDQU	YMM,M256	AVX	VLDDQU YMM1,M256	Load unaligned packed integer values from mem to ymm1.
VLDMXCSR	M32	AVX	VLDMXCSR M32	Load MXCSR register from m32.
VMASKMOVDQU	XMM,XMM	AVX	VMASKMOVDQU XMM1,XMM2	Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI.
VMASKMOVPD	M128,XMM,XMM	AVX	VMASKMOVPD M128,XMM1,XMM2	Conditionally store packed double-precision values from xmm2 using mask in xmm1.
VMASKMOVPD	M256,YMM,YMM	AVX	VMASKMOVPD M256,YMM1,YMM2	Conditionally store packed double-precision values from ymm2 using mask in ymm1.
VMASKMOVPD	XMM,XMM,M128	AVX	VMASKMOVPD XMM1,XMM2,M128	Conditionally load packed double-precision values from m128 using mask in xmm2 and store in xmm1.
VMASKMOVPD	YMM,YMM,M256	AVX	VMASKMOVPD YMM1,YMM2,M256	Conditionally load packed double-precision values from m256 using mask in ymm2 and store in ymm1.
VMASKMOVPS	M128,XMM,XMM	AVX	VMASKMOVPS M128,XMM1,XMM2	Conditionally store packed single-precision values from xmm2 using mask in xmm1.
VMASKMOVPS	M256,YMM,YMM	AVX	VMASKMOVPS M256,YMM1,YMM2	Conditionally store packed single-precision values from ymm2 using mask in ymm1.
VMASKMOVPS	XMM,XMM,M128	AVX	VMASKMOVPS XMM1,XMM2,M128	Conditionally load packed single-precision values from m128 using mask in xmm2 and store in xmm1.
VMASKMOVPS	YMM,YMM,M256	AVX	VMASKMOVPS YMM1,YMM2,M256	Conditionally load packed single-precision values from m256 using mask in ymm2 and store in ymm1.
VMAXPD	XMM,XMM,XMM/M128	AVX	VMAXPD XMM1,XMM2,XMM3/M128	Return the maximum double-precision floating-point values between xmm2 and xmm3/m128.
VMAXPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VMAXPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the maximum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1.
VMAXPD	YMM,YMM,YMM/M256	AVX	VMAXPD YMM1,YMM2,YMM3/M256	Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256.
VMAXPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VMAXPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1.
VMAXPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE}	AVX512F	VMAXPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE}	Return the maximum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1.
VMAXPS	XMM,XMM,XMM/M128	AVX	VMAXPS XMM1,XMM2,XMM3/M128	Return the maximum single-precision floating-point values between xmm2 and xmm3/mem.
VMAXPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VMAXPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the maximum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1.
VMAXPS	YMM,YMM,YMM/M256	AVX	VMAXPS YMM1,YMM2,YMM3/M256	Return the maximum single-precision floating-point values between ymm2 and ymm3/mem.
VMAXPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VMAXPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the maximum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1.
VMAXPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE}	AVX512F	VMAXPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE}	Return the maximum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1.
VMAXSD	XMM,XMM,XMM/M64	AVX	VMAXSD XMM1,XMM2,XMM3/M64	Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2.
VMAXSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512F	VMAXSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2.
VMAXSS	XMM,XMM,XMM/M32	AVX	VMAXSS XMM1,XMM2,XMM3/M32	Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2.
VMAXSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512F	VMAXSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2.
VMINPD	XMM,XMM,XMM/M128	AVX	VMINPD XMM1,XMM2,XMM3/M128	Return the minimum double-precision floating-point values between xmm2 and xmm3/mem.
VMINPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VMINPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the minimum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1.
VMINPD	YMM,YMM,YMM/M256	AVX	VMINPD YMM1,YMM2,YMM3/M256	Return the minimum packed double-precision floating-point values between ymm2 and ymm3/mem.
VMINPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VMINPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the minimum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1.
VMINPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE}	AVX512F	VMINPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE}	Return the minimum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1.
VMINPS	XMM,XMM,XMM/M128	AVX	VMINPS XMM1,XMM2,XMM3/M128	Return the minimum single-precision floating-point values between xmm2 and xmm3/mem.
VMINPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VMINPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the minimum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1.
VMINPS	YMM,YMM,YMM/M256	AVX	VMINPS YMM1,YMM2,YMM3/M256	Return the minimum single double-precision floating-point values between ymm2 and ymm3/mem.
VMINPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VMINPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the minimum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1.
VMINPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE}	AVX512F	VMINPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE}	Return the minimum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1.
VMINSD	XMM,XMM,XMM/M64	AVX	VMINSD XMM1,XMM2,XMM3/M64	Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2.
VMINSD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512F	VMINSD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2.
VMINSS	XMM,XMM,XMM/M32	AVX	VMINSS XMM1,XMM2,XMM3/M32	Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2.
VMINSS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512F	VMINSS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2.
VMOVAPD	XMM,XMM/M128	AVX	VMOVAPD XMM1,XMM2/M128	Move aligned packed double-precision floating-point values from xmm2/mem to xmm1.
VMOVAPD	XMM/M128,XMM	AVX	VMOVAPD XMM2/M128,XMM1	Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem.
VMOVAPD	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVAPD XMM2/M128{K1}{Z},XMM1	Move aligned packed double-precision floating-point values from xmm1 to xmm2/m128 using writemask k1.
VMOVAPD	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVAPD XMM1{K1}{Z},XMM2/M128	Move aligned packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1.
VMOVAPD	YMM,YMM/M256	AVX	VMOVAPD YMM1,YMM2/M256	Move aligned packed double-precision floating-point values from ymm2/mem to ymm1.
VMOVAPD	YMM/M256,YMM	AVX	VMOVAPD YMM2/M256,YMM1	Move aligned packed double-precision floating-point values from ymm1 to ymm2/mem.
VMOVAPD	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVAPD YMM2/M256{K1}{Z},YMM1	Move aligned packed double-precision floating-point values from ymm1 to ymm2/m256 using writemask k1.
VMOVAPD	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVAPD YMM1{K1}{Z},YMM2/M256	Move aligned packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1.
VMOVAPD	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVAPD ZMM2/M512{K1}{Z},ZMM1	Move aligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1.
VMOVAPD	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVAPD ZMM1{K1}{Z},ZMM2/M512	Move aligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VMOVAPS	XMM,XMM/M128	AVX	VMOVAPS XMM1,XMM2/M128	Move aligned packed single-precision floating-point values from xmm2/mem to xmm1.
VMOVAPS	XMM/M128,XMM	AVX	VMOVAPS XMM2/M128,XMM1	Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem.
VMOVAPS	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVAPS XMM2/M128{K1}{Z},XMM1	Move aligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1.
VMOVAPS	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVAPS XMM1{K1}{Z},XMM2/M128	Move aligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1.
VMOVAPS	YMM,YMM/M256	AVX	VMOVAPS YMM1,YMM2/M256	Move aligned packed single-precision floating-point values from ymm2/mem to ymm1.
VMOVAPS	YMM/M256,YMM	AVX	VMOVAPS YMM2/M256,YMM1	Move aligned packed single-precision floating-point values from ymm1 to ymm2/mem.
VMOVAPS	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVAPS YMM2/M256{K1}{Z},YMM1	Move aligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1.
VMOVAPS	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVAPS YMM1{K1}{Z},YMM2/M256	Move aligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1.
VMOVAPS	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVAPS ZMM2/M512{K1}{Z},ZMM1	Move aligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1.
VMOVAPS	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVAPS ZMM1{K1}{Z},ZMM2/M512	Move aligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VMOVD	R32/M32,XMM	AVX	VMOVD R32/M32,XMM1	Move doubleword from xmm1 register to r/m32.
VMOVD	XMM,R32/M32	AVX	VMOVD XMM1,R32/M32	Move doubleword from r/m32 to xmm1.
VMOVDDUP	XMM,XMM/M64	AVX	VMOVDDUP XMM1,XMM2/M64	Move double-precision floating-point value from xmm2/m64 and duplicate into xmm1.
VMOVDDUP	XMM{K}{Z},XMM/M64	AVX512VL,AVX512F	VMOVDDUP XMM1{K1}{Z},XMM2/M64	Move double-precision floating-point value from xmm2/m64 and duplicate each element into xmm1 subject to writemask k1.
VMOVDDUP	YMM,YMM/M256	AVX	VMOVDDUP YMM1,YMM2/M256	Move even index double-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
VMOVDDUP	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVDDUP YMM1{K1}{Z},YMM2/M256	Move even index double-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 subject to writemask k1.
VMOVDDUP	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVDDUP ZMM1{K1}{Z},ZMM2/M512	Move even index double-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 subject to writemask k1.
VMOVDQA	XMM,XMM/M128	AVX	VMOVDQA XMM1,XMM2/M128	Move aligned packed integer values from xmm2/mem to xmm1.
VMOVDQA	XMM/M128,XMM	AVX	VMOVDQA XMM2/M128,XMM1	Move aligned packed integer values from xmm1 to xmm2/mem.
VMOVDQA	YMM,YMM/M256	AVX	VMOVDQA YMM1,YMM2/M256	Move aligned packed integer values from ymm2/mem to ymm1.
VMOVDQA	YMM/M256,YMM	AVX	VMOVDQA YMM2/M256,YMM1	Move aligned packed integer values from ymm1 to ymm2/mem.
VMOVDQA32	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVDQA32 XMM2/M128{K1}{Z},XMM1	Move aligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQA32	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVDQA32 XMM1{K1}{Z},XMM2/M128	Move aligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQA32	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVDQA32 YMM2/M256{K1}{Z},YMM1	Move aligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQA32	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVDQA32 YMM1{K1}{Z},YMM2/M256	Move aligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQA32	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVDQA32 ZMM2/M512{K1}{Z},ZMM1	Move aligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1.
VMOVDQA32	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVDQA32 ZMM1{K1}{Z},ZMM2/M512	Move aligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQA64	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVDQA64 XMM2/M128{K1}{Z},XMM1	Move aligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQA64	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVDQA64 XMM1{K1}{Z},XMM2/M128	Move aligned quadword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQA64	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVDQA64 YMM2/M256{K1}{Z},YMM1	Move aligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQA64	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVDQA64 YMM1{K1}{Z},YMM2/M256	Move aligned quadword integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQA64	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVDQA64 ZMM2/M512{K1}{Z},ZMM1	Move aligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1.
VMOVDQA64	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVDQA64 ZMM1{K1}{Z},ZMM2/M512	Move aligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU	XMM,XMM/M128	AVX	VMOVDQU XMM1,XMM2/M128	Move unaligned packed integer values from xmm2/m128 to xmm1.
VMOVDQU	XMM/M128,XMM	AVX	VMOVDQU XMM2/M128,XMM1	Move unaligned packed integer values from xmm1 to xmm2/m128.
VMOVDQU	YMM,YMM/M256	AVX	VMOVDQU YMM1,YMM2/M256	Move unaligned packed integer values from ymm2/m256 to ymm1.
VMOVDQU	YMM/M256,YMM	AVX	VMOVDQU YMM2/M256,YMM1	Move unaligned packed integer values from ymm1 to ymm2/m256.
VMOVDQU16	XMM/M128{K}{Z},XMM	AVX512VL,AVX512BW	VMOVDQU16 XMM2/M128{K1}{Z},XMM1	Move unaligned packed word integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU16	XMM{K}{Z},XMM/M128	AVX512VL,AVX512BW	VMOVDQU16 XMM1{K1}{Z},XMM2/M128	Move unaligned packed word integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU16	YMM/M256{K}{Z},YMM	AVX512VL,AVX512BW	VMOVDQU16 YMM2/M256{K1}{Z},YMM1	Move unaligned packed word integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU16	YMM{K}{Z},YMM/M256	AVX512VL,AVX512BW	VMOVDQU16 YMM1{K1}{Z},YMM2/M256	Move unaligned packed word integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU16	ZMM/M512{K}{Z},ZMM	AVX512BW	VMOVDQU16 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed word integer values from zmm1 to zmm2/m512 using writemask k1.
VMOVDQU16	ZMM{K}{Z},ZMM/M512	AVX512BW	VMOVDQU16 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed word integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVDQU32	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVDQU32 XMM1{K1}{Z},XMM2/MM128	Move unaligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU8	XMM/M128{K}{Z},XMM	AVX512VL,AVX512BW	VMOVDQU8 XMM2/M128{K1}{Z},XMM1	Move unaligned packed byte integer values from xmm1 to xmm2/m128 using writemask k1.
VMOVDQU8	XMM{K}{Z},XMM/M128	AVX512VL,AVX512BW	VMOVDQU8 XMM1{K1}{Z},XMM2/M128	Move unaligned packed byte integer values from xmm2/m128 to xmm1 using writemask k1.
VMOVDQU8	YMM/M256{K}{Z},YMM	AVX512VL,AVX512BW	VMOVDQU8 YMM2/M256{K1}{Z},YMM1	Move unaligned packed byte integer values from ymm1 to ymm2/m256 using writemask k1.
VMOVDQU8	YMM{K}{Z},YMM/M256	AVX512VL,AVX512BW	VMOVDQU8 YMM1{K1}{Z},YMM2/M256	Move unaligned packed byte integer values from ymm2/m256 to ymm1 using writemask k1.
VMOVDQU8	ZMM/M512{K}{Z},ZMM	AVX512BW	VMOVDQU8 ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed byte integer values from zmm1 to zmm2/m512 using writemask k1.
VMOVDQU8	ZMM{K}{Z},ZMM/M512	AVX512BW	VMOVDQU8 ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed byte integer values from zmm2/m512 to zmm1 using writemask k1.
VMOVHLPS	XMM,XMM,XMM	AVX	VMOVHLPS XMM1,XMM2,XMM3	Merge two packed single-precision floating-point values from high quadword of xmm3 and low quadword of xmm2.
VMOVHPD	M64,XMM	AVX	VMOVHPD M64,XMM1	Move double-precision floating-point value from high quadword of xmm1 to m64.
VMOVHPD	XMM,XMM,M64	AVX	VMOVHPD XMM2,XMM1,M64	Merge double-precision floating-point value from m64 and the low quadword of xmm1.
VMOVHPS	M64,XMM	AVX	VMOVHPS M64,XMM1	Move two packed single-precision floating-point values from high quadword of xmm1 to m64.
VMOVHPS	XMM,XMM,M64	AVX	VMOVHPS XMM2,XMM1,M64	Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1.
VMOVLHPS	XMM,XMM,XMM	AVX	VMOVLHPS XMM1,XMM2,XMM3	Merge two packed single-precision floating-point values from low quadword of xmm3 and low quadword of xmm2.
VMOVLPD	M64,XMM	AVX	VMOVLPD M64,XMM1	Move double-precision floating-point value from low quadword of xmm1 to m64.
VMOVLPD	XMM,XMM,M64	AVX	VMOVLPD XMM2,XMM1,M64	Merge double-precision floating-point value from m64 and the high quadword of xmm1.
VMOVLPS	M64,XMM	AVX	VMOVLPS M64,XMM1	Move two packed single-precision floating-point values from low quadword of xmm1 to m64.
VMOVLPS	XMM,XMM,M64	AVX	VMOVLPS XMM2,XMM1,M64	Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1.
VMOVMSKPD	REG,XMM	AVX	VMOVMSKPD REG,XMM2	Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVMSKPD	REG,YMM	AVX	VMOVMSKPD REG,YMM2	Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVMSKPS	REG,XMM	AVX	VMOVMSKPS REG,XMM2	Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVMSKPS	REG,YMM	AVX	VMOVMSKPS REG,YMM2	Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed.
VMOVNTDQ	M128,XMM	AVX	VMOVNTDQ M128,XMM1	Move packed integer values in xmm1 to m128 using non-temporal hint.
VMOVNTDQ	M256,YMM	AVX	VMOVNTDQ M256,YMM1	Move packed integer values in ymm1 to m256 using non-temporal hint.
VMOVNTDQ	M512,ZMM	AVX512F	VMOVNTDQ M512,ZMM1	Move packed integer values in zmm1 to m512 using non-temporal hint.
VMOVNTDQA	XMM,M128	AVX	VMOVNTDQA XMM1,M128	Move double quadword from m128 to xmm using non-temporal hint if WC memory type.
VMOVNTDQA	YMM,M256	AVX2	VMOVNTDQA YMM1,M256	Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type.
VMOVNTDQA	ZMM,M512	AVX512F	VMOVNTDQA ZMM1,M512	Move 512-bit data from m512 to zmm using non-temporal hint if WC memory type.
VMOVNTPD	M128,XMM	AVX	VMOVNTPD M128,XMM1	Move packed double-precision values in xmm1 to m128 using non-temporal hint.
VMOVNTPD	M256,YMM	AVX	VMOVNTPD M256,YMM1	Move packed double-precision values in ymm1 to m256 using non-temporal hint.
VMOVNTPD	M512,ZMM	AVX512F	VMOVNTPD M512,ZMM1	Move packed double-precision values in zmm1 to m512 using non-temporal hint.
VMOVNTPS	M128,XMM	AVX	VMOVNTPS M128,XMM1	Move packed single-precision values xmm1 to mem using non-temporal hint.
VMOVNTPS	M256,YMM	AVX	VMOVNTPS M256,YMM1	Move packed single-precision values ymm1 to mem using non-temporal hint.
VMOVNTPS	M512,ZMM	AVX512F	VMOVNTPS M512,ZMM1	Move packed single-precision values in zmm1 to m512 using non-temporal hint.
VMOVQ	R64/M64,XMM	AVX,AVX512F,AVX512F	VMOVQ R64/M64,XMM1	Move quadword from xmm1 register to r/m64. Move doubleword from r/m32 to xmm1. Move quadword from r/m64 to xmm1.
VMOVQ	XMM,R64/M64	AVX	VMOVQ XMM1,R64/M64	Move quadword from r/m64 to xmm1.
VMOVQ	XMM,XMM/M64	AVX	VMOVQ XMM1,XMM2/M64	Move quadword from xmm2 to xmm1.
VMOVQ	XMM/M64,XMM	AVX	VMOVQ XMM1/M64,XMM2	Move quadword from xmm2 register to xmm1/m64.
VMOVSD	M64,XMM	AVX	VMOVSD M64,XMM1	Store scalar double-precision floating-point value from xmm1 register to m64.
VMOVSD	M64{K},XMM	AVX512F	VMOVSD M64{K1},XMM1	Store scalar double-precision floating-point value from xmm1 register to m64 under writemask k1.
VMOVSD	XMM,M64	AVX	VMOVSD XMM1,M64	Load scalar double-precision floating-point value from m64 to xmm1 register.
VMOVSD	XMM,XMM,XMM	AVX	VMOVSD XMM1,XMM2,XMM3	Merge scalar double-precision floating-point value from xmm2 and xmm3 to xmm1 register.
VMOVSD	XMM{K}{Z},M64	AVX512F	VMOVSD XMM1{K1}{Z},M64	Load scalar double-precision floating-point value from m64 to xmm1 register under writemask k1.
VMOVSD	XMM{K}{Z},XMM,XMM	AVX512F	VMOVSD XMM1{K1}{Z},XMM2,XMM3	Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1 under writemask k1.
VMOVSHDUP	XMM,XMM/M128	AVX	VMOVSHDUP XMM1,XMM2/M128	Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VMOVSHDUP	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVSHDUP XMM1{K1}{Z},XMM2/M128	Move odd index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask.
VMOVSHDUP	YMM,YMM/M256	AVX	VMOVSHDUP YMM1,YMM2/M256	Move odd index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
VMOVSHDUP	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVSHDUP YMM1{K1}{Z},YMM2/M256	Move odd index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask.
VMOVSHDUP	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVSHDUP ZMM1{K1}{Z},ZMM2/M512	Move odd index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask.
VMOVSLDUP	XMM,XMM/M128	AVX	VMOVSLDUP XMM1,XMM2/M128	Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1.
VMOVSLDUP	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVSLDUP XMM1{K1}{Z},XMM2/M128	Move even index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask.
VMOVSLDUP	YMM,YMM/M256	AVX	VMOVSLDUP YMM1,YMM2/M256	Move even index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1.
VMOVSLDUP	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVSLDUP YMM1{K1}{Z},YMM2/M256	Move even index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask.
VMOVSLDUP	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVSLDUP ZMM1{K1}{Z},ZMM2/M512	Move even index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask.
VMOVSS	M32,XMM	AVX	VMOVSS M32,XMM1	Move scalar single-precision floating-point value from xmm1 register to m32.
VMOVSS	M32{K},XMM	AVX512F	VMOVSS M32{K1},XMM1	Move scalar single-precision floating-point values from xmm1 to m32 under writemask k1.
VMOVSS	XMM,M32	AVX	VMOVSS XMM1,M32	Load scalar single-precision floating-point value from m32 to xmm1 register.
VMOVSS	XMM,XMM,XMM	AVX	VMOVSS XMM1,XMM2,XMM3	Merge scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register
VMOVSS	XMM{K}{Z},M32	AVX512F	VMOVSS XMM1{K1}{Z},M32	Move scalar single-precision floating-point values from m32 to xmm1 under writemask k1.
VMOVSS	XMM{K}{Z},XMM,XMM	AVX512F	VMOVSS XMM1{K1}{Z},XMM2,XMM3	Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1.
VMOVUPD	XMM,XMM/M128	AVX	VMOVUPD XMM1,XMM2/M128	Move unaligned packed double-precision floating-point from xmm2/mem to xmm1.
VMOVUPD	XMM/M128,XMM	AVX	VMOVUPD XMM2/M128,XMM1	Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem.
VMOVUPD	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVUPD XMM2/M128{K1}{Z},XMM1	Move unaligned packed double-precision floating-point from xmm1 to xmm2/m128 using writemask k1.
VMOVUPD	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVUPD XMM1{K1}{Z},XMM2/M128	Move unaligned packed double-precision floating-point from xmm2/m128 to xmm1 using writemask k1.
VMOVUPD	YMM,YMM/M256	AVX	VMOVUPD YMM1,YMM2/M256	Move unaligned packed double-precision floating-point from ymm2/mem to ymm1.
VMOVUPD	YMM/M256,YMM	AVX	VMOVUPD YMM2/M256,YMM1	Move unaligned packed double-precision floating-point from ymm1 to ymm2/mem.
VMOVUPD	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVUPD YMM2/M256{K1}{Z},YMM1	Move unaligned packed double-precision floating-point from ymm1 to ymm2/m256 using writemask k1.
VMOVUPD	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVUPD YMM1{K1}{Z},YMM2/M256	Move unaligned packed double-precision floating-point from ymm2/m256 to ymm1 using writemask k1.
VMOVUPD	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVUPD ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1.
VMOVUPD	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVUPD ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VMOVUPS	XMM,XMM/M128	AVX	VMOVUPS XMM1,XMM2/M128	Move unaligned packed single-precision floating-point from xmm2/mem to xmm1.
VMOVUPS	XMM/M128,XMM	AVX	VMOVUPS XMM2/M128,XMM1	Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem.
VMOVUPS	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VMOVUPS XMM2/M128{K1}{Z},XMM1	Move unaligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1.
VMOVUPS	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VMOVUPS XMM1{K1}{Z},XMM2/M128	Move unaligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1.
VMOVUPS	YMM,YMM/M256	AVX	VMOVUPS YMM1,YMM2/M256	Move unaligned packed single-precision floating-point from ymm2/mem to ymm1.
VMOVUPS	YMM/M256,YMM	AVX	VMOVUPS YMM2/M256,YMM1	Move unaligned packed single-precision floating-point from ymm1 to ymm2/mem.
VMOVUPS	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VMOVUPS YMM2/M256{K1}{Z},YMM1	Move unaligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1.
VMOVUPS	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VMOVUPS YMM1{K1}{Z},YMM2/M256	Move unaligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1.
VMOVUPS	ZMM/M512{K}{Z},ZMM	AVX512F	VMOVUPS ZMM2/M512{K1}{Z},ZMM1	Move unaligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1.
VMOVUPS	ZMM{K}{Z},ZMM/M512	AVX512F	VMOVUPS ZMM1{K1}{Z},ZMM2/M512	Move unaligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1.
VMPSADBW	XMM,XMM,XMM/M128,IMM8	AVX	VMPSADBW XMM1,XMM2,XMM3/M128,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8.
VMPSADBW	YMM,YMM,YMM/M256,IMM8	AVX2	VMPSADBW YMM1,YMM2,YMM3/M256,IMM8	Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by imm8.
VMULPD	XMM,XMM,XMM/M128	AVX	VMULPD XMM1,XMM2,XMM3/M128	Multiply packed double-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1.
VMULPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VMULPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1.
VMULPD	YMM,YMM,YMM/M256	AVX	VMULPD YMM1,YMM2,YMM3/M256	Multiply packed double-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1.
VMULPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VMULPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1.
VMULPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VMULPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Multiply packed double-precision floating-point values in zmm3/m512/m64bcst with zmm2 and store result in zmm1.
VMULPS	XMM,XMM,XMM/M128	AVX	VMULPS XMM1,XMM2,XMM3/M128	Multiply packed single-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1.
VMULPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VMULPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1.
VMULPS	YMM,YMM,YMM/M256	AVX	VMULPS YMM1,YMM2,YMM3/M256	Multiply packed single-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1.
VMULPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VMULPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1.
VMULPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VMULPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Multiply packed single-precision floating-point values in zmm3/m512/m32bcst with zmm2 and store result in zmm1.
VMULSD	XMM,XMM,XMM/M64	AVX	VMULSD XMM1,XMM2,XMM3/M64	Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2.
VMULSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VMULSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2.
VMULSS	XMM,XMM,XMM/M32	AVX	VMULSS XMM1,XMM2,XMM3/M32	Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2.
VMULSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VMULSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2.
VORPD	XMM,XMM,XMM/M128	AVX	VORPD XMM1,XMM2,XMM3/M128	Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/mem
VORPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512DQ	VORPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VORPD	YMM,YMM,YMM/M256	AVX	VORPD YMM1,YMM2,YMM3/M256	Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/mem
VORPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512DQ	VORPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VORPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512DQ	VORPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical OR of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
VORPS	XMM,XMM,XMM/M128	AVX	VORPS XMM1,XMM2,XMM3/M128	Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/mem
VORPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512DQ	VORPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VORPS	YMM,YMM,YMM/M256	AVX	VORPS YMM1,YMM2,YMM3/M256	Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/mem
VORPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512DQ	VORPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VORPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512DQ	VORPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical OR of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
VPABSB	XMM,XMM/M128	AVX	VPABSB XMM1,XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSB	XMM{K}{Z},XMM/M128	AVX512VL,AVX512BW	VPABSB XMM1{K1}{Z},XMM2/M128	Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1.
VPABSB	YMM,YMM/M256	AVX2	VPABSB YMM1,YMM2/M256	Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1.
VPABSB	YMM{K}{Z},YMM/M256	AVX512VL,AVX512BW	VPABSB YMM1{K1}{Z},YMM2/M256	Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1. zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1.
VPABSD	XMM,XMM/M128	AVX	VPABSD XMM1,XMM2/M128	Compute the absolute value of 32- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSD	YMM,YMM/M256	AVX2	VPABSD YMM1,YMM2/M256	Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1.
VPABSW	XMM,XMM/M128	AVX	VPABSW XMM1,XMM2/M128	Compute the absolute value of 16- bit integers in xmm2/m128 and store UNSIGNED result in xmm1.
VPABSW	XMM{K}{Z},XMM/M128	AVX512VL,AVX512BW	VPABSW XMM1{K1}{Z},XMM2/M128	Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1.
VPABSW	YMM,YMM/M256	AVX2	VPABSW YMM1,YMM2/M256	Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1.
VPACKSSDW	XMM,XMM,XMM/M128	AVX	VPACKSSDW XMM1,XMM2,XMM3/M128	Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation.
VPACKSSDW	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512BW	VPACKSSDW XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Converts packed signed doubleword integers from xmm2 and from xmm3/m128/m32bcst into packed signed word integers in xmm1 using signed saturation under writemask k1.
VPACKSSDW	YMM,YMM,YMM/M256	AVX2	VPACKSSDW YMM1,YMM2,YMM3/M256	Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation.
VPACKSSWB	XMM,XMM,XMM/M128	AVX	VPACKSSWB XMM1,XMM2,XMM3/M128	Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation.
VPACKSSWB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPACKSSWB XMM1{K1}{Z},XMM2,XMM3/M128	Converts packed signed word integers from xmm2 and from xmm3/m128 into packed signed byte integers in xmm1 using signed saturation under writemask k1.
VPACKSSWB	YMM,YMM,YMM/M256	AVX2	VPACKSSWB YMM1,YMM2,YMM3/M256	Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation.
VPACKSSWB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPACKSSWB YMM1{K1}{Z},YMM2,YMM3/M256	Converts packed signed word integers from ymm2 and from ymm3/m256 into packed signed byte integers in ymm1 using signed saturation under writemask k1.
VPACKSSWB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPACKSSWB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Converts packed signed word integers from zmm2 and from zmm3/m512 into packed signed byte integers in zmm1 using signed saturation under writemask k1.
VPACKUSDW	XMM,XMM,XMM/M128	AVX	VPACKUSDW XMM1,XMM2,XMM3/M128	Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation.
VPACKUSDW	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512BW	VPACKUSDW XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Convert packed signed doubleword integers from xmm2 and packed signed doubleword integers from xmm3/m128/m32bcst into packed unsigned word integers in xmm1 using unsigned saturation under writemask k1.
VPACKUSDW	YMM,YMM,YMM/M256	AVX2	VPACKUSDW YMM1,YMM2,YMM3/M256	Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m256 into 16 packed unsigned word integers in ymm1 using unsigned saturation.
VPACKUSDW	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512BW	VPACKUSDW YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Convert packed signed doubleword integers from ymm2 and packed signed doubleword integers from ymm3/m256/m32bcst into packed unsigned word integers in ymm1 using unsigned saturation under writemask k1.
VPACKUSDW	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512BW	VPACKUSDW ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Convert packed signed doubleword integers from zmm2 and packed signed doubleword integers from zmm3/m512/m32bcst into packed unsigned word integers in zmm1 using unsigned saturation under writemask k1.
VPACKUSWB	XMM,XMM,XMM/M128	AVX	VPACKUSWB XMM1,XMM2,XMM3/M128	Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation.
VPACKUSWB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPACKUSWB XMM1{K1}{Z},XMM2,XMM3/M128	Converts signed word integers from xmm2 and signed word integers from xmm3/m128 into unsigned byte integers in xmm1 using unsigned saturation under writemask k1.
VPACKUSWB	YMM,YMM,YMM/M256	AVX2	VPACKUSWB YMM1,YMM2,YMM3/M256	Converts 16 signed word integers from ymm2 and 16signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation.
VPACKUSWB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPACKUSWB YMM1{K1}{Z},YMM2,YMM3/M256	Converts signed word integers from ymm2 and signed word integers from ymm3/m256 into unsigned byte integers in ymm1 using unsigned saturation under writemask k1.
VPACKUSWB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPACKUSWB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Converts signed word integers from zmm2 and signed word integers from zmm3/m512 into unsigned byte integers in zmm1 using unsigned saturation under writemask k1.
VPADDB	XMM,XMM,XMM/M128	AVX	VPADDB XMM1,XMM2,XMM3/M128	Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1.
VPADDB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1.
VPADDB	YMM,YMM,YMM/M256	AVX2	VPADDB YMM1,YMM2,YMM3/M256	Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1.
VPADDB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1.
VPADDD	XMM,XMM,XMM/M128	AVX	VPADDD XMM1,XMM2,XMM3/M128	Add packed doubleword integers from xmm2, xmm3/m128 and store in xmm1.
VPADDD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPADDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Add packed doubleword integers from xmm2, and xmm3/m128/m32bcst and store in xmm1 using writemask k1.
VPADDD	YMM,YMM,YMM/M256	AVX2	VPADDD YMM1,YMM2,YMM3/M256	Add packed doubleword integers from ymm2, ymm3/m256 and store in ymm1.
VPADDQ	XMM,XMM,XMM/M128	AVX	VPADDQ XMM1,XMM2,XMM3/M128	Add packed quadword integers from xmm2, xmm3/m128 and store in xmm1.
VPADDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPADDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Add packed quadword integers from xmm2, and xmm3/m128/m64bcst and store in xmm1 using writemask k1.
VPADDQ	YMM,YMM,YMM/M256	AVX2	VPADDQ YMM1,YMM2,YMM3/M256	Add packed quadword integers from ymm2, ymm3/m256 and store in ymm1.
VPADDSB	XMM,XMM,XMM/M128	AVX	VPADDSB XMM1,XMM2,XMM3/M128	Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results.
VPADDSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDSB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed signed byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDSB	YMM,YMM,YMM/M256	AVX2	VPADDSB YMM1,YMM2,YMM3/M256	Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDSB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPADDSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed signed byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
VPADDSW	XMM,XMM,XMM/M128	AVX	VPADDSW XMM1,XMM2,XMM3/M128	Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results.
VPADDSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDSW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed signed word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDSW	YMM,YMM,YMM/M256	AVX2	VPADDSW YMM1,YMM2,YMM3/M256	Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDSW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPADDSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed signed word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
VPADDUSB	XMM,XMM,XMM/M128	AVX	VPADDUSB XMM1,XMM2,XMM3/M128	Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results.
VPADDUSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDUSB XMM1{K1}{Z},XMM2,XMM3/M128	Add packed unsigned byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDUSB	YMM,YMM,YMM/M256	AVX2	VPADDUSB YMM1,YMM2,YMM3/M256	Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDUSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDUSB YMM1{K1}{Z},YMM2,YMM3/M256	Add packed unsigned byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDUSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPADDUSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Add packed unsigned byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1.
VPADDUSW	XMM,XMM,XMM/M128	AVX	VPADDUSW XMM1,XMM2,XMM3/M128	Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results.
VPADDUSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDUSW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed unsigned word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1.
VPADDUSW	YMM,YMM,YMM/M256	AVX2	VPADDUSW YMM1,YMM2,YMM3/M256	Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1.
VPADDUSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDUSW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed unsigned word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1.
VPADDW	XMM,XMM,XMM/M128	AVX	VPADDW XMM1,XMM2,XMM3/M128	Add packed word integers from xmm2, xmm3/m128 and store in xmm1.
VPADDW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPADDW XMM1{K1}{Z},XMM2,XMM3/M128	Add packed word integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1.
VPADDW	YMM,YMM,YMM/M256	AVX2	VPADDW YMM1,YMM2,YMM3/M256	Add packed word integers from ymm2, ymm3/m256 and store in ymm1.
VPADDW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPADDW YMM1{K1}{Z},YMM2,YMM3/M256	Add packed word integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1.
VPALIGNR	XMM,XMM,XMM/M128,IMM8	AVX	VPALIGNR XMM1,XMM2,XMM3/M128,IMM8	Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
VPALIGNR	XMM{K}{Z},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPALIGNR XMM1{K1}{Z},XMM2,XMM3/M128,IMM8	Concatenate xmm2 and xmm3/m128 into a 32-byte intermediate result, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1.
VPALIGNR	YMM,YMM,YMM/M256,IMM8	AVX2	VPALIGNR YMM1,YMM2,YMM3/M256,IMM8	Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1.
VPALIGNR	YMM{K}{Z},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPALIGNR YMM1{K1}{Z},YMM2,YMM3/M256,IMM8	Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1. zmm3/m512 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and four 16-byte results are stored in zmm1.
VPAND	XMM,XMM,XMM/M128	AVX	VPAND XMM1,XMM2,XMM3/M128	Bitwise AND of xmm3/m128 and xmm.
VPAND	YMM,YMM,YMM/M256	AVX2	VPAND YMM1,YMM2,YMM3/M256	Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1.
VPANDD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPANDD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1.
VPANDD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPANDD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1.
VPANDD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPANDD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1.
VPANDN	XMM,XMM,XMM/M128	AVX	VPANDN XMM1,XMM2,XMM3/M128	Bitwise AND NOT of xmm3/m128 and xmm2.
VPANDN	YMM,YMM,YMM/M256	AVX2	VPANDN YMM1,YMM2,YMM3/M256	Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1.
VPANDND	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPANDND XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise AND NOT of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1.
VPANDND	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPANDND YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise AND NOT of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1.
VPANDND	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPANDND ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise AND NOT of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1.
VPANDNQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPANDNQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise AND NOT of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1.
VPANDNQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPANDNQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise AND NOT of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1.
VPANDNQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPANDNQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise AND NOT of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1.
VPANDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPANDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1.
VPANDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPANDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1.
VPANDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPANDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1.
VPAVGB	XMM,XMM,XMM/M128	AVX	VPAVGB XMM1,XMM2,XMM3/M128	Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding.
VPAVGB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPAVGB XMM1{K1}{Z},XMM2,XMM3/M128	Average packed unsigned byte integers from xmm2, and xmm3/m128 with rounding and store to xmm1 under writemask k1.
VPAVGB	YMM,YMM,YMM/M256	AVX2	VPAVGB YMM1,YMM2,YMM3/M256	Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1.
VPAVGB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPAVGB YMM1{K1}{Z},YMM2,YMM3/M256	Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1 under writemask k1. zmm2, and zmm3/m512 with rounding and store to zmm1 under writemask k1.
VPAVGW	XMM,XMM,XMM/M128	AVX	VPAVGW XMM1,XMM2,XMM3/M128	Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding.
VPAVGW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPAVGW XMM1{K1}{Z},XMM2,XMM3/M128	Average packed unsigned word integers from xmm2, xmm3/m128 with rounding to xmm1 under writemask k1.
VPAVGW	YMM,YMM,YMM/M256	AVX2	VPAVGW YMM1,YMM2,YMM3/M256	Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1.
VPAVGW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPAVGW YMM1{K1}{Z},YMM2,YMM3/M256	Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1 under writemask k1. zmm2, zmm3/m512 with rounding to zmm1 under writemask k1.
VPBLENDD	XMM,XMM,XMM/M128,IMM8	AVX2	VPBLENDD XMM1,XMM2,XMM3/M128,IMM8	Select dwords from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VPBLENDD	YMM,YMM,YMM/M256,IMM8	AVX2	VPBLENDD YMM1,YMM2,YMM3/M256,IMM8	Select dwords from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
VPBLENDMB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPBLENDMB XMM1{K1}{Z},XMM2,XMM3/M128	Blend byte integer vector xmm2 and byte vector xmm3/m128 and store the result in xmm1, under control mask.
VPBLENDMB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPBLENDMB YMM1{K1}{Z},YMM2,YMM3/M256	Blend byte integer vector ymm2 and byte vector ymm3/m256 and store the result in ymm1, under control mask.
VPBLENDMB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPBLENDMB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Blend byte integer vector zmm2 and byte vector zmm3/m512 and store the result in zmm1, under control mask.
VPBLENDMD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPBLENDMD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Blend doubleword integer vector xmm2 and doubleword vector xmm3/m128/m32bcst and store the result in xmm1, under control mask.
VPBLENDMD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPBLENDMD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Blend doubleword integer vector ymm2 and doubleword vector ymm3/m256/m32bcst and store the result in ymm1, under control mask.
VPBLENDMD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPBLENDMD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Blend doubleword integer vector zmm2 and doubleword vector zmm3/m512/m32bcst and store the result in zmm1, under control mask.
VPBLENDMQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPBLENDMQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Blend quadword integer vector xmm2 and quadword vector xmm3/m128/m64bcst and store the result in xmm1, under control mask.
VPBLENDMQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPBLENDMQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Blend quadword integer vector ymm2 and quadword vector ymm3/m256/m64bcst and store the result in ymm1, under control mask.
VPBLENDMQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPBLENDMQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Blend quadword integer vector zmm2 and quadword vector zmm3/m512/m64bcst and store the result in zmm1, under control mask.
VPBLENDMW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPBLENDMW XMM1{K1}{Z},XMM2,XMM3/M128	Blend word integer vector xmm2 and word vector xmm3/m128 and store the result in xmm1, under control mask.
VPBLENDMW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPBLENDMW YMM1{K1}{Z},YMM2,YMM3/M256	Blend word integer vector ymm2 and word vector ymm3/m256 and store the result in ymm1, under control mask.
VPBLENDMW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPBLENDMW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Blend word integer vector zmm2 and word vector zmm3/m512 and store the result in zmm1, under control mask.
VPBLENDVB	XMM,XMM,XMM/M128,XMM	AVX	VPBLENDVB XMM1,XMM2,XMM3/M128,XMM4	Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1.
VPBLENDVB	YMM,YMM,YMM/M256,YMM	AVX2	VPBLENDVB YMM1,YMM2,YMM3/M256,YMM4	Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1.
VPBLENDW	XMM,XMM,XMM/M128,IMM8	AVX	VPBLENDW XMM1,XMM2,XMM3/M128,IMM8	Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1.
VPBLENDW	YMM,YMM,YMM/M256,IMM8	AVX2	VPBLENDW YMM1,YMM2,YMM3/M256,IMM8	Select words from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1.
VPBROADCASTB	XMM,XMM/M8	AVX2	VPBROADCASTB XMM1,XMM2/M8	Broadcast a byte integer in the source operand to sixteen locations in xmm1.
VPBROADCASTB	XMM{K}{Z},REG	AVX512VL,AVX512BW	VPBROADCASTB XMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 128-bit destination subject to writemask k1.
VPBROADCASTB	XMM{K}{Z},XMM/M8	AVX512VL,AVX512BW	VPBROADCASTB XMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTB	YMM,XMM/M8	AVX2	VPBROADCASTB YMM1,XMM2/M8	Broadcast a byte integer in the source operand to thirty-two locations in ymm1.
VPBROADCASTB	YMM{K}{Z},REG	AVX512VL,AVX512BW	VPBROADCASTB YMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 256-bit destination subject to writemask k1.
VPBROADCASTB	YMM{K}{Z},XMM/M8	AVX512VL,AVX512BW	VPBROADCASTB YMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTB	ZMM{K}{Z},REG	AVX512BW	VPBROADCASTB ZMM1{K1}{Z},REG	Broadcast an 8-bit value from a GPR to all bytes in the 512-bit destination subject to writemask k1.
VPBROADCASTB	ZMM{K}{Z},XMM/M8	AVX512BW	VPBROADCASTB ZMM1{K1}{Z},XMM2/M8	Broadcast a byte integer in the source operand to 64 locations in zmm1 subject to writemask k1.
VPBROADCASTD	XMM,XMM/M32	AVX2	VPBROADCASTD XMM1,XMM2/M32	Broadcast a dword integer in the source operand to four locations in xmm1.
VPBROADCASTD	XMM{K}{Z},R32	AVX512VL,AVX512F	VPBROADCASTD XMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 128-bit destination subject to writemask k1.
VPBROADCASTD	XMM{K}{Z},XMM/M32	AVX512VL,AVX512F	VPBROADCASTD XMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTD	YMM,XMM/M32	AVX2	VPBROADCASTD YMM1,XMM2/M32	Broadcast a dword integer in the source operand to eight locations in ymm1.
VPBROADCASTD	YMM{K}{Z},R32	AVX512VL,AVX512F	VPBROADCASTD YMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 256-bit destination subject to writemask k1.
VPBROADCASTD	YMM{K}{Z},XMM/M32	AVX512VL,AVX512F	VPBROADCASTD YMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTD	ZMM{K}{Z},R32	AVX512F	VPBROADCASTD ZMM1{K1}{Z},R32	Broadcast a 32-bit value from a GPR to all double-words in the 512-bit destination subject to writemask k1.
VPBROADCASTD	ZMM{K}{Z},XMM/M32	AVX512F	VPBROADCASTD ZMM1{K1}{Z},XMM2/M32	Broadcast a dword integer in the source operand to locations in zmm1 subject to writemask k1.
VPBROADCASTMB2Q	XMM,K	AVX512VL,AVX512CD	VPBROADCASTMB2Q XMM1,K1	Broadcast low byte value in k1 to two locations in xmm1.
VPBROADCASTMB2Q	YMM,K	AVX512VL,AVX512CD	VPBROADCASTMB2Q YMM1,K1	Broadcast low byte value in k1 to four locations in ymm1.
VPBROADCASTMB2Q	ZMM,K	AVX512CD	VPBROADCASTMB2Q ZMM1,K1	Broadcast low byte value in k1 to eight locations in zmm1.
VPBROADCASTMW2D	XMM,K	AVX512VL,AVX512CD	VPBROADCASTMW2D XMM1,K1	Broadcast low word value in k1 to four locations in xmm1.
VPBROADCASTMW2D	YMM,K	AVX512VL,AVX512CD	VPBROADCASTMW2D YMM1,K1	Broadcast low word value in k1 to eight locations in ymm1.
VPBROADCASTMW2D	ZMM,K	AVX512CD	VPBROADCASTMW2D ZMM1,K1	Broadcast low word value in k1 to sixteen locations in zmm1.
VPBROADCASTQ	XMM,XMM/M64	AVX2	VPBROADCASTQ XMM1,XMM2/M64	Broadcast a qword element in source operand to two locations in xmm1.
VPBROADCASTQ	XMM{K}{Z},R64	AVX512VL,AVX512F	VPBROADCASTQ XMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 128-bit destination subject to writemask k1.
VPBROADCASTQ	YMM{K}{Z},R64	AVX512VL,AVX512F	VPBROADCASTQ YMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 256-bit destination subject to writemask k1.
VPBROADCASTQ	ZMM{K}{Z},R64	AVX512F	VPBROADCASTQ ZMM1{K1}{Z},R64	Broadcast a 64-bit value from a GPR to all quad-words in the 512-bit destination subject to writemask k1.
VPBROADCASTW	XMM,XMM/M16	AVX2	VPBROADCASTW XMM1,XMM2/M16	Broadcast a word integer in the source operand to eight locations in xmm1.
VPBROADCASTW	XMM{K}{Z},REG	AVX512VL,AVX512BW	VPBROADCASTW XMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 128-bit destination subject to writemask k1.
VPBROADCASTW	XMM{K}{Z},XMM/M16	AVX512VL,AVX512BW	VPBROADCASTW XMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to locations in xmm1 subject to writemask k1.
VPBROADCASTW	YMM,XMM/M16	AVX2	VPBROADCASTW YMM1,XMM2/M16	Broadcast a word integer in the source operand to sixteen locations in ymm1.
VPBROADCASTW	YMM{K}{Z},REG	AVX512VL,AVX512BW	VPBROADCASTW YMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 256-bit destination subject to writemask k1.
VPBROADCASTW	YMM{K}{Z},XMM/M16	AVX512VL,AVX512BW	VPBROADCASTW YMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to locations in ymm1 subject to writemask k1.
VPBROADCASTW	ZMM{K}{Z},REG	AVX512BW	VPBROADCASTW ZMM1{K1}{Z},REG	Broadcast a 16-bit value from a GPR to all words in the 512-bit destination subject to writemask k1.
VPBROADCASTW	ZMM{K}{Z},XMM/M16	AVX512BW	VPBROADCASTW ZMM1{K1}{Z},XMM2/M16	Broadcast a word integer in the source operand to 32 locations in zmm1 subject to writemask k1.
VPCLMULQDQ	XMM,XMM,XMM/M128,IMM8	PCLMULQDQ,AVX	VPCLMULQDQ XMM1,XMM2,XMM3/M128,IMM8	Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The imme-diate is used to determine which quadwords of xmm2 and xmm3/m128 should be used.
VPCMPB	K{K},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPCMPB K1{K2},XMM2,XMM3/M128,IMM8	Compare packed signed byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPB	K{K},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPCMPB K1{K2},YMM2,YMM3/M256,IMM8	Compare packed signed byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPB	K{K},ZMM,ZMM/M512,IMM8	AVX512BW	VPCMPB K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed signed byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPD	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPCMPD K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed signed doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPD	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPCMPD K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed signed doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPD	K{K},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VPCMPD K1{K2},ZMM2,ZMM3/M512/M32BCST,IMM8	Compare packed signed doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2.
VPCMPEQB	K{K},XMM,XMM	AVX512VL,AVX512BW	VPCMPEQB K1{K2},XMM2,XMM3	Compare packed bytes in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQB	XMM,XMM,XMM/M128	AVX	VPCMPEQB XMM1,XMM2,XMM3/M128	Compare packed bytes in xmm3/m128 and xmm2 for equality.
VPCMPEQB	YMM,YMM,YMM/M256	AVX2	VPCMPEQB YMM1,YMM2,YMM3/M256	Compare packed bytes in ymm3/m256 and ymm2 for equality.
VPCMPEQD	K{K},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPCMPEQD K1{K2},XMM2,XMM3/M128/M32BCST	Compare Equal between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQD	K{K},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPCMPEQD K1{K2},YMM2,YMM3/M256/M32BCST	Compare Equal between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQD	K{K},ZMM,ZMM/M512/M32BCST	AVX512F	VPCMPEQD K1{K2},ZMM2,ZMM3/M512/M32BCST	Compare Equal between int32 vectors in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask k2, 
VPCMPEQD	XMM,XMM,XMM/M128	AVX	VPCMPEQD XMM1,XMM2,XMM3/M128	Compare packed doublewords in xmm3/m128 and xmm2 for equality.
VPCMPEQD	YMM,YMM,YMM/M256	AVX2	VPCMPEQD YMM1,YMM2,YMM3/M256	Compare packed doublewords in ymm3/m256 and ymm2 for equality.
VPCMPEQQ	K{K},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPCMPEQQ K1{K2},XMM2,XMM3/M128/M64BCST	Compare Equal between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQQ	K{K},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPCMPEQQ K1{K2},YMM2,YMM3/M256/M64BCST	Compare Equal between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512F	VPCMPEQQ K1{K2},ZMM2,ZMM3/M512/M64BCST	Compare Equal between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPEQQ	XMM,XMM,XMM/M128	AVX	VPCMPEQQ XMM1,XMM2,XMM3/M128	Compare packed quadwords in xmm3/m128 and xmm2 for equality.
VPCMPEQQ	YMM,YMM,YMM/M256	AVX2	VPCMPEQQ YMM1,YMM2,YMM3/M256	Compare packed quadwords in ymm3/m256 and ymm2 for equality.
VPCMPEQW	XMM,XMM,XMM/M128	AVX	VPCMPEQW XMM1,XMM2,XMM3/M128	Compare packed words in xmm3/m128 and xmm2 for equality.
VPCMPEQW	YMM,YMM,YMM/M256	AVX2	VPCMPEQW YMM1,YMM2,YMM3/M256	Compare packed words in ymm3/m256 and ymm2 for equality.
VPCMPESTRI	XMM,XMM/M128,IMM8	AVX	VPCMPESTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX.
VPCMPESTRM	XMM,XMM/M128,IMM8	AVX	VPCMPESTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0.
VPCMPGTB	K{K},XMM,XMM/M128	AVX512VL,AVX512BW	VPCMPGTB K1{K2},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTB	K{K},YMM,YMM/M256	AVX512VL,AVX512BW	VPCMPGTB K1{K2},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTB	XMM,XMM,XMM/M128	AVX	VPCMPGTB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTB	YMM,YMM,YMM/M256	AVX2	VPCMPGTB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than.
VPCMPGTD	K{K},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPCMPGTD K1{K2},XMM2,XMM3/M128/M32BCST	Compare Greater between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTD	K{K},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPCMPGTD K1{K2},YMM2,YMM3/M256/M32BCST	Compare Greater between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTD	K{K},ZMM,ZMM/M512/M32BCST	AVX512F	VPCMPGTD K1{K2},ZMM2,ZMM3/M512/M32BCST	Compare Greater between int32 elements in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask. k2.
VPCMPGTD	XMM,XMM,XMM/M128	AVX	VPCMPGTD XMM1,XMM2,XMM3/M128	Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTD	YMM,YMM,YMM/M256	AVX2	VPCMPGTD YMM1,YMM2,YMM3/M256	Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than.
VPCMPGTQ	K{K},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPCMPGTQ K1{K2},XMM2,XMM3/M128/M64BCST	Compare Greater between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTQ	K{K},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPCMPGTQ K1{K2},YMM2,YMM3/M256/M64BCST	Compare Greater between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512F	VPCMPGTQ K1{K2},ZMM2,ZMM3/M512/M64BCST	Compare Greater between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask.
VPCMPGTQ	XMM,XMM,XMM/M128	AVX	VPCMPGTQ XMM1,XMM2,XMM3/M128	Compare packed signed qwords in xmm2 and xmm3/m128 for greater than.
VPCMPGTQ	YMM,YMM,YMM/M256	AVX2	VPCMPGTQ YMM1,YMM2,YMM3/M256	Compare packed signed qwords in ymm2 and ymm3/m256 for greater than.
VPCMPGTW	XMM,XMM,XMM/M128	AVX	VPCMPGTW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 for greater than.
VPCMPGTW	YMM,YMM,YMM/M256	AVX2	VPCMPGTW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 for greater than.
VPCMPISTRI	XMM,XMM/M128,IMM8	AVX	VPCMPISTRI XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX.
VPCMPISTRM	XMM,XMM/M128,IMM8	AVX	VPCMPISTRM XMM1,XMM2/M128,IMM8	Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0.
VPCMPQ	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPCMPQ K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed signed quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPQ	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPCMPQ K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed signed quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPQ	K{K},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VPCMPQ K1{K2},ZMM2,ZMM3/M512/M64BCST,IMM8	Compare packed signed quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUB	K{K},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPCMPUB K1{K2},XMM2,XMM3/M128,IMM8	Compare packed unsigned byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUB	K{K},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPCMPUB K1{K2},YMM2,YMM3/M256,IMM8	Compare packed unsigned byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUB	K{K},ZMM,ZMM/M512,IMM8	AVX512BW	VPCMPUB K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed unsigned byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUD	K{K},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPCMPUD K1{K2},XMM2,XMM3/M128/M32BCST,IMM8	Compare packed unsigned doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUD	K{K},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPCMPUD K1{K2},YMM2,YMM3/M256/M32BCST,IMM8	Compare packed unsigned doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUD	K{K},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VPCMPUD K1{K2},ZMM2,ZMM3/M512/M32BCST,IMM8	Compare packed unsigned doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2.
VPCMPUQ	K{K},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPCMPUQ K1{K2},XMM2,XMM3/M128/M64BCST,IMM8	Compare packed unsigned quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUQ	K{K},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPCMPUQ K1{K2},YMM2,YMM3/M256/M64BCST,IMM8	Compare packed unsigned quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUQ	K{K},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VPCMPUQ K1{K2},ZMM2,ZMM3/M512/M64BCST,IMM8	Compare packed unsigned quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUW	K{K},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPCMPUW K1{K2},XMM2,XMM3/M128,IMM8	Compare packed unsigned word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUW	K{K},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPCMPUW K1{K2},YMM2,YMM3/M256,IMM8	Compare packed unsigned word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPUW	K{K},ZMM,ZMM/M512,IMM8	AVX512BW	VPCMPUW K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed unsigned word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPW	K{K},XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPCMPW K1{K2},XMM2,XMM3/M128,IMM8	Compare packed signed word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPW	K{K},YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPCMPW K1{K2},YMM2,YMM3/M256,IMM8	Compare packed signed word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCMPW	K{K},ZMM,ZMM/M512,IMM8	AVX512BW	VPCMPW K1{K2},ZMM2,ZMM3/M512,IMM8	Compare packed signed word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
VPCOMPRESSD	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VPCOMPRESSD XMM1/M128{K1}{Z},XMM2	Compress packed doubleword integer values from xmm2 to xmm1/m128 using controlmask k1.
VPCOMPRESSD	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VPCOMPRESSD YMM1/M256{K1}{Z},YMM2	Compress packed doubleword integer values from ymm2 to ymm1/m256 using controlmask k1.
VPCOMPRESSD	ZMM/M512{K}{Z},ZMM	AVX512F	VPCOMPRESSD ZMM1/M512{K1}{Z},ZMM2	Compress packed doubleword integer values from zmm2 to zmm1/m512 using controlmask k1.
VPCOMPRESSQ	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VPCOMPRESSQ XMM1/M128{K1}{Z},XMM2	Compress packed quadword integer values from xmm2 to xmm1/m128 using controlmask k1.
VPCOMPRESSQ	YMM/M256{K}{Z},YMM	AVX512VL,AVX512F	VPCOMPRESSQ YMM1/M256{K1}{Z},YMM2	Compress packed quadword integer values from ymm2 to ymm1/m256 using controlmask k1.
VPCOMPRESSQ	ZMM/M512{K}{Z},ZMM	AVX512F	VPCOMPRESSQ ZMM1/M512{K1}{Z},ZMM2	Compress packed quadword integer values from zmm2 to zmm1/m512 using controlmask k1.
VPCONFLICTD	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512CD	VPCONFLICTD XMM1{K1}{Z},XMM2/M128/M32BCST	Detect duplicate double-word values in xmm2/m128/m32bcst using writemask k1.
VPCONFLICTD	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512CD	VPCONFLICTD YMM1{K1}{Z},YMM2/M256/M32BCST	Detect duplicate double-word values in ymm2/m256/m32bcst using writemask k1.
VPCONFLICTD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512CD	VPCONFLICTD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Detect duplicate double-word values in zmm2/m512/m32bcst using writemask k1.
VPCONFLICTQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512CD	VPCONFLICTQ XMM1{K1}{Z},XMM2/M128/M64BCST	Detect duplicate quad-word values in xmm2/m128/m64bcst using writemask k1.
VPCONFLICTQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512CD	VPCONFLICTQ YMM1{K1}{Z},YMM2/M256/M64BCST	Detect duplicate quad-word values in ymm2/m256/m64bcst using writemask k1.
VPCONFLICTQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512CD	VPCONFLICTQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Detect duplicate quad-word values in zmm2/m512/m64bcst using writemask k1.
VPERM2F128	YMM,YMM,YMM/M256,IMM8	AVX	VPERM2F128 YMM1,YMM2,YMM3/M256,IMM8	Permute 128-bit floating-point fields in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
VPERM2I128	YMM,YMM,YMM/M256,IMM8	AVX2	VPERM2I128 YMM1,YMM2,YMM3/M256,IMM8	Permute 128-bit integer data in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1.
VPERMD	YMM,YMM,YMM/M256	AVX2	VPERMD YMM1,YMM2,YMM3/M256	Permute doublewords in ymm3/m256 using indices in ymm2 and store the result in ymm1.
VPERMD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPERMD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute doublewords in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPERMD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute doublewords in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 using writemask k1.
VPERMI2D	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPERMI2D XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute double-words from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2D	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPERMI2D YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute double-words from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2D	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPERMI2D ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute double-words from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
VPERMI2PS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPERMI2PS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute single-precision FP values from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2PS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPERMI2PS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute single-precision FP values from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2PS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPERMI2PS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute single-precision FP values from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
VPERMI2Q	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPERMI2Q XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute quad-words from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2Q	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPERMI2Q YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute quad-words from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2Q	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPERMI2Q ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute quad-words from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
VPERMI2W	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPERMI2W XMM1{K1}{Z},XMM2,XMM3/M128	Permute word integers from two tables in xmm3/m128 and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
VPERMI2W	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPERMI2W YMM1{K1}{Z},YMM2,YMM3/M256	Permute word integers from two tables in ymm3/m256 and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
VPERMI2W	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPERMI2W ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute word integers from two tables in zmm3/m512 and zmm2 using indexes in zmm1 and store the result in zmm1 using writemask k1.
VPERMILPD	XMM,XMM,XMM/M128	AVX	VPERMILPD XMM1,XMM2,XMM3/M128	Permute double-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1.
VPERMILPD	XMM,XMM/M128,IMM8	AVX	VPERMILPD XMM1,XMM2/M128,IMM8	Permute double-precision floating-point values in xmm2/m128 using controls from imm8.
VPERMILPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPERMILPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Permute double-precision floating-point values in xmm2 using control from xmm3/m128/m64bcst and store the result in xmm1 using writemask k1.
VPERMILPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPERMILPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Permute double-precision floating-point values in xmm2/m128/m64bcst using controls from imm8 and store the result in xmm1 using writemask k1.
VPERMILPD	YMM,YMM,YMM/M256	AVX	VPERMILPD YMM1,YMM2,YMM3/M256	Permute double-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1.
VPERMILPD	YMM,YMM/M256,IMM8	AVX	VPERMILPD YMM1,YMM2/M256,IMM8	Permute double-precision floating-point values in ymm2/m256 using controls from imm8.
VPERMILPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPERMILPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute double-precision floating-point values in ymm2 using control from ymm3/m256/m64bcst and store the result in ymm1 using writemask k1.
VPERMILPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPERMILPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute double-precision floating-point values in ymm2/m256/m64bcst using controls from imm8 and store the result in ymm1 using writemask k1.
VPERMILPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPERMILPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute double-precision floating-point values in zmm2 using control from zmm3/m512/m64bcst and store the result in zmm1 using writemask k1.
VPERMILPD	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512F	VPERMILPD ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute double-precision floating-point values in zmm2/m512/m64bcst using controls from imm8 and store the result in zmm1 using writemask k1.
VPERMILPS	XMM,XMM,XMM/M128	AVX	VPERMILPS XMM1,XMM2,XMM3/M128	Permute single-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1.
VPERMILPS	XMM,XMM/M128,IMM8	AVX	VPERMILPS XMM1,XMM2/M128,IMM8	Permute single-precision floating-point values in xmm2/m128 using controls from imm8 and store result in xmm1.
VPERMILPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPERMILPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Permute single-precision floating-point values xmm2 using control from xmm3/m128/m32bcst and store the result in xmm1 using writemask k1.
VPERMILPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPERMILPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Permute single-precision floating-point values xmm2/m128/m32bcst using controls from imm8 and store the result in xmm1 using writemask k1.
VPERMILPS	YMM,YMM,YMM/M256	AVX	VPERMILPS YMM1,YMM2,YMM3/M256	Permute single-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1.
VPERMILPS	YMM,YMM/M256,IMM8	AVX	VPERMILPS YMM1,YMM2/M256,IMM8	Permute single-precision floating-point values in ymm2/m256 using controls from imm8 and store result in ymm1.
VPERMILPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPERMILPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute single-precision floating-point values ymm2 using control from ymm3/m256/m32bcst and store the result in ymm1 using writemask k1.
VPERMILPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPERMILPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Permute single-precision floating-point values ymm2/m256/m32bcst using controls from imm8 and store the result in ymm1 using writemask k1.
VPERMILPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPERMILPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute single-precision floating-point values zmm2 using control from zmm3/m512/m32bcst and store the result in zmm1 using writemask k1.
VPERMILPS	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512F	VPERMILPS ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Permute single-precision floating-point values zmm2/m512/m32bcst using controls from imm8 and store the result in zmm1 using writemask k1.
VPERMPD	YMM,YMM/M256,IMM8	AVX2	VPERMPD YMM1,YMM2/M256,IMM8	Permute double-precision floating-point elements in ymm2/m256 using indices in imm8 and store the result in ymm1.
VPERMPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPERMPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute double-precision floating-point elements in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1 subject to writemask k1.
VPERMPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPERMPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute double-precision floating-point elements in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1 subject to writemask k1.
VPERMPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPERMPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute double-precision floating-point elements in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1 subject to writemask k1.
VPERMPD	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512F	VPERMPD ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute double-precision floating-point elements in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1 subject to writemask k1.
VPERMPS	YMM,YMM,YMM/M256	AVX2	VPERMPS YMM1,YMM2,YMM3/M256	Permute single-precision floating-point elements in ymm3/m256 using indices in ymm2 and store the result in ymm1.
VPERMPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPERMPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Permute single-precision floating-point elements in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 subject to write mask k1.
VPERMPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPERMPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Permute single-precision floating-point values in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 subject to write mask k1.
VPERMQ	YMM,YMM/M256,IMM8	AVX2	VPERMQ YMM1,YMM2/M256,IMM8	Permute qwords in ymm2/m256 using indices in imm8 and store the result in ymm1.
VPERMQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPERMQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Permute qwords in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1.
VPERMQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPERMQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Permute qwords in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1.
VPERMQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPERMQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Permute qwords in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1.
VPERMQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512F	VPERMQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Permute qwords in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1.
VPERMW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPERMW XMM1{K1}{Z},XMM2,XMM3/M128	Permute word integers in xmm3/m128 using indexes in xmm2 and store the result in xmm1 using writemask k1.
VPERMW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPERMW YMM1{K1}{Z},YMM2,YMM3/M256	Permute word integers in ymm3/m256 using indexes in ymm2 and store the result in ymm1 using writemask k1.
VPERMW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPERMW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Permute word integers in zmm3/m512 using indexes in zmm2 and store the result in zmm1 using writemask k1.
VPEXPANDD	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VPEXPANDD XMM1{K1}{Z},XMM2/M128	Expand packed double-word integer values from xmm2/m128 to xmm1 using writemask k1.
VPEXPANDD	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VPEXPANDD YMM1{K1}{Z},YMM2/M256	Expand packed double-word integer values from ymm2/m256 to ymm1 using writemask k1.
VPEXPANDD	ZMM{K}{Z},ZMM/M512	AVX512F	VPEXPANDD ZMM1{K1}{Z},ZMM2/M512	Expand packed double-word integer values from zmm2/m512 to zmm1 using writemask k1.
VPEXPANDQ	XMM{K}{Z},XMM/M128	AVX512VL,AVX512F	VPEXPANDQ XMM1{K1}{Z},XMM2/M128	Expand packed quad-word integer values from xmm2/m128 to xmm1 using writemask k1.
VPEXPANDQ	YMM{K}{Z},YMM/M256	AVX512VL,AVX512F	VPEXPANDQ YMM1{K1}{Z},YMM2/M256	Expand packed quad-word integer values from ymm2/m256 to ymm1 using writemask k1.
VPEXPANDQ	ZMM{K}{Z},ZMM/M512	AVX512F	VPEXPANDQ ZMM1{K1}{Z},ZMM2/M512	Expand packed quad-word integer values from zmm2/m512 to zmm1 using writemask k1.
VPEXTRB	REG/M8,XMM,IMM8	AVX	VPEXTRB REG/M8,XMM2,IMM8	Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros.
VPEXTRD	R32/M32,XMM,IMM8	AVX	VPEXTRD R32/M32,XMM2,IMM8	Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
VPEXTRQ	R64/M64,XMM,IMM8	AVX	VPEXTRQ R64/M64,XMM2,IMM8	Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
VPEXTRW	REG,XMM,IMM8	AVX	VPEXTRW REG,XMM1,IMM8	Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros.
VPEXTRW	REG/M16,XMM,IMM8	AVX	VPEXTRW REG/M16,XMM2,IMM8	Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros.
VPGATHERDD	XMM,VM32X,XMM	AVX2	VPGATHERDD XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERDD	XMM{K},VM32X	AVX512VL,AVX512F	VPGATHERDD XMM1{K1},VM32X	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERDD	YMM,VM32Y,YMM	AVX2	VPGATHERDD YMM1,VM32Y,YMM2	Using dword indices specified in vm32y, gather dword from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VPGATHERDD	YMM{K},VM32Y	AVX512VL,AVX512F	VPGATHERDD YMM1{K1},VM32Y	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERDD	ZMM{K},VM32Z	AVX512F	VPGATHERDD ZMM1{K1},VM32Z	Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERDQ	XMM,VM32X,XMM	AVX2	VPGATHERDQ XMM1,VM32X,XMM2	Using dword indices specified in vm32x, gather qword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERDQ	XMM{K},VM32X	AVX512VL,AVX512F	VPGATHERDQ XMM1{K1},VM32X	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERDQ	YMM,VM32X,YMM	AVX2	VPGATHERDQ YMM1,VM32X,YMM2	Using dword indices specified in vm32x, gather qword values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VPGATHERDQ	YMM{K},VM32X	AVX512VL,AVX512F	VPGATHERDQ YMM1{K1},VM32X	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERDQ	ZMM{K},VM32Y	AVX512F	VPGATHERDQ ZMM1{K1},VM32Y	Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERQD	XMM,VM64X,XMM	AVX2	VPGATHERQD XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERQD	XMM,VM64Y,XMM	AVX2	VPGATHERQD XMM1,VM64Y,XMM2	Using qword indices specified in vm64y, gather dword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERQD	XMM{K},VM64X	AVX512VL,AVX512F	VPGATHERQD XMM1{K1},VM64X	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERQD	XMM{K},VM64Y	AVX512VL,AVX512F	VPGATHERQD XMM1{K1},VM64Y	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERQD	YMM{K},VM64Z	AVX512F	VPGATHERQD YMM1{K1},VM64Z	Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking.
VPGATHERQQ	XMM,VM64X,XMM	AVX2	VPGATHERQQ XMM1,VM64X,XMM2	Using qword indices specified in vm64x, gather qword values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1.
VPGATHERQQ	XMM{K},VM64X	AVX512VL,AVX512F	VPGATHERQQ XMM1{K1},VM64X	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERQQ	YMM,VM64Y,YMM	AVX2	VPGATHERQQ YMM1,VM64Y,YMM2	Using qword indices specified in vm64y, gather qword values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1.
VPGATHERQQ	YMM{K},VM64Y	AVX512VL,AVX512F	VPGATHERQQ YMM1{K1},VM64Y	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPGATHERQQ	ZMM{K},VM64Z	AVX512F	VPGATHERQQ ZMM1{K1},VM64Z	Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking.
VPHADDD	XMM,XMM,XMM/M128	AVX	VPHADDD XMM1,XMM2,XMM3/M128	Add 32-bit integers horizontally, pack to xmm1.
VPHADDD	YMM,YMM,YMM/M256	AVX2	VPHADDD YMM1,YMM2,YMM3/M256	Add 32-bit signed integers horizontally, pack to ymm1.
VPHADDSW	XMM,XMM,XMM/M128	AVX	VPHADDSW XMM1,XMM2,XMM3/M128	Add 16-bit signed integers horizontally, pack saturated integers to xmm1.
VPHADDSW	YMM,YMM,YMM/M256	AVX2	VPHADDSW YMM1,YMM2,YMM3/M256	Add 16-bit signed integers horizontally, pack saturated integers to ymm1.
VPHADDW	XMM,XMM,XMM/M128	AVX	VPHADDW XMM1,XMM2,XMM3/M128	Add 16-bit integers horizontally, pack to xmm1.
VPHADDW	YMM,YMM,YMM/M256	AVX2	VPHADDW YMM1,YMM2,YMM3/M256	Add 16-bit signed integers horizontally, pack to ymm1.
VPHMINPOSUW	XMM,XMM/M128	AVX	VPHMINPOSUW XMM1,XMM2/M128	Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1.
VPHSUBD	XMM,XMM,XMM/M128	AVX	VPHSUBD XMM1,XMM2,XMM3/M128	Subtract 32-bit signed integers horizontally, pack to xmm1.
VPHSUBD	YMM,YMM,YMM/M256	AVX2	VPHSUBD YMM1,YMM2,YMM3/M256	Subtract 32-bit signed integers horizontally, pack to ymm1.
VPHSUBSW	XMM,XMM,XMM/M128	AVX	VPHSUBSW XMM1,XMM2,XMM3/M128	Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1.
VPHSUBSW	YMM,YMM,YMM/M256	AVX2	VPHSUBSW YMM1,YMM2,YMM3/M256	Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1.
VPHSUBW	XMM,XMM,XMM/M128	AVX	VPHSUBW XMM1,XMM2,XMM3/M128	Subtract 16-bit signed integers horizontally, pack to xmm1.
VPHSUBW	YMM,YMM,YMM/M256	AVX2	VPHSUBW YMM1,YMM2,YMM3/M256	Subtract 16-bit signed integers horizontally, pack to ymm1.
VPINSRB	XMM,XMM,R32/M8,IMM8	AVX	VPINSRB XMM1,XMM2,R32/M8,IMM8	Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
VPINSRD	XMM,XMM,R/M32,IMM8	AVX	VPINSRD XMM1,XMM2,R/M32,IMM8	Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
VPINSRD	XMM,XMM,R32/M32,IMM8	AVX512DQ	VPINSRD XMM1,XMM2,R32/M32,IMM8	Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
VPINSRQ	XMM,XMM,R/M64,IMM8	AVX	VPINSRQ XMM1,XMM2,R/M64,IMM8	Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
VPINSRQ	XMM,XMM,R64/M64,IMM8	AVX512DQ	VPINSRQ XMM1,XMM2,R64/M64,IMM8	Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
VPINSRW	XMM,XMM,R32/M16,IMM8	AVX,AVX512BW	VPINSRW XMM1,XMM2,R32/M16,IMM8	Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8. rest from xmm2 into xmm1 at the word offset in imm8.
VPLZCNTD	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512CD	VPLZCNTD XMM1{K1}{Z},XMM2/M128/M32BCST	Count the number of leading zero bits in each dword element of xmm2/m128/m32bcst using writemask k1.
VPLZCNTD	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512CD	VPLZCNTD YMM1{K1}{Z},YMM2/M256/M32BCST	Count the number of leading zero bits in each dword element of ymm2/m256/m32bcst using writemask k1.
VPLZCNTD	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512CD	VPLZCNTD ZMM1{K1}{Z},ZMM2/M512/M32BCST	Count the number of leading zero bits in each dword element of zmm2/m512/m32bcst using writemask k1.
VPLZCNTQ	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512CD	VPLZCNTQ XMM1{K1}{Z},XMM2/M128/M64BCST	Count the number of leading zero bits in each qword element of xmm2/m128/m64bcst using writemask k1.
VPLZCNTQ	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512CD	VPLZCNTQ YMM1{K1}{Z},YMM2/M256/M64BCST	Count the number of leading zero bits in each qword element of ymm2/m256/m64bcst using writemask k1.
VPLZCNTQ	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512CD	VPLZCNTQ ZMM1{K1}{Z},ZMM2/M512/M64BCST	Count the number of leading zero bits in each qword element of zmm2/m512/m64bcst using writemask k1.
VPMADDUBSW	XMM,XMM,XMM/M128	AVX	VPMADDUBSW XMM1,XMM2,XMM3/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1.
VPMADDUBSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMADDUBSW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1 under writemask k1.
VPMADDUBSW	YMM,YMM,YMM/M256	AVX2	VPMADDUBSW YMM1,YMM2,YMM3/M256	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1.
VPMADDUBSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMADDUBSW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1 under writemask k1. horizontal pair of signed words, pack saturated signed-words to zmm1 under writemask k1.
VPMADDWD	XMM,XMM,XMM/M128	AVX	VPMADDWD XMM1,XMM2,XMM3/M128	Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1.
VPMADDWD	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMADDWD XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1 under writemask k1.
VPMADDWD	YMM,YMM,YMM/M256	AVX2	VPMADDWD YMM1,YMM2,YMM3/M256	Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1.
VPMADDWD	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMADDWD YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1 under writemask k1.
VPMADDWD	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMADDWD ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed word integers in zmm2 by the packed word integers in zmm3/m512, add adjacent doubleword results, and store in zmm1 under writemask k1.
VPMASKMOVD	M128,XMM,XMM	AVX2	VPMASKMOVD M128,XMM1,XMM2	Conditionally store dword values from xmm2 using mask in xmm1.
VPMASKMOVD	M256,YMM,YMM	AVX2	VPMASKMOVD M256,YMM1,YMM2	Conditionally store dword values from ymm2 using mask in ymm1.
VPMASKMOVD	XMM,XMM,M128	AVX2	VPMASKMOVD XMM1,XMM2,M128	Conditionally load dword values from m128 using mask in xmm2 and store in xmm1.
VPMASKMOVD	YMM,YMM,M256	AVX2	VPMASKMOVD YMM1,YMM2,M256	Conditionally load dword values from m256 using mask in ymm2 and store in ymm1.
VPMASKMOVQ	M128,XMM,XMM	AVX2	VPMASKMOVQ M128,XMM1,XMM2	Conditionally store qword values from xmm2 using mask in xmm1.
VPMASKMOVQ	M256,YMM,YMM	AVX2	VPMASKMOVQ M256,YMM1,YMM2	Conditionally store qword values from ymm2 using mask in ymm1.
VPMASKMOVQ	XMM,XMM,M128	AVX2	VPMASKMOVQ XMM1,XMM2,M128	Conditionally load qword values from m128 using mask in xmm2 and store in xmm1.
VPMASKMOVQ	YMM,YMM,M256	AVX2	VPMASKMOVQ YMM1,YMM2,M256	Conditionally load qword values from m256 using mask in ymm2 and store in ymm1.
VPMAXSB	XMM,XMM,XMM/M128	AVX	VPMAXSB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMAXSB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXSB	YMM,YMM,YMM/M256	AVX2	VPMAXSB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMAXSB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMAXSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
VPMAXSD	XMM,XMM,XMM/M128	AVX	VPMAXSD XMM1,XMM2,XMM3/M128	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXSD	YMM,YMM,YMM/M256	AVX2	VPMAXSD YMM1,YMM2,YMM3/M256	Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXSW	XMM,XMM,XMM/M128	AVX	VPMAXSW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1.
VPMAXSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMAXSW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXSW	YMM,YMM,YMM/M256	AVX2	VPMAXSW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm3/m256 and ymm2 and store packed maximum values in ymm1.
VPMAXSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMAXSW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXUB	XMM,XMM,XMM/M128	AVX	VPMAXUB XMM1,XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXUB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMAXUB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXUB	YMM,YMM,YMM/M256	AVX2	VPMAXUB YMM1,YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXUB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMAXUB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXUB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMAXUB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
VPMAXUD	XMM,XMM,XMM/M128	AVX	VPMAXUD XMM1,XMM2,XMM3/M128	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1.
VPMAXUD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPMAXUD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 under writemask k1.
VPMAXUD	YMM,YMM,YMM/M256	AVX2	VPMAXUD YMM1,YMM2,YMM3/M256	Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1.
VPMAXUD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPMAXUD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 under writemask k1.
VPMAXUD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPMAXUD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 under writemask k1.
VPMAXUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPMAXUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 under writemask k1.
VPMAXUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPMAXUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 under writemask k1.
VPMAXUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPMAXUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 under writemask k1.
VPMAXUW	XMM,XMM,XMM/M128	AVX	VPMAXUW XMM1,XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1.
VPMAXUW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMAXUW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1.
VPMAXUW	YMM,YMM,YMM/M256	AVX2	VPMAXUW YMM1,YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1.
VPMAXUW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMAXUW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1.
VPMAXUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMAXUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1.
VPMINSB	XMM,XMM,XMM/M128	AVX	VPMINSB XMM1,XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMINSB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSB	YMM,YMM,YMM/M256	AVX2	VPMINSB YMM1,YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMINSB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMINSB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
VPMINSD	XMM,XMM,XMM/M128	AVX	VPMINSD XMM1,XMM2,XMM3/M128	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINSD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPMINSD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSD	YMM,YMM,YMM/M256	AVX2	VPMINSD YMM1,YMM2,YMM3/M256	Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed minimum values in ymm1.
VPMINSD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPMINSD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPMINSD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1.
VPMINSQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPMINSQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed signed qword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPMINSQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed signed qword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPMINSQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1.
VPMINSW	XMM,XMM,XMM/M128	AVX	VPMINSW XMM1,XMM2,XMM3/M128	Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
VPMINSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMINSW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed signed word integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINSW	YMM,YMM,YMM/M256	AVX2	VPMINSW YMM1,YMM2,YMM3/M256	Compare packed signed word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1.
VPMINSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMINSW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed signed word integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMINSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed signed word integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
VPMINUB	XMM,XMM,XMM/M128	AVX	VPMINUB XMM1,XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINUB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMINUB XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1.
VPMINUB	YMM,YMM,YMM/M256	AVX2	VPMINUB YMM1,YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINUB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMINUB YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1.
VPMINUB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMINUB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1.
VPMINUD	XMM,XMM,XMM/M128	AVX	VPMINUD XMM1,XMM2,XMM3/M128	Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1.
VPMINUD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPMINUD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed minimum values in xmm1 under writemask k1.
VPMINUD	YMM,YMM,YMM/M256	AVX2	VPMINUD YMM1,YMM2,YMM3/M256	Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1.
VPMINUD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPMINUD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed minimum values in ymm1 under writemask k1.
VPMINUD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPMINUD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1.
VPMINUQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPMINUQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed minimum values in xmm1 under writemask k1.
VPMINUQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPMINUQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed minimum values in ymm1 under writemask k1.
VPMINUQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPMINUQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1.
VPMINUW	XMM,XMM,XMM/M128	AVX	VPMINUW XMM1,XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1.
VPMINUW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMINUW XMM1{K1}{Z},XMM2,XMM3/M128	Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1 under writemask k1.
VPMINUW	YMM,YMM,YMM/M256	AVX2	VPMINUW YMM1,YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1.
VPMINUW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMINUW YMM1{K1}{Z},YMM2,YMM3/M256	Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1 under writemask k1.
VPMINUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMINUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Compare packed unsigned word integers in zmm3/m512 and zmm2 and return packed minimum values in zmm1 under writemask k1.
VPMOVB2M	K,XMM	AVX512VL,AVX512BW	VPMOVB2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in XMM1.
VPMOVB2M	K,YMM	AVX512VL,AVX512BW	VPMOVB2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in YMM1.
VPMOVB2M	K,ZMM	AVX512BW	VPMOVB2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in ZMM1.
VPMOVD2M	K,XMM	AVX512VL,AVX512DQ	VPMOVD2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in XMM1.
VPMOVD2M	K,YMM	AVX512VL,AVX512DQ	VPMOVD2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in YMM1.
VPMOVD2M	K,ZMM	AVX512DQ	VPMOVD2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in ZMM1.
VPMOVDB	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed double-word integers from zmm2 into 16 packed byte integers in xmm1/m128 with truncation under writemask k1.
VPMOVDB	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed double-word integers from xmm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1.
VPMOVDB	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed double-word integers from ymm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1.
VPMOVDW	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed double-word integers from ymm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1.
VPMOVDW	XMM/M64{K}{Z},XMM	AVX512VL,AVX512F	VPMOVDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed double-word integers from xmm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1.
VPMOVDW	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed double-word integers from zmm2 into 16 packed word integers in ymm1/m256 with truncation under writemask k1.
VPMOVM2B	XMM,K	AVX512VL,AVX512BW	VPMOVM2B XMM1,K1	Sets each byte in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2B	YMM,K	AVX512VL,AVX512BW	VPMOVM2B YMM1,K1	Sets each byte in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2B	ZMM,K	AVX512BW	VPMOVM2B ZMM1,K1	Sets each byte in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2D	XMM,K	AVX512VL,AVX512DQ	VPMOVM2D XMM1,K1	Sets each doubleword in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2D	YMM,K	AVX512VL,AVX512DQ	VPMOVM2D YMM1,K1	Sets each doubleword in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2D	ZMM,K	AVX512DQ	VPMOVM2D ZMM1,K1	Sets each doubleword in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2Q	XMM,K	AVX512VL,AVX512DQ	VPMOVM2Q XMM1,K1	Sets each quadword in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2Q	YMM,K	AVX512VL,AVX512DQ	VPMOVM2Q YMM1,K1	Sets each quadword in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2Q	ZMM,K	AVX512DQ	VPMOVM2Q ZMM1,K1	Sets each quadword in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2W	XMM,K	AVX512VL,AVX512BW	VPMOVM2W XMM1,K1	Sets each word in XMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2W	YMM,K	AVX512VL,AVX512BW	VPMOVM2W YMM1,K1	Sets each word in YMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVM2W	ZMM,K	AVX512BW	VPMOVM2W ZMM1,K1	Sets each word in ZMM1 to all 1’s or all 0’s based on the value of the corresponding bit in k1.
VPMOVMSKB	REG,XMM	AVX	VPMOVMSKB REG,XMM1	Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros.
VPMOVMSKB	REG,YMM	AVX2	VPMOVMSKB REG,YMM1	Move a 32-bit mask of ymm1 to reg. The upper bits of r64 are filled with zeros.
VPMOVQ2M	K,XMM	AVX512VL,AVX512DQ	VPMOVQ2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in XMM1.
VPMOVQ2M	K,YMM	AVX512VL,AVX512DQ	VPMOVQ2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in YMM1.
VPMOVQ2M	K,ZMM	AVX512DQ	VPMOVQ2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in ZMM1.
VPMOVQB	XMM/M16{K}{Z},XMM	AVX512VL,AVX512F	VPMOVQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed byte integers in xmm1/m16 with truncation under writemask k1.
VPMOVQB	XMM/M32{K}{Z},YMM	AVX512VL,AVX512F	VPMOVQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1.
VPMOVQB	XMM/M64{K}{Z},ZMM	AVX512F	VPMOVQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1.
VPMOVQD	XMM/M128{K}{Z},XMM	AVX512VL,AVX512F	VPMOVQD XMM1/M128{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed double-word integers in xmm1/m128 with truncation subject to writemask k1.
VPMOVQD	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed double-word integers in xmm1/m128 with truncation subject to writemask k1.
VPMOVQD	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed double-word integers in ymm1/m256 with truncation subject to writemask k1.
VPMOVQW	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed quad-word integers from zmm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1.
VPMOVQW	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVQW XMM1/M32{K1}{Z},XMM2	Converts 2 packed quad-word integers from xmm2 into 2 packed word integers in xmm1/m32 with truncation under writemask k1.
VPMOVQW	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed quad-word integers from ymm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1.
VPMOVSDB	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVSDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed signed double-word integers from zmm2 into 16 packed signed byte integers in xmm1/m128 using signed saturation under writemask k1.
VPMOVSDB	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVSDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed signed double-word integers from xmm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSDB	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVSDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed signed double-word integers from ymm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1.
VPMOVSDW	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVSDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed signed double-word integers from ymm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1.
VPMOVSDW	XMM/M64{K}{Z},XMM	AVX512VL,AVX512F	VPMOVSDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed signed double-word integers from xmm2 into 4 packed signed word integers in ymm1/m64 using signed saturation under writemask k1.
VPMOVSDW	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVSDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed signed double-word integers from zmm2 into 16 packed signed word integers in ymm1/m256 using signed saturation under writemask k1.
VPMOVSQB	XMM/M16{K}{Z},XMM	AVX512VL,AVX512F	VPMOVSQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed byte integers in xmm1/m16 using signed saturation under writemask k1.
VPMOVSQB	XMM/M32{K}{Z},YMM	AVX512VL,AVX512F	VPMOVSQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSQB	XMM/M64{K}{Z},ZMM	AVX512F	VPMOVSQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1.
VPMOVSQD	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVSQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed double-word integers in xmm1/m128 using signed saturation subject to writemask k1.
VPMOVSQD	XMM/M64{K}{Z},XMM	AVX512VL,AVX512F	VPMOVSQD XMM1/M64{K1}{Z},XMM2	Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed double-word integers in xmm1/m64 using signed saturation subject to writemask k1.
VPMOVSQD	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVSQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed double-word integers in ymm1/m256 using signed saturation subject to writemask k1.
VPMOVSQW	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVSQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1.
VPMOVSQW	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVSQW XMM1/M32{K1}{Z},XMM2	Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m32 using signed saturation under writemask k1.
VPMOVSQW	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVSQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed word integers in xmm1/m64 using signed saturation under writemask k1.
VPMOVSWB	XMM/M128{K}{Z},YMM	AVX512VL,AVX512BW	VPMOVSWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed signed word integers from ymm2 into 16 packed signed bytes in xmm1/m128 using signed saturation under writemask k1.
VPMOVSWB	XMM/M64{K}{Z},XMM	AVX512VL,AVX512BW	VPMOVSWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed signed word integers from xmm2 into 8 packed signed bytes in xmm1/m64 using signed saturation under writemask k1.
VPMOVSWB	YMM/M256{K}{Z},ZMM	AVX512BW	VPMOVSWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed signed word integers from zmm2 into 32 packed signed bytes in ymm1/m256 using signed saturation under writemask k1.
VPMOVSXBD	XMM,XMM/M32	AVX	VPMOVSXBD XMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VPMOVSXBD	YMM,XMM/M64	AVX2	VPMOVSXBD YMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VPMOVSXBQ	XMM,XMM/M16	AVX	VPMOVSXBQ XMM1,XMM2/M16	Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VPMOVSXBQ	YMM,XMM/M32	AVX2	VPMOVSXBQ YMM1,XMM2/M32	Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VPMOVSXBW	XMM,XMM/M64	AVX	VPMOVSXBW XMM1,XMM2/M64	Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VPMOVSXBW	YMM,XMM/M128	AVX2	VPMOVSXBW YMM1,XMM2/M128	Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVSXDQ	XMM,XMM/M64	AVX	VPMOVSXDQ XMM1,XMM2/M64	Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VPMOVSXWD	XMM,XMM/M64	AVX	VPMOVSXWD XMM1,XMM2/M64	Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VPMOVSXWD	YMM,XMM/M128	AVX2	VPMOVSXWD YMM1,XMM2/M128	Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1.
VPMOVSXWQ	XMM,XMM/M32	AVX	VPMOVSXWQ XMM1,XMM2/M32	Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VPMOVUSDB	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVUSDB XMM1/M128{K1}{Z},ZMM2	Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned byte integers in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSDB	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVUSDB XMM1/M32{K1}{Z},XMM2	Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSDB	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVUSDB XMM1/M64{K1}{Z},YMM2	Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSDW	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVUSDW XMM1/M128{K1}{Z},YMM2	Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSDW	XMM/M64{K}{Z},XMM	AVX512VL,AVX512F	VPMOVUSDW XMM1/M64{K1}{Z},XMM2	Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSDW	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVUSDW YMM1/M256{K1}{Z},ZMM2	Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned word integers in ymm1/m256 using unsigned saturation under writemask k1.
VPMOVUSQB	XMM/M16{K}{Z},XMM	AVX512VL,AVX512F	VPMOVUSQB XMM1/M16{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned byte integers in xmm1/m16 using unsigned saturation under writemask k1.
VPMOVUSQB	XMM/M32{K}{Z},YMM	AVX512VL,AVX512F	VPMOVUSQB XMM1/M32{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSQB	XMM/M64{K}{Z},ZMM	AVX512F	VPMOVUSQB XMM1/M64{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSQD	XMM/M128{K}{Z},YMM	AVX512VL,AVX512F	VPMOVUSQD XMM1/M128{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned double-word integers in xmm1/m128 using unsigned saturation subject to writemask k1.
VPMOVUSQD	XMM/M64{K}{Z},XMM	AVX512VL,AVX512F	VPMOVUSQD XMM1/M64{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned double-word integers in xmm1/m64 using unsigned saturation subject to writemask k1.
VPMOVUSQD	YMM/M256{K}{Z},ZMM	AVX512F	VPMOVUSQD YMM1/M256{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned double-word integers in ymm1/m256 using unsigned saturation subject to writemask k1.
VPMOVUSQW	XMM/M128{K}{Z},ZMM	AVX512F	VPMOVUSQW XMM1/M128{K1}{Z},ZMM2	Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSQW	XMM/M32{K}{Z},XMM	AVX512VL,AVX512F	VPMOVUSQW XMM1/M32{K1}{Z},XMM2	Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned word integers in xmm1/m32 using unsigned saturation under writemask k1.
VPMOVUSQW	XMM/M64{K}{Z},YMM	AVX512VL,AVX512F	VPMOVUSQW XMM1/M64{K1}{Z},YMM2	Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1.
VPMOVUSWB	XMM/M128{K}{Z},YMM	AVX512VL,AVX512BW	VPMOVUSWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed unsigned word integers from ymm2 into 16 packed unsigned bytes in xmm1/m128 using unsigned saturation under writemask k1.
VPMOVUSWB	XMM/M64{K}{Z},XMM	AVX512VL,AVX512BW	VPMOVUSWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed unsigned word integers from xmm2 into 8 packed unsigned bytes in 8mm1/m64 using unsigned saturation under writemask k1.
VPMOVUSWB	YMM/M256{K}{Z},ZMM	AVX512BW	VPMOVUSWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed unsigned word integers from zmm2 into 32 packed unsigned bytes in ymm1/m256 using unsigned saturation under writemask k1.
VPMOVW2M	K,XMM	AVX512VL,AVX512BW	VPMOVW2M K1,XMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in XMM1.
VPMOVW2M	K,YMM	AVX512VL,AVX512BW	VPMOVW2M K1,YMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in YMM1.
VPMOVW2M	K,ZMM	AVX512BW	VPMOVW2M K1,ZMM1	Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in ZMM1.
VPMOVWB	XMM/M128{K}{Z},YMM	AVX512VL,AVX512BW	VPMOVWB XMM1/M128{K1}{Z},YMM2	Converts 16 packed word integers from ymm2 into 16 packed bytes in xmm1/m128 with truncation under writemask k1.
VPMOVWB	XMM/M64{K}{Z},XMM	AVX512VL,AVX512BW	VPMOVWB XMM1/M64{K1}{Z},XMM2	Converts 8 packed word integers from xmm2 into 8 packed bytes in xmm1/m64 with truncation under writemask k1.
VPMOVWB	YMM/M256{K}{Z},ZMM	AVX512BW	VPMOVWB YMM1/M256{K1}{Z},ZMM2	Converts 32 packed word integers from zmm2 into 32 packed bytes in ymm1/m256 with truncation under writemask k1.
VPMOVZXBD	XMM,XMM/M32	AVX	VPMOVZXBD XMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1.
VPMOVZXBD	YMM,XMM/M64	AVX2	VPMOVZXBD YMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1.
VPMOVZXBQ	XMM,XMM/M16	AVX	VPMOVZXBQ XMM1,XMM2/M16	Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1.
VPMOVZXBQ	YMM,XMM/M32	AVX2	VPMOVZXBQ YMM1,XMM2/M32	Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1.
VPMOVZXBW	XMM,XMM/M64	AVX	VPMOVZXBW XMM1,XMM2/M64	Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1.
VPMOVZXBW	YMM,XMM/M128	AVX2	VPMOVZXBW YMM1,XMM2/M128	Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1.
VPMOVZXDQ	XMM,XMM/M64	AVX	VPMOVZXDQ XMM1,XMM2/M64	Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1.
VPMOVZXWD	XMM,XMM/M64	AVX	VPMOVZXWD XMM1,XMM2/M64	Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1.
VPMOVZXWD	YMM,XMM/M128	AVX2	VPMOVZXWD YMM1,XMM2/M128	Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1.
VPMOVZXWQ	XMM,XMM/M32	AVX	VPMOVZXWQ XMM1,XMM2/M32	Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1.
VPMULDQ	XMM,XMM,XMM/M128	AVX	VPMULDQ XMM1,XMM2,XMM3/M128	Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VPMULDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPMULDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 using writemask k1.
VPMULDQ	YMM,YMM,YMM/M256	AVX2	VPMULDQ YMM1,YMM2,YMM3/M256	Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1.
VPMULDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPMULDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 using writemask k1.
VPMULDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPMULDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply packed signed doubleword integers in zmm2 by packed signed doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 using writemask k1.
VPMULHRSW	XMM,XMM,XMM/M128	AVX	VPMULHRSW XMM1,XMM2,XMM3/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1.
VPMULHRSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMULHRSW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1 under writemask k1.
VPMULHRSW	YMM,YMM,YMM/M256	AVX2	VPMULHRSW YMM1,YMM2,YMM3/M256	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1.
VPMULHRSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMULHRSW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1 under writemask k1.
VPMULHRSW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMULHRSW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to zmm1 under writemask k1.
VPMULHUW	XMM,XMM,XMM/M128	AVX	VPMULHUW XMM1,XMM2,XMM3/M128	Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VPMULHUW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMULHUW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1.
VPMULHUW	YMM,YMM,YMM/M256	AVX2	VPMULHUW YMM1,YMM2,YMM3/M256	Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
VPMULHUW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMULHUW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1.
VPMULHUW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMULHUW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed unsigned word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1.
VPMULHW	XMM,XMM,XMM/M128	AVX	VPMULHW XMM1,XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1.
VPMULHW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMULHW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1.
VPMULHW	YMM,YMM,YMM/M256	AVX2	VPMULHW YMM1,YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1.
VPMULHW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMULHW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1.
VPMULHW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPMULHW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1.
VPMULLD	XMM,XMM,XMM/M128	AVX	VPMULLD XMM1,XMM2,XMM3/M128	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VPMULLD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPMULLD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Multiply the packed dword signed integers in xmm2 and xmm3/m128/m32bcst and store the low 32 bits of each product in xmm1 under writemask k1.
VPMULLD	YMM,YMM,YMM/M256	AVX2	VPMULLD YMM1,YMM2,YMM3/M256	Multiply the packed dword signed integers in ymm2 and ymm3/m256 and store the low 32 bits of each product in ymm1.
VPMULLD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPMULLD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Multiply the packed dword signed integers in ymm2 and ymm3/m256/m32bcst and store the low 32 bits of each product in ymm1 under writemask k1.
VPMULLD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPMULLD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Multiply the packed dword signed integers in zmm2 and zmm3/m512/m32bcst and store the low 32 bits of each product in zmm1 under writemask k1.
VPMULLQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512DQ	VPMULLQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply the packed qword signed integers in xmm2 and xmm3/m128/m64bcst and store the low 64 bits of each product in xmm1 under writemask k1.
VPMULLQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512DQ	VPMULLQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply the packed qword signed integers in ymm2 and ymm3/m256/m64bcst and store the low 64 bits of each product in ymm1 under writemask k1.
VPMULLQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512DQ	VPMULLQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply the packed qword signed integers in zmm2 and zmm3/m512/m64bcst and store the low 64 bits of each product in zmm1 under writemask k1.
VPMULLW	XMM,XMM,XMM/M128	AVX	VPMULLW XMM1,XMM2,XMM3/M128	Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1.
VPMULLW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPMULLW XMM1{K1}{Z},XMM2,XMM3/M128	Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the low 16 bits of the results in xmm1 under writemask k1.
VPMULLW	YMM,YMM,YMM/M256	AVX2	VPMULLW YMM1,YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1.
VPMULLW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPMULLW YMM1{K1}{Z},YMM2,YMM3/M256	Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1 under writemask k1. zmm2 and zmm3/m512, and store the low 16 bits of the results in zmm1 under writemask k1.
VPMULUDQ	XMM,XMM,XMM/M128	AVX	VPMULUDQ XMM1,XMM2,XMM3/M128	Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1.
VPMULUDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPMULUDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 under writemask k1.
VPMULUDQ	YMM,YMM,YMM/M256	AVX2	VPMULUDQ YMM1,YMM2,YMM3/M256	Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1.
VPMULUDQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPMULUDQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 under writemask k1.
VPMULUDQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPMULUDQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Multiply packed unsigned doubleword integers in zmm2 by packed unsigned doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 under writemask k1.
VPOR	XMM,XMM,XMM/M128	AVX	VPOR XMM1,XMM2,XMM3/M128	Bitwise OR of xmm2/m128 and xmm3.
VPOR	YMM,YMM,YMM/M256	AVX2	VPOR YMM1,YMM2,YMM3/M256	Bitwise OR of ymm2/m256 and ymm3.
VPORD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPORD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise OR of packed doubleword integers in xmm2 and xmm3/m128/m32bcst using writemask k1.
VPORD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPORD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise OR of packed doubleword integers in ymm2 and ymm3/m256/m32bcst using writemask k1.
VPORD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPORD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise OR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1.
VPORQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPORQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise OR of packed quadword integers in xmm2 and xmm3/m128/m64bcst using writemask k1.
VPORQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPORQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise OR of packed quadword integers in ymm2 and ymm3/m256/m64bcst using writemask k1.
VPORQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPORQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise OR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1.
VPROLD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPROLD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rotate doublewords in xmm2/m128/m32bcst left by imm8. Result written to xmm1 using writemask k1.
VPROLD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPROLD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rotate doublewords in ymm2/m256/m32bcst left by imm8. Result written to ymm1 using writemask k1.
VPROLD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512F	VPROLD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Rotate left of doublewords in zmm3/m512/m32bcst by imm8. Result written to zmm1 using writemask k1.
VPROLQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPROLQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rotate quadwords in xmm2/m128/m64bcst left by imm8. Result written to xmm1 using writemask k1.
VPROLQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPROLQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rotate quadwords in ymm2/m256/m64bcst left by imm8. Result written to ymm1 using writemask k1.
VPROLQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512F	VPROLQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Rotate quadwords in zmm2/m512/m64bcst left by imm8. Result written to zmm1 using writemask k1.
VPROLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPROLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Rotate doublewords in xmm2 left by count in the corresponding element of xmm3/m128/m32bcst. Result written to xmm1 under writemask k1.
VPROLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPROLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Rotate doublewords in ymm2 left by count in the corresponding element of ymm3/m256/m32bcst. Result written to ymm1 under writemask k1.
VPROLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPROLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Rotate left of doublewords in zmm2 by count in the corresponding element of zmm3/m512/m32bcst. Result written to zmm1 using writemask k1.
VPROLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPROLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Rotate quadwords in xmm2 left by count in the corresponding element of xmm3/m128/m64bcst. Result written to xmm1 under writemask k1.
VPROLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPROLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Rotate quadwords in ymm2 left by count in the corresponding element of ymm3/m256/m64bcst. Result written to ymm1 under writemask k1.
VPROLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPROLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Rotate quadwords in zmm2 left by count in the corresponding element of zmm3/m512/m64bcst. Result written to zmm1under writemask k1.
VPRORD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPRORD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rotate doublewords in xmm2/m128/m32bcst right by imm8, store result using writemask k1.
VPRORD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPRORD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rotate doublewords in ymm2/m256/m32bcst right by imm8, store result using writemask k1.
VPRORD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512F	VPRORD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Rotate doublewords in zmm2/m512/m32bcst right by imm8, store result using writemask k1.
VPRORQ	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPRORQ XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rotate quadwords in xmm2/m128/m64bcst right by imm8, store result using writemask k1.
VPRORQ	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPRORQ YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rotate quadwords in ymm2/m256/m64bcst right by imm8, store result using writemask k1.
VPRORQ	ZMM{K}{Z},ZMM/M512/M64BCST,IMM8	AVX512F	VPRORQ ZMM1{K1}{Z},ZMM2/M512/M64BCST,IMM8	Rotate quadwords in zmm2/m512/m64bcst right by imm8, store result using writemask k1.
VPRORVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPRORVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Rotate doublewords in xmm2 right by count in the corresponding element of xmm3/m128/m32bcst, store result using writemask k1.
VPRORVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPRORVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Rotate doublewords in ymm2 right by count in the corresponding element of ymm3/m256/m32bcst, store using result writemask k1.
VPRORVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPRORVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Rotate doublewords in zmm2 right by count in the corresponding element of zmm3/m512/m32bcst, store result using writemask k1.
VPRORVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPRORVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Rotate quadwords in xmm2 right by count in the corresponding element of xmm3/m128/m64bcst, store result using writemask k1.
VPRORVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPRORVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Rotate quadwords in ymm2 right by count in the corresponding element of ymm3/m256/m64bcst, store result using writemask k1.
VPRORVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPRORVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Rotate quadwords in zmm2 right by count in the corresponding element of zmm3/m512/m64bcst, store result using writemask k1.
VPSADBW	XMM,XMM,XMM/M128	AVX	VPSADBW XMM1,XMM2,XMM3/M128	Computes the absolute differences of the packed unsigned byte integers from xmm3 /m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results.
VPSADBW	YMM,YMM,YMM/M256	AVX2	VPSADBW YMM1,YMM2,YMM3/M256	Computes the absolute differences of the packed unsigned byte integers from ymm3 /m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
VPSADBW	ZMM,ZMM,ZMM/M512	AVX512BW	VPSADBW ZMM1,ZMM2,ZMM3/M512	Computes the absolute differences of the packed unsigned byte integers from zmm3 /m512 and zmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results.
VPSCATTERDD	VM32X{K},XMM	AVX512VL,AVX512F	VPSCATTERDD VM32X{K1},XMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
VPSCATTERDD	VM32Y{K},YMM	AVX512VL,AVX512F	VPSCATTERDD VM32Y{K1},YMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
VPSCATTERDD	VM32Z{K},ZMM	AVX512F	VPSCATTERDD VM32Z{K1},ZMM1	Using signed dword indices, scatter dword values to memory using writemask k1.
VPSCATTERDQ	VM32X{K},XMM	AVX512VL,AVX512F	VPSCATTERDQ VM32X{K1},XMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
VPSCATTERDQ	VM32X{K},YMM	AVX512VL,AVX512F	VPSCATTERDQ VM32X{K1},YMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
VPSCATTERDQ	VM32Y{K},ZMM	AVX512F	VPSCATTERDQ VM32Y{K1},ZMM1	Using signed dword indices, scatter qword values to memory using writemask k1.
VPSCATTERQD	VM64X{K},XMM	AVX512VL,AVX512F	VPSCATTERQD VM64X{K1},XMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
VPSCATTERQD	VM64Y{K},XMM	AVX512VL,AVX512F	VPSCATTERQD VM64Y{K1},XMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
VPSCATTERQD	VM64Z{K},YMM	AVX512F	VPSCATTERQD VM64Z{K1},YMM1	Using signed qword indices, scatter dword values to memory using writemask k1.
VPSCATTERQQ	VM64X{K},XMM	AVX512VL,AVX512F	VPSCATTERQQ VM64X{K1},XMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
VPSCATTERQQ	VM64Y{K},YMM	AVX512VL,AVX512F	VPSCATTERQQ VM64Y{K1},YMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
VPSCATTERQQ	VM64Z{K},ZMM	AVX512F	VPSCATTERQQ VM64Z{K1},ZMM1	Using signed qword indices, scatter qword values to memory using writemask k1.
VPSHUFB	XMM,XMM,XMM/M128	AVX	VPSHUFB XMM1,XMM2,XMM3/M128	Shuffle bytes in xmm2 according to contents of xmm3/m128.
VPSHUFB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSHUFB XMM1{K1}{Z},XMM2,XMM3/M128	Shuffle bytes in xmm2 according to contents of xmm3/m128 under write mask k1.
VPSHUFB	YMM,YMM,YMM/M256	AVX2	VPSHUFB YMM1,YMM2,YMM3/M256	Shuffle bytes in ymm2 according to contents of ymm3/m256.
VPSHUFB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSHUFB YMM1{K1}{Z},YMM2,YMM3/M256	Shuffle bytes in ymm2 according to contents of ymm3/m256 under write mask k1. zmm3/m512 under write mask k1.
VPSHUFD	XMM,XMM/M128,IMM8	AVX	VPSHUFD XMM1,XMM2/M128,IMM8	Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFD	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPSHUFD XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Shuffle the doublewords in xmm2/m128/m32bcst based on the encoding in imm8 and store the result in xmm1 using writemask k1.
VPSHUFD	YMM,YMM/M256,IMM8	AVX2	VPSHUFD YMM1,YMM2/M256,IMM8	Shuffle the doublewords in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFD	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPSHUFD YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Shuffle the doublewords in ymm2/m256/m32bcst based on the encoding in imm8 and store the result in ymm1 using writemask k1.
VPSHUFD	ZMM{K}{Z},ZMM/M512/M32BCST,IMM8	AVX512F	VPSHUFD ZMM1{K1}{Z},ZMM2/M512/M32BCST,IMM8	Shuffle the doublewords in zmm2/m512/m32bcst based on the encoding in imm8 and store the result in zmm1 using writemask k1.
VPSHUFHW	XMM,XMM/M128,IMM8	AVX	VPSHUFHW XMM1,XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFHW	XMM{K}{Z},XMM/M128,IMM8	AVX512VL,AVX512BW	VPSHUFHW XMM1{K1}{Z},XMM2/M128,IMM8	Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1.
VPSHUFHW	YMM,YMM/M256,IMM8	AVX2	VPSHUFHW YMM1,YMM2/M256,IMM8	Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFHW	YMM{K}{Z},YMM/M256,IMM8	AVX512VL,AVX512BW	VPSHUFHW YMM1{K1}{Z},YMM2/M256,IMM8	Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1. on the encoding in imm8 and store the result in zmm1 under write mask k1.
VPSHUFLW	XMM,XMM/M128,IMM8	AVX	VPSHUFLW XMM1,XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1.
VPSHUFLW	XMM{K}{Z},XMM/M128,IMM8	AVX512VL,AVX512BW	VPSHUFLW XMM1{K1}{Z},XMM2/M128,IMM8	Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1.
VPSHUFLW	YMM,YMM/M256,IMM8	AVX2	VPSHUFLW YMM1,YMM2/M256,IMM8	Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1.
VPSHUFLW	YMM{K}{Z},YMM/M256,IMM8	AVX512VL,AVX512BW	VPSHUFLW YMM1{K1}{Z},YMM2/M256,IMM8	Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1.
VPSHUFLW	ZMM{K}{Z},ZMM/M512,IMM8	AVX512BW	VPSHUFLW ZMM1{K1}{Z},ZMM2/M512,IMM8	Shuffle the low words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1.
VPSIGNB	XMM,XMM,XMM/M128	AVX	VPSIGNB XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGNB	YMM,YMM,YMM/M256	AVX2	VPSIGNB YMM1,YMM2,YMM3/M256	Negate packed byte integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
VPSIGND	XMM,XMM,XMM/M128	AVX	VPSIGND XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGND	YMM,YMM,YMM/M256	AVX2	VPSIGND YMM1,YMM2,YMM3/M256	Negate packed doubleword integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
VPSIGNW	XMM,XMM,XMM/M128	AVX	VPSIGNW XMM1,XMM2,XMM3/M128	Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128.
VPSIGNW	YMM,YMM,YMM/M256	AVX2	VPSIGNW YMM1,YMM2,YMM3/M256	Negate packed 16-bit integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero.
VPSLLD	XMM,XMM,IMM8	AVX	VPSLLD XMM1,XMM2,IMM8	Shift doublewords in xmm2 left by imm8 while shifting in 0s.
VPSLLD	XMM,XMM,XMM/M128	AVX	VPSLLD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLDQ	XMM,XMM,IMM8	AVX	VPSLLDQ XMM1,XMM2,IMM8	Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1.
VPSLLDQ	XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPSLLDQ XMM1,XMM2/M128,IMM8	Shift xmm2/m128 left by imm8 bytes while shifting in 0s and store result in xmm1.
VPSLLDQ	YMM,YMM,IMM8	AVX2	VPSLLDQ YMM1,YMM2,IMM8	Shift ymm2 left by imm8 bytes while shifting in 0s and store result in ymm1.
VPSLLDQ	YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPSLLDQ YMM1,YMM2/M256,IMM8	Shift ymm2/m256 left by imm8 bytes while shifting in 0s and store result in ymm1.
VPSLLDQ	ZMM,ZMM/M512,IMM8	AVX512BW	VPSLLDQ ZMM1,ZMM2/M512,IMM8	Shift zmm2/m512 left by imm8 bytes while shifting in 0s and store result in zmm1.
VPSLLQ	XMM,XMM,IMM8	AVX	VPSLLQ XMM1,XMM2,IMM8	Shift quadwords in xmm2 left by imm8 while shifting in 0s.
VPSLLQ	XMM,XMM,XMM/M128	AVX	VPSLLQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLVD	XMM,XMM,XMM/M128	AVX2	VPSLLVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSLLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPSLLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1.
VPSLLVD	YMM,YMM,YMM/M256	AVX2	VPSLLVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSLLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPSLLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1.
VPSLLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPSLLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1.
VPSLLVQ	XMM,XMM,XMM/M128	AVX2	VPSLLVQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSLLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPSLLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1.
VPSLLVQ	YMM,YMM,YMM/M256	AVX2	VPSLLVQ YMM1,YMM2,YMM3/M256	Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSLLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPSLLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1.
VPSLLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPSLLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1.
VPSLLVW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSLLVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1.
VPSLLVW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSLLVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1.
VPSLLVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPSLLVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 left by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1.
VPSLLW	XMM,XMM,IMM8	AVX	VPSLLW XMM1,XMM2,IMM8	Shift words in xmm2 left by imm8 while shifting in 0s.
VPSLLW	XMM,XMM,XMM/M128	AVX	VPSLLW XMM1,XMM2,XMM3/M128	Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSLLW	YMM,YMM,IMM8	AVX2	VPSLLW YMM1,YMM2,IMM8	Shift words in ymm2 left by imm8 while shifting in 0s.
VPSLLW	YMM,YMM,XMM/M128	AVX2	VPSLLW YMM1,YMM2,XMM3/M128	Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s.
VPSRAD	XMM,XMM,IMM8	AVX	VPSRAD XMM1,XMM2,IMM8	Shift doublewords in xmm2 right by imm8 while shifting in sign bits.
VPSRAD	XMM,XMM,XMM/M128	AVX	VPSRAD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAD	YMM,YMM,IMM8	AVX2	VPSRAD YMM1,YMM2,IMM8	Shift doublewords in ymm2 right by imm8 while shifting in sign bits.
VPSRAD	YMM,YMM,XMM/M128	AVX2	VPSRAD YMM1,YMM2,XMM3/M128	Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAVD	XMM,XMM,XMM/M128	AVX2	VPSRAVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits.
VPSRAVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPSRAVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in sign bits using writemask k1.
VPSRAVD	YMM,YMM,YMM/M256	AVX2	VPSRAVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits.
VPSRAVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPSRAVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in sign bits using writemask k1.
VPSRAVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPSRAVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in sign bits using writemask k1.
VPSRAVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPSRAVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in sign bits using writemask k1.
VPSRAVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPSRAVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in sign bits using writemask k1.
VPSRAVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPSRAVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in sign bits using writemask k1.
VPSRAVW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSRAVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAVW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSRAVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits using writemask k1.
VPSRAVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPSRAVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in sign bits using writemask k1.
VPSRAW	XMM,XMM,IMM8	AVX	VPSRAW XMM1,XMM2,IMM8	Shift words in xmm2 right by imm8 while shifting in sign bits.
VPSRAW	XMM,XMM,XMM/M128	AVX	VPSRAW XMM1,XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSRAW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRAW	YMM,YMM,IMM8	AVX2	VPSRAW YMM1,YMM2,IMM8	Shift words in ymm2 right by imm8 while shifting in sign bits.
VPSRAW	YMM,YMM,XMM/M128	AVX2	VPSRAW YMM1,YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
VPSRAW	YMM{K}{Z},YMM,XMM/M128	AVX512VL,AVX512BW	VPSRAW YMM1{K1}{Z},YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
VPSRLD	XMM,XMM,IMM8	AVX	VPSRLD XMM1,XMM2,IMM8	Shift doublewords in xmm2 right by imm8 while shifting in 0s.
VPSRLD	XMM,XMM,XMM/M128	AVX	VPSRLD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLDQ	XMM,XMM,IMM8	AVX	VPSRLDQ XMM1,XMM2,IMM8	Shift xmm2 right by imm8 bytes while shifting in 0s.
VPSRLDQ	XMM,XMM/M128,IMM8	AVX512VL,AVX512BW	VPSRLDQ XMM1,XMM2/M128,IMM8	Shift xmm2/m128 right by imm8 bytes while shifting in 0s and store result in xmm1.
VPSRLDQ	YMM,YMM,IMM8	AVX2	VPSRLDQ YMM1,YMM2,IMM8	Shift ymm1 right by imm8 bytes while shifting in 0s.
VPSRLDQ	YMM,YMM/M256,IMM8	AVX512VL,AVX512BW	VPSRLDQ YMM1,YMM2/M256,IMM8	Shift ymm2/m256 right by imm8 bytes while shifting in 0s and store result in ymm1.
VPSRLDQ	ZMM,ZMM/M512,IMM8	AVX512BW	VPSRLDQ ZMM1,ZMM2/M512,IMM8	Shift zmm2/m512 right by imm8 bytes while shifting in 0s and store result in zmm1.
VPSRLQ	XMM,XMM,IMM8	AVX	VPSRLQ XMM1,XMM2,IMM8	Shift quadwords in xmm2 right by imm8 while shifting in 0s.
VPSRLQ	XMM,XMM,XMM/M128	AVX	VPSRLQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLVD	XMM,XMM,XMM/M128	AVX2	VPSRLVD XMM1,XMM2,XMM3/M128	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSRLVD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPSRLVD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1.
VPSRLVD	YMM,YMM,YMM/M256	AVX2	VPSRLVD YMM1,YMM2,YMM3/M256	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSRLVD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPSRLVD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1.
VPSRLVD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPSRLVD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1.
VPSRLVQ	XMM,XMM,XMM/M128	AVX2	VPSRLVQ XMM1,XMM2,XMM3/M128	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.
VPSRLVQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPSRLVQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1.
VPSRLVQ	YMM,YMM,YMM/M256	AVX2	VPSRLVQ YMM1,YMM2,YMM3/M256	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.
VPSRLVQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPSRLVQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1.
VPSRLVQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPSRLVQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1.
VPSRLVW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSRLVW XMM1{K1}{Z},XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1.
VPSRLVW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSRLVW YMM1{K1}{Z},YMM2,YMM3/M256	Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1.
VPSRLVW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPSRLVW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1.
VPSRLW	XMM,XMM,IMM8	AVX	VPSRLW XMM1,XMM2,IMM8	Shift words in xmm2 right by imm8 while shifting in 0s.
VPSRLW	XMM,XMM,XMM/M128	AVX	VPSRLW XMM1,XMM2,XMM3/M128	Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSRLW	YMM,YMM,IMM8	AVX2	VPSRLW YMM1,YMM2,IMM8	Shift words in ymm2 right by imm8 while shifting in 0s.
VPSRLW	YMM,YMM,XMM/M128	AVX2	VPSRLW YMM1,YMM2,XMM3/M128	Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s.
VPSUBB	XMM,XMM,XMM/M128	AVX	VPSUBB XMM1,XMM2,XMM3/M128	Subtract packed byte integers in xmm3/m128 from xmm2.
VPSUBB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed byte integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1.
VPSUBB	YMM,YMM,YMM/M256	AVX2	VPSUBB YMM1,YMM2,YMM3/M256	Subtract packed byte integers in ymm3/m256 from ymm2.
VPSUBB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed byte integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1.
VPSUBB	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPSUBB ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed byte integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1.
VPSUBD	XMM,XMM,XMM/M128	AVX	VPSUBD XMM1,XMM2,XMM3/M128	Subtract packed doubleword integers in xmm3/m128 from xmm2.
VPSUBD	YMM,YMM,YMM/M256	AVX2	VPSUBD YMM1,YMM2,YMM3/M256	Subtract packed doubleword integers in ymm3/m256 from ymm2.
VPSUBQ	XMM,XMM,XMM/M128	AVX	VPSUBQ XMM1,XMM2,XMM3/M128	Subtract packed quadword integers in xmm3/m128 from xmm2.
VPSUBQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPSUBQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Subtract packed quadword integers in xmm3/m128/m64bcst from xmm2 and store in xmm1 using writemask k1.
VPSUBQ	YMM,YMM,YMM/M256	AVX2	VPSUBQ YMM1,YMM2,YMM3/M256	Subtract packed quadword integers in ymm3/m256 from ymm2.
VPSUBQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPSUBQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Subtract packed quadword integers in ymm3/m256/m64bcst from ymm2 and store in ymm1 using writemask k1.
VPSUBQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPSUBQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Subtract packed quadword integers in zmm3/m512/m64bcst from zmm2 and store in zmm1 using writemask k1.
VPSUBSB	XMM,XMM,XMM/M128	AVX	VPSUBSB XMM1,XMM2,XMM3/M128	Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results.
VPSUBSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBSB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBSB	YMM,YMM,YMM/M256	AVX2	VPSUBSB YMM1,YMM2,YMM3/M256	Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results.
VPSUBSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBSB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results and store in ymm1 using writemask k1. zmm3/m512 from packed signed byte integers in zmm2 and saturate results and store in zmm1 using writemask k1.
VPSUBSW	XMM,XMM,XMM/M128	AVX	VPSUBSW XMM1,XMM2,XMM3/M128	Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results.
VPSUBSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBSW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBSW	YMM,YMM,YMM/M256	AVX2	VPSUBSW YMM1,YMM2,YMM3/M256	Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results.
VPSUBSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBSW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results and store in ymm1 using writemask k1.
VPSUBUSB	XMM,XMM,XMM/M128	AVX	VPSUBUSB XMM1,XMM2,XMM3/M128	Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result.
VPSUBUSB	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBUSB XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2, saturate results and store in xmm1 using writemask k1.
VPSUBUSB	YMM,YMM,YMM/M256	AVX2	VPSUBUSB YMM1,YMM2,YMM3/M256	Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result.
VPSUBUSB	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBUSB YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2, saturate results and store in ymm1 using writemask k1. zmm3/m512 from packed unsigned byte integers in zmm2, saturate results and store in zmm1 using writemask k1.
VPSUBUSW	XMM,XMM,XMM/M128	AVX	VPSUBUSW XMM1,XMM2,XMM3/M128	Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result.
VPSUBUSW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBUSW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate results and store in xmm1 using writemask k1.
VPSUBUSW	YMM,YMM,YMM/M256	AVX2	VPSUBUSW YMM1,YMM2,YMM3/M256	Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result.
VPSUBUSW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBUSW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2, saturate results and store in ymm1 using writemask k1.
VPSUBW	XMM,XMM,XMM/M128	AVX	VPSUBW XMM1,XMM2,XMM3/M128	Subtract packed word integers in xmm3/m128 from xmm2.
VPSUBW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPSUBW XMM1{K1}{Z},XMM2,XMM3/M128	Subtract packed word integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1.
VPSUBW	YMM,YMM,YMM/M256	AVX2	VPSUBW YMM1,YMM2,YMM3/M256	Subtract packed word integers in ymm3/m256 from ymm2.
VPSUBW	YMM{K}{Z},YMM,YMM/M256	AVX512VL,AVX512BW	VPSUBW YMM1{K1}{Z},YMM2,YMM3/M256	Subtract packed word integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1.
VPSUBW	ZMM{K}{Z},ZMM,ZMM/M512	AVX512BW	VPSUBW ZMM1{K1}{Z},ZMM2,ZMM3/M512	Subtract packed word integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1.
VPTERNLOGD	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VPTERNLOGD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m32bcst as source operands and writing the result to xmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGD	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VPTERNLOGD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m32bcst as source operands and writing the result to ymm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VPTERNLOGD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m32bcst as source operands and writing the result to zmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGQ	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VPTERNLOGQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m64bcst as source operands and writing the result to xmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGQ	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VPTERNLOGQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m64bcst as source operands and writing the result to ymm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
VPTERNLOGQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VPTERNLOGQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m64bcst as source operands and writing the result to zmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented.
VPTEST	XMM,XMM/M128	AVX	VPTEST XMM1,XMM2/M128	Set ZF and CF depending on bitwise AND and ANDN of sources.
VPTEST	YMM,YMM/M256	AVX	VPTEST YMM1,YMM2/M256	Set ZF and CF depending on bitwise AND and ANDN of sources.
VPTESTMB	K{K},XMM,XMM/M128	AVX512VL,AVX512BW	VPTESTMB K2{K1},XMM2,XMM3/M128	Bitwise AND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMB	K{K},YMM,YMM/M256	AVX512VL,AVX512BW	VPTESTMB K2{K1},YMM2,YMM3/M256	Bitwise AND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMB	K{K},ZMM,ZMM/M512	AVX512BW	VPTESTMB K2{K1},ZMM2,ZMM3/M512	Bitwise AND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMD	K{K},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPTESTMD K2{K1},XMM2,XMM3/M128/M32BCST	Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMD	K{K},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPTESTMD K2{K1},YMM2,YMM3/M256/M32BCST	Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMD	K{K},ZMM,ZMM/M512/M32BCST	AVX512F	VPTESTMD K2{K1},ZMM2,ZMM3/M512/M32BCST	Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMQ	K{K},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPTESTMQ K2{K1},XMM2,XMM3/M128/M64BCST	Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMQ	K{K},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPTESTMQ K2{K1},YMM2,YMM3/M256/M64BCST	Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512F	VPTESTMQ K2{K1},ZMM2,ZMM3/M512/M64BCST	Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMW	K{K},XMM,XMM/M128	AVX512VL,AVX512BW	VPTESTMW K2{K1},XMM2,XMM3/M128	Bitwise AND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMW	K{K},YMM,YMM/M256	AVX512VL,AVX512BW	VPTESTMW K2{K1},YMM2,YMM3/M256	Bitwise AND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTMW	K{K},ZMM,ZMM/M512	AVX512BW	VPTESTMW K2{K1},ZMM2,ZMM3/M512	Bitwise AND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMB	K{K},XMM,XMM/M128	AVX512VL,AVX512BW	VPTESTNMB K2{K1},XMM2,XMM3/M128	Bitwise NAND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMB	K{K},YMM,YMM/M256	AVX512VL,AVX512BW	VPTESTNMB K2{K1},YMM2,YMM3/M256	Bitwise NAND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMB	K{K},ZMM,ZMM/M512	AVX512F,AVX512BW	VPTESTNMB K2{K1},ZMM2,ZMM3/M512	Bitwise NAND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMD	K{K},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPTESTNMD K2{K1},XMM2,XMM3/M128/M32BCST	Bitwise NAND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMD	K{K},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPTESTNMD K2{K1},YMM2,YMM3/M256/M32BCST	Bitwise NAND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMD	K{K},ZMM,ZMM/M512/M32BCST	AVX512F	VPTESTNMD K2{K1},ZMM2,ZMM3/M512/M32BCST	Bitwise NAND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMQ	K{K},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPTESTNMQ K2{K1},XMM2,XMM3/M128/M64BCST	Bitwise NAND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMQ	K{K},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPTESTNMQ K2{K1},YMM2,YMM3/M256/M64BCST	Bitwise NAND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMQ	K{K},ZMM,ZMM/M512/M64BCST	AVX512F	VPTESTNMQ K2{K1},ZMM2,ZMM3/M512/M64BCST	Bitwise NAND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMW	K{K},XMM,XMM/M128	AVX512VL,AVX512BW	VPTESTNMW K2{K1},XMM2,XMM3/M128	Bitwise NAND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMW	K{K},YMM,YMM/M256	AVX512VL,AVX512BW	VPTESTNMW K2{K1},YMM2,YMM3/M256	Bitwise NAND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPTESTNMW	K{K},ZMM,ZMM/M512	AVX512F,AVX512BW	VPTESTNMW K2{K1},ZMM2,ZMM3/M512	Bitwise NAND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1.
VPUNPCKHBW	XMM,XMM,XMM/M128	AVX	VPUNPCKHBW XMM1,XMM2,XMM3/M128	Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHBW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPUNPCKHBW XMM1{K1}{Z},XMM2,XMM3/M128	Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1 register using k1 write mask.
VPUNPCKHBW	YMM,YMM,YMM/M256	AVX2	VPUNPCKHBW YMM1,YMM2,YMM3/M256	Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKHDQ XMM1,XMM2,XMM3/M128	Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHDQ	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPUNPCKHDQ XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Interleave high-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register using k1 write mask.
VPUNPCKHDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKHDQ YMM1,YMM2,YMM3/M256	Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHQDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKHQDQ XMM1,XMM2,XMM3/M128	Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VPUNPCKHQDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKHQDQ YMM1,YMM2,YMM3/M256	Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKHWD	XMM,XMM,XMM/M128	AVX	VPUNPCKHWD XMM1,XMM2,XMM3/M128	Interleave high-order words from xmm2 and xmm3/m128 into xmm1.
VPUNPCKHWD	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPUNPCKHWD XMM1{K1}{Z},XMM2,XMM3/M128	Interleave high-order words from xmm2 and xmm3/m128 into xmm1 register using k1 write mask.
VPUNPCKHWD	YMM,YMM,YMM/M256	AVX2	VPUNPCKHWD YMM1,YMM2,YMM3/M256	Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLBW	XMM,XMM,XMM/M128	AVX	VPUNPCKLBW XMM1,XMM2,XMM3/M128	Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLBW	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPUNPCKLBW XMM1{K1}{Z},XMM2,XMM3/M128	Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
VPUNPCKLBW	YMM,YMM,YMM/M256	AVX2	VPUNPCKLBW YMM1,YMM2,YMM3/M256	Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKLDQ XMM1,XMM2,XMM3/M128	Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLDQ	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPUNPCKLDQ XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1.
VPUNPCKLDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKLDQ YMM1,YMM2,YMM3/M256	Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLQDQ	XMM,XMM,XMM/M128	AVX	VPUNPCKLQDQ XMM1,XMM2,XMM3/M128	Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register.
VPUNPCKLQDQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPUNPCKLQDQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.
VPUNPCKLQDQ	YMM,YMM,YMM/M256	AVX2	VPUNPCKLQDQ YMM1,YMM2,YMM3/M256	Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register.
VPUNPCKLWD	XMM,XMM,XMM/M128	AVX	VPUNPCKLWD XMM1,XMM2,XMM3/M128	Interleave low-order words from xmm2 and xmm3/m128 into xmm1.
VPUNPCKLWD	XMM{K}{Z},XMM,XMM/M128	AVX512VL,AVX512BW	VPUNPCKLWD XMM1{K1}{Z},XMM2,XMM3/M128	Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1.
VPUNPCKLWD	YMM,YMM,YMM/M256	AVX2	VPUNPCKLWD YMM1,YMM2,YMM3/M256	Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register.
VPXOR	XMM,XMM,XMM/M128	AVX	VPXOR XMM1,XMM2,XMM3/M128	Bitwise XOR of xmm3/m128 and xmm2.
VPXOR	YMM,YMM,YMM/M256	AVX2	VPXOR YMM1,YMM2,YMM3/M256	Bitwise XOR of ymm3/m256 and ymm2.
VPXORD	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VPXORD XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Bitwise XOR of packed doubleword integers in xmm2 and xmm3/m128 using writemask k1.
VPXORD	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VPXORD YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Bitwise XOR of packed doubleword integers in ymm2 and ymm3/m256 using writemask k1.
VPXORD	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VPXORD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Bitwise XOR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1.
VPXORQ	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VPXORQ XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Bitwise XOR of packed quadword integers in xmm2 and xmm3/m128 using writemask k1.
VPXORQ	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VPXORQ YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Bitwise XOR of packed quadword integers in ymm2 and ymm3/m256 using writemask k1.
VPXORQ	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VPXORQ ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Bitwise XOR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1.
VRANGEPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512DQ	VRANGEPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Calculate two RANGE operation output value from 2 pairs of double-precision floating-point values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512DQ	VRANGEPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Calculate four RANGE operation output value from 4pairs of double-precision floating-point values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{SAE},IMM8	AVX512DQ	VRANGEPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{SAE},IMM8	Calculate eight RANGE operation output value from 8 pairs of double-precision floating-point values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512DQ	VRANGEPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Calculate four RANGE operation output value from 4 pairs of single-precision floating-point values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512DQ	VRANGEPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Calculate eight RANGE operation output value from 8 pairs of single-precision floating-point values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGEPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{SAE},IMM8	AVX512DQ	VRANGEPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{SAE},IMM8	Calculate 16 RANGE operation output value from 16 pairs of single-precision floating-point values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation.
VRANGESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512DQ	VRANGESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Calculate a RANGE operation output value from 2 double-precision floating-point values in xmm2 and xmm3/m64, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation.
VRANGESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512DQ	VRANGESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Calculate a RANGE operation output value from 2 single-precision floating-point values in xmm2 and xmm3/m32, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation.
VRCP14PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VRCP14PD XMM1{K1}{Z},XMM2/M128/M64BCST	Computes the approximate reciprocals of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask.
VRCP14PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VRCP14PD YMM1{K1}{Z},YMM2/M256/M64BCST	Computes the approximate reciprocals of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask.
VRCP14PD	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512F	VRCP14PD ZMM1{K1}{Z},ZMM2/M512/M64BCST	Computes the approximate reciprocals of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask.
VRCP14PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VRCP14PS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask.
VRCP14PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VRCP14PS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes the approximate reciprocals of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask.
VRCP14PS	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512F	VRCP14PS ZMM1{K1}{Z},ZMM2/M512/M32BCST	Computes the approximate reciprocals of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
VRCP14SD	XMM{K}{Z},XMM,XMM/M64	AVX512F	VRCP14SD XMM1{K1}{Z},XMM2,XMM3/M64	Computes the approximate reciprocal of the scalar double-precision floating-point value in xmm3/m64 and stores the result in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VRCP14SS	XMM{K}{Z},XMM,XMM/M32	AVX512F	VRCP14SS XMM1{K1}{Z},XMM2,XMM3/M32	Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the results in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:32]) from xmm2 is copied to xmm1[127:32].
VRCP28PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512ER	VRCP28PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes the approximate reciprocals ( &lt; 2^-28 relative error) of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask.
VRCP28PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512ER	VRCP28PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes the approximate reciprocals ( &lt; 2^-28 relative error) of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
VRCP28SD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512ER	VRCP28SD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Computes the approximate reciprocal ( &lt; 2^-28 relative error) of the scalar double-precision floating-point value in xmm3/m64 and stores the results in xmm1. Under writemask. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VRCP28SS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512ER	VRCP28SS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Computes the approximate reciprocal ( &lt; 2^-28 relative error) of the scalar single-precision floating-point value in xmm3/m32 and stores the results in xmm1. Under writemask. Also, upper 3 single-precision floating-point values (bits[127:32]) from xmm2 is copied to xmm1[127:32].
VRCPPS	XMM,XMM/M128	AVX	VRCPPS XMM1,XMM2/M128	Computes the approximate reciprocals of packed single-precision values in xmm2/mem and stores the results in xmm1.
VRCPPS	YMM,YMM/M256	AVX	VRCPPS YMM1,YMM2/M256	Computes the approximate reciprocals of packed single-precision values in ymm2/mem and stores the results in ymm1.
VRCPSS	XMM,XMM,XMM/M32	AVX	VRCPSS XMM1,XMM2,XMM3/M32	Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VREDUCEPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512DQ	VREDUCEPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Perform reduction transformation on packed double-precision floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1.
VREDUCEPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512DQ	VREDUCEPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Perform reduction transformation on packed double-precision floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1.
VREDUCEPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512DQ	VREDUCEPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Perform reduction transformation on double-precision floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1.
VREDUCEPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512DQ	VREDUCEPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Perform reduction transformation on packed single-precision floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1.
VREDUCEPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512DQ	VREDUCEPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Perform reduction transformation on packed single-precision floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1.
VREDUCEPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512DQ	VREDUCEPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Perform reduction transformation on packed single-precision floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1.
VREDUCESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512DQ	VREDUCESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Perform a reduction transformation on a scalar double-precision floating point value in xmm3/m64 by subtracting a number of fraction bits specified by the imm8 field. Also, upper double precision floating-point value (bits[127:64]) from xmm2 are copied to xmm1[127:64]. Stores the result in xmm1 register.
VREDUCESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512DQ	VREDUCESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Perform a reduction transformation on a scalar single-precision floating point value in xmm3/m32 by subtracting a number of fraction bits specified by the imm8 field. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]. Stores the result in xmm1 register.
VRNDSCALEPD	XMM{K}{Z},XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VRNDSCALEPD XMM1{K1}{Z},XMM2/M128/M64BCST,IMM8	Rounds packed double-precision floating point values in xmm2/m128/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask.
VRNDSCALEPD	YMM{K}{Z},YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VRNDSCALEPD YMM1{K1}{Z},YMM2/M256/M64BCST,IMM8	Rounds packed double-precision floating point values in ymm2/m256/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask.
VRNDSCALEPD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE},IMM8	AVX512F	VRNDSCALEPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE},IMM8	Rounds packed double-precision floating-point values in zmm2/m512/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask k1.
VRNDSCALEPS	XMM{K}{Z},XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VRNDSCALEPS XMM1{K1}{Z},XMM2/M128/M32BCST,IMM8	Rounds packed single-precision floating point values in xmm2/m128/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask.
VRNDSCALEPS	YMM{K}{Z},YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VRNDSCALEPS YMM1{K1}{Z},YMM2/M256/M32BCST,IMM8	Rounds packed single-precision floating point values in ymm2/m256/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask.
VRNDSCALEPS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE},IMM8	AVX512F	VRNDSCALEPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE},IMM8	Rounds packed single-precision floating-point values in zmm2/m512/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask.
VRNDSCALESD	XMM{K}{Z},XMM,XMM/M64{SAE},IMM8	AVX512F	VRNDSCALESD XMM1{K1}{Z},XMM2,XMM3/M64{SAE},IMM8	Rounds scalar double-precision floating-point value in xmm3/m64 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register.
VRNDSCALESS	XMM{K}{Z},XMM,XMM/M32{SAE},IMM8	AVX512F	VRNDSCALESS XMM1{K1}{Z},XMM2,XMM3/M32{SAE},IMM8	Rounds scalar single-precision floating-point value in xmm3/m32 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask.
VROUNDPD	XMM,XMM/M128,IMM8	AVX	VROUNDPD XMM1,XMM2/M128,IMM8	Round packed double-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VROUNDPD	YMM,YMM/M256,IMM8	AVX	VROUNDPD YMM1,YMM2/M256,IMM8	Round packed double-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
VROUNDPS	XMM,XMM/M128,IMM8	AVX	VROUNDPS XMM1,XMM2/M128,IMM8	Round packed single-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8.
VROUNDPS	YMM,YMM/M256,IMM8	AVX	VROUNDPS YMM1,YMM2/M256,IMM8	Round packed single-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8.
VROUNDSD	XMM,XMM,XMM/M64,IMM8	AVX	VROUNDSD XMM1,XMM2,XMM3/M64,IMM8	Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VROUNDSS	XMM,XMM,XMM/M32,IMM8	AVX	VROUNDSS XMM1,XMM2,XMM3/M32,IMM8	Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VRSQRT14PD	XMM{K}{Z},XMM/M128/M64BCST	AVX512VL,AVX512F	VRSQRT14PD XMM1{K1}{Z},XMM2/M128/M64BCST	Computes the approximate reciprocal square roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask.
VRSQRT14PD	YMM{K}{Z},YMM/M256/M64BCST	AVX512VL,AVX512F	VRSQRT14PD YMM1{K1}{Z},YMM2/M256/M64BCST	Computes the approximate reciprocal square roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask.
VRSQRT14PD	ZMM{K}{Z},ZMM/M512/M64BCST	AVX512F	VRSQRT14PD ZMM1{K1}{Z},ZMM2/M512/M64BCST	Computes the approximate reciprocal square roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1 under writemask.
VRSQRT14PS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VRSQRT14PS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes the approximate reciprocal square roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask.
VRSQRT14PS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VRSQRT14PS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes the approximate reciprocal square roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask.
VRSQRT14PS	ZMM{K}{Z},ZMM/M512/M32BCST	AVX512F	VRSQRT14PS ZMM1{K1}{Z},ZMM2/M512/M32BCST	Computes the approximate reciprocal square roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask.
VRSQRT14SD	XMM{K}{Z},XMM,XMM/M64	AVX512F	VRSQRT14SD XMM1{K1}{Z},XMM2,XMM3/M64	Computes the approximate reciprocal square root of the scalar double-precision floating-point value in xmm3/m64 and stores the result in the low quadword element of xmm1 using writemask k1. Bits[127:64] of xmm2 is copied to xmm1[127:64].
VRSQRT14SS	XMM{K}{Z},XMM,XMM/M32	AVX512F	VRSQRT14SS XMM1{K1}{Z},XMM2,XMM3/M32	Computes the approximate reciprocal square root of the scalar single-precision floating-point value in xmm3/m32 and stores the result in the low doubleword element of xmm1 using writemask k1. Bits[127:32] of xmm2 is copied to xmm1[127:32].
VRSQRT28PD	ZMM{K}{Z},ZMM/M512/M64BCST{SAE}	AVX512ER	VRSQRT28PD ZMM1{K1}{Z},ZMM2/M512/M64BCST{SAE}	Computes approximations to the Reciprocal square root (&lt;2^-28 relative error) of the packed double-precision floating-point values from zmm2/m512/m64bcst and stores result in zmm1with writemask k1.
VRSQRT28PS	ZMM{K}{Z},ZMM/M512/M32BCST{SAE}	AVX512ER	VRSQRT28PS ZMM1{K1}{Z},ZMM2/M512/M32BCST{SAE}	Computes approximations to the Reciprocal square root (&lt;2^-28 relative error) of the packed single-precision floating-point values from zmm2/m512/m32bcst and stores result in zmm1with writemask k1.
VRSQRT28SD	XMM{K}{Z},XMM,XMM/M64{SAE}	AVX512ER	VRSQRT28SD XMM1{K1}{Z},XMM2,XMM3/M64{SAE}	Computes approximate reciprocal square root (&lt;2^-28 relative error) of the scalar double-precision floating-point value from xmm3/m64 and stores result in xmm1with writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VRSQRT28SS	XMM{K}{Z},XMM,XMM/M32{SAE}	AVX512ER	VRSQRT28SS XMM1{K1}{Z},XMM2,XMM3/M32{SAE}	Computes approximate reciprocal square root (&lt;2^-28 relative error) of the scalar single-precision floating-point value from xmm3/m32 and stores result in xmm1with writemask k1. Also, upper 3 single-precision floating-point value (bits[127:32]) from xmm2 is copied to xmm1[127:32].
VRSQRTPS	XMM,XMM/M128	AVX	VRSQRTPS XMM1,XMM2/M128	Computes the approximate reciprocals of the square roots of packed single-precision values in xmm2/mem and stores the results in xmm1.
VRSQRTPS	YMM,YMM/M256	AVX	VRSQRTPS YMM1,YMM2/M256	Computes the approximate reciprocals of the square roots of packed single-precision values in ymm2/mem and stores the results in ymm1.
VRSQRTSS	XMM,XMM,XMM/M32	AVX	VRSQRTSS XMM1,XMM2,XMM3/M32	Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VSCALEFPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VSCALEFPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Scale the packed double-precision floating-point values in xmm2 using values from xmm3/m128/m64bcst. Under writemask k1.
VSCALEFPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VSCALEFPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Scale the packed double-precision floating-point values in ymm2 using values from ymm3/m256/m64bcst. Under writemask k1.
VSCALEFPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VSCALEFPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Scale the packed double-precision floating-point values in zmm2 using values from zmm3/m512/m64bcst. Under writemask k1.
VSCALEFPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VSCALEFPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Scale the packed single-precision floating-point values in xmm2 using values from xmm3/m128/m32bcst. Under writemask k1.
VSCALEFPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VSCALEFPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Scale the packed single-precision values in ymm2 using floating point values from ymm3/m256/m32bcst. Under writemask k1.
VSCALEFPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VSCALEFPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Scale the packed single-precision floating-point values in zmm2 using floating-point values from zmm3/m512/m32bcst. Under writemask k1.
VSCALEFSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VSCALEFSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Scale the scalar double-precision floating-point values in xmm2 using the value from xmm3/m64. Under writemask k1.
VSCALEFSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VSCALEFSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Scale the scalar single-precision floating-point value in xmm2 using floating-point value from xmm3/m32. Under writemask k1.
VSCATTERDPD	VM32X{K},XMM	AVX512VL,AVX512F	VSCATTERDPD VM32X{K1},XMM1	Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERDPD	VM32X{K},YMM	AVX512VL,AVX512F	VSCATTERDPD VM32X{K1},YMM1	Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERDPD	VM32Y{K},ZMM	AVX512F	VSCATTERDPD VM32Y{K1},ZMM1	Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERDPS	VM32X{K},XMM	AVX512VL,AVX512F	VSCATTERDPS VM32X{K1},XMM1	Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1.
VSCATTERDPS	VM32Y{K},YMM	AVX512VL,AVX512F	VSCATTERDPS VM32Y{K1},YMM1	Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1.
VSCATTERDPS	VM32Z{K},ZMM	AVX512F	VSCATTERDPS VM32Z{K1},ZMM1	Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1.
VSCATTERPF0DPD	VM32Y{K}	AVX512PF	VSCATTERPF0DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T0 hint with intent to write.
VSCATTERPF0DPS	VM32Z{K}	AVX512PF	VSCATTERPF0DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T0 hint with intent to write.
VSCATTERPF0QPD	VM64Z{K}	AVX512PF	VSCATTERPF0QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T0 hint with intent to write.
VSCATTERPF0QPS	VM64Z{K}	AVX512PF	VSCATTERPF0QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T0 hint with intent to write.
VSCATTERPF1DPD	VM32Y{K}	AVX512PF	VSCATTERPF1DPD VM32Y{K1}	Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T1 hint with intent to write.
VSCATTERPF1DPS	VM32Z{K}	AVX512PF	VSCATTERPF1DPS VM32Z{K1}	Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T1 hint with intent to write.
VSCATTERPF1QPD	VM64Z{K}	AVX512PF	VSCATTERPF1QPD VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T1 hint with intent to write.
VSCATTERPF1QPS	VM64Z{K}	AVX512PF	VSCATTERPF1QPS VM64Z{K1}	Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T1 hint with intent to write.
VSCATTERQPD	VM64X{K},XMM	AVX512VL,AVX512F	VSCATTERQPD VM64X{K1},XMM1	Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERQPD	VM64Y{K},YMM	AVX512VL,AVX512F	VSCATTERQPD VM64Y{K1},YMM1	Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERQPD	VM64Z{K},ZMM	AVX512F	VSCATTERQPD VM64Z{K1},ZMM1	Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1.
VSCATTERQPS	VM64X{K},XMM	AVX512VL,AVX512F	VSCATTERQPS VM64X{K1},XMM1	Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1.
VSCATTERQPS	VM64Y{K},XMM	AVX512VL,AVX512F	VSCATTERQPS VM64Y{K1},XMM1	Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1.
VSCATTERQPS	VM64Z{K},YMM	AVX512F	VSCATTERQPS VM64Z{K1},YMM1	Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1.
VSHUFF32x4	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VSHUFF32x4 YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shuffle 128-bit packed single-precision floating-point values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1.
VSHUFF32x4	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VSHUFF32x4 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shuffle 128-bit packed single-precision floating-point values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1.
VSHUFF64x2	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VSHUFF64x2 YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle 128-bit packed double-precision floating-point values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1.
VSHUFF64x2	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VSHUFF64x2 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle 128-bit packed double-precision floating-point values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1.
VSHUFI32x4	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VSHUFI32x4 YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Shuffle 128-bit packed double-word values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1.
VSHUFI32x4	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VSHUFI32x4 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Shuffle 128-bit packed double-word values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1.
VSHUFI64x2	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VSHUFI64x2 YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle 128-bit packed quad-word values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1.
VSHUFI64x2	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VSHUFI64x2 ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle 128-bit packed quad-word values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1.
VSHUFPD	XMM,XMM,XMM/M128,IMM8	AVX	VSHUFPD XMM1,XMM2,XMM3/M128,IMM8	Shuffle two pairs of double-precision floating-point values from xmm2 and xmm3/m128 using imm8 to select from each pair, interleaved result is stored in xmm1.
VSHUFPD	XMM{K}{Z},XMM,XMM/M128/M64BCST,IMM8	AVX512VL,AVX512F	VSHUFPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST,IMM8	Shuffle two paris of double-precision floating-point values from xmm2 and xmm3/m128/m64bcst using imm8 to select from each pair. store interleaved results in xmm1 subject to writemask k1.
VSHUFPD	YMM,YMM,YMM/M256,IMM8	AVX	VSHUFPD YMM1,YMM2,YMM3/M256,IMM8	Shuffle four pairs of double-precision floating-point values from ymm2 and ymm3/m256 using imm8 to select from each pair, interleaved result is stored in xmm1.
VSHUFPD	YMM{K}{Z},YMM,YMM/M256/M64BCST,IMM8	AVX512VL,AVX512F	VSHUFPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST,IMM8	Shuffle four paris of double-precision floating-point values from ymm2 and ymm3/m256/m64bcst using imm8 to select from each pair. store interleaved results in ymm1 subject to writemask k1.
VSHUFPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST,IMM8	AVX512F	VSHUFPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST,IMM8	Shuffle eight paris of double-precision floating-point values from zmm2 and zmm3/m512/m64bcst using imm8 to select from each pair. store interleaved results in zmm1 subject to writemask k1.
VSHUFPS	XMM,XMM,XMM/M128,IMM8	AVX	VSHUFPS XMM1,XMM2,XMM3/M128,IMM8	Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1.
VSHUFPS	XMM{K}{Z},XMM,XMM/M128/M32BCST,IMM8	AVX512VL,AVX512F	VSHUFPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST,IMM8	Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1, subject to writemask k1.
VSHUFPS	YMM,YMM,YMM/M256,IMM8	AVX	VSHUFPS YMM1,YMM2,YMM3/M256,IMM8	Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1.
VSHUFPS	YMM{K}{Z},YMM,YMM/M256/M32BCST,IMM8	AVX512VL,AVX512F	VSHUFPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST,IMM8	Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1, subject to writemask k1.
VSHUFPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST,IMM8	AVX512F	VSHUFPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST,IMM8	Select from quadruplet of single-precision floating-point values in zmm2 and zmm3/m512 using imm8, interleaved result pairs are stored in zmm1, subject to writemask k1.
VSQRTPD	XMM,XMM/M128	AVX	VSQRTPD XMM1,XMM2/M128	Computes Square Roots of the packed double-precision floating-point values in xmm2/m128 and stores the result in xmm1.
VSQRTPD	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VSQRTPD XMM1{K1}{Z},XMM2/M128/M32BCST	Computes Square Roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the result in xmm1 subject to writemask k1.
VSQRTPD	YMM,YMM/M256	AVX	VSQRTPD YMM1,YMM2/M256	Computes Square Roots of the packed double-precision floating-point values in ymm2/m256 and stores the result in ymm1.
VSQRTPD	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VSQRTPD YMM1{K1}{Z},YMM2/M256/M32BCST	Computes Square Roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the result in ymm1 subject to writemask k1.
VSQRTPD	ZMM{K}{Z},ZMM/M512/M64BCST{ER}	AVX512F	VSQRTPD ZMM1{K1}{Z},ZMM2/M512/M64BCST{ER}	Computes Square Roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the result in zmm1 subject to writemask k1.
VSQRTPS	XMM,XMM/M128	AVX	VSQRTPS XMM1,XMM2/M128	Computes Square Roots of the packed single-precision floating-point values in xmm2/m128 and stores the result in xmm1.
VSQRTPS	XMM{K}{Z},XMM/M128/M32BCST	AVX512VL,AVX512F	VSQRTPS XMM1{K1}{Z},XMM2/M128/M32BCST	Computes Square Roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the result in xmm1 subject to writemask k1.
VSQRTPS	YMM,YMM/M256	AVX	VSQRTPS YMM1,YMM2/M256	Computes Square Roots of the packed single-precision floating-point values in ymm2/m256 and stores the result in ymm1.
VSQRTPS	YMM{K}{Z},YMM/M256/M32BCST	AVX512VL,AVX512F	VSQRTPS YMM1{K1}{Z},YMM2/M256/M32BCST	Computes Square Roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the result in ymm1 subject to writemask k1.
VSQRTPS	ZMM{K}{Z},ZMM/M512/M32BCST{ER}	AVX512F	VSQRTPS ZMM1{K1}{Z},ZMM2/M512/M32BCST{ER}	Computes Square Roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the result in zmm1 subject to writemask k1.
VSQRTSD	XMM,XMM,XMM/M64	AVX	VSQRTSD XMM1,XMM2,XMM3/M64	Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VSQRTSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VSQRTSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1 under writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64].
VSQRTSS	XMM,XMM,XMM/M32	AVX	VSQRTSS XMM1,XMM2,XMM3/M32	Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VSQRTSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VSQRTSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1 under writemask k1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32].
VSTMXCSR	M32	AVX	VSTMXCSR M32	Store contents of MXCSR register to m32.
VSUBPD	XMM,XMM,XMM/M128	AVX	VSUBPD XMM1,XMM2,XMM3/M128	Subtract packed double-precision floating-point values in xmm3/mem from xmm2 and store result in xmm1.
VSUBPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VSUBPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Subtract packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1.
VSUBPD	YMM,YMM,YMM/M256	AVX	VSUBPD YMM1,YMM2,YMM3/M256	Subtract packed double-precision floating-point values in ymm3/mem from ymm2 and store result in ymm1.
VSUBPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VSUBPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Subtract packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1.
VSUBPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST{ER}	AVX512F	VSUBPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST{ER}	Subtract packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1.
VSUBPS	XMM,XMM,XMM/M128	AVX	VSUBPS XMM1,XMM2,XMM3/M128	Subtract packed single-precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1.
VSUBPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VSUBPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Subtract packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and stores result in xmm1 with writemask k1.
VSUBPS	YMM,YMM,YMM/M256	AVX	VSUBPS YMM1,YMM2,YMM3/M256	Subtract packed single-precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1.
VSUBPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VSUBPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Subtract packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and stores result in ymm1 with writemask k1.
VSUBPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST{ER}	AVX512F	VSUBPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST{ER}	Subtract packed single-precision floating-point values in zmm3/m512/m32bcst from zmm2 and stores result in zmm1 with writemask k1.
VSUBSD	XMM,XMM,XMM/M64	AVX	VSUBSD XMM1,XMM2,XMM3/M64	Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1.
VSUBSD	XMM{K}{Z},XMM,XMM/M64{ER}	AVX512F	VSUBSD XMM1{K1}{Z},XMM2,XMM3/M64{ER}	Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1 under writemask k1.
VSUBSS	XMM,XMM,XMM/M32	AVX	VSUBSS XMM1,XMM2,XMM3/M32	Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1.
VSUBSS	XMM{K}{Z},XMM,XMM/M32{ER}	AVX512F	VSUBSS XMM1{K1}{Z},XMM2,XMM3/M32{ER}	Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1 under writemask k1.
VTESTPD	XMM,XMM/M128	AVX	VTESTPD XMM1,XMM2/M128	Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources.
VTESTPD	YMM,YMM/M256	AVX	VTESTPD YMM1,YMM2/M256	Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources.
VTESTPS	XMM,XMM/M128	AVX	VTESTPS XMM1,XMM2/M128	Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources.
VTESTPS	YMM,YMM/M256	AVX	VTESTPS YMM1,YMM2/M256	Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources.
VUCOMISD	XMM,XMM/M64	AVX	VUCOMISD XMM1,XMM2/M64	Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly.
VUCOMISD	XMM,XMM/M64{SAE}	AVX512F	VUCOMISD XMM1,XMM2/M64{SAE}	Compare low double-precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS flags accordingly.
VUCOMISS	XMM,XMM/M32	AVX	VUCOMISS XMM1,XMM2/M32	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VUCOMISS	XMM,XMM/M32{SAE}	AVX512F	VUCOMISS XMM1,XMM2/M32{SAE}	Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly.
VUNPCKHPD	XMM,XMM,XMM/M128	AVX	VUNPCKHPD XMM1,XMM2,XMM3/M128	Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VUNPCKHPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VUNPCKHPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VUNPCKHPD	YMM,YMM,YMM/M256	AVX	VUNPCKHPD YMM1,YMM2,YMM3/M256	Unpacks and Interleaves double-precision floating-point values from high quadwords of ymm2 and ymm3/m256.
VUNPCKHPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VUNPCKHPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VUNPCKHPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VUNPCKHPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Unpacks and Interleaves double-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m64bcst subject to writemask k1.
VUNPCKHPS	XMM,XMM,XMM/M128	AVX	VUNPCKHPS XMM1,XMM2,XMM3/M128	Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128.
VUNPCKHPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VUNPCKHPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128/m32bcst and write result to xmm1 subject to writemask k1.
VUNPCKHPS	YMM,YMM,YMM/M256	AVX	VUNPCKHPS YMM1,YMM2,YMM3/M256	Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256.
VUNPCKHPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VUNPCKHPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256/m32bcst and write result to ymm1 subject to writemask k1.
VUNPCKHPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VUNPCKHPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Unpacks and Interleaves single-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to writemask k1.
VUNPCKLPD	XMM,XMM,XMM/M128	AVX	VUNPCKLPD XMM1,XMM2,XMM3/M128	Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm2 and xmm3/m128.
VUNPCKLPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512F	VUNPCKLPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Unpacks and Interleaves double precision floating-point values from low quadwords of xmm2 and xmm3/m128/m64bcst subject to write mask k1.
VUNPCKLPD	YMM,YMM,YMM/M256	AVX	VUNPCKLPD YMM1,YMM2,YMM3/M256	Unpacks and Interleaves double-precision floating-point values from low quadwords of ymm2 and ymm3/m256.
VUNPCKLPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512F	VUNPCKLPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Unpacks and Interleaves double precision floating-point values from low quadwords of ymm2 and ymm3/m256/m64bcst subject to write mask k1.
VUNPCKLPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512F	VUNPCKLPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Unpacks and Interleaves double-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m64bcst subject to write mask k1.
VUNPCKLPS	XMM,XMM,XMM/M128	AVX	VUNPCKLPS XMM1,XMM2,XMM3/M128	Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/m128.
VUNPCKLPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512F	VUNPCKLPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/mem and write result to xmm1 subject to write mask k1.
VUNPCKLPS	YMM,YMM,YMM/M256	AVX	VUNPCKLPS YMM1,YMM2,YMM3/M256	Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/m256.
VUNPCKLPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512F	VUNPCKLPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/mem and write result to ymm1 subject to write mask k1.
VUNPCKLPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512F	VUNPCKLPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Unpacks and Interleaves single-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to write mask k1.
VXORPD	XMM,XMM,XMM/M128	AVX	VXORPD XMM1,XMM2,XMM3/M128	Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/mem.
VXORPD	XMM{K}{Z},XMM,XMM/M128/M64BCST	AVX512VL,AVX512DQ	VXORPD XMM1{K1}{Z},XMM2,XMM3/M128/M64BCST	Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1.
VXORPD	YMM,YMM,YMM/M256	AVX	VXORPD YMM1,YMM2,YMM3/M256	Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/mem.
VXORPD	YMM{K}{Z},YMM,YMM/M256/M64BCST	AVX512VL,AVX512DQ	VXORPD YMM1{K1}{Z},YMM2,YMM3/M256/M64BCST	Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1.
VXORPD	ZMM{K}{Z},ZMM,ZMM/M512/M64BCST	AVX512DQ	VXORPD ZMM1{K1}{Z},ZMM2,ZMM3/M512/M64BCST	Return the bitwise logical XOR of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1.
VXORPS	XMM,XMM,XMM/M128	AVX	VXORPS XMM1,XMM2,XMM3/M128	Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/mem.
VXORPS	XMM{K}{Z},XMM,XMM/M128/M32BCST	AVX512VL,AVX512DQ	VXORPS XMM1{K1}{Z},XMM2,XMM3/M128/M32BCST	Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1.
VXORPS	YMM,YMM,YMM/M256	AVX	VXORPS YMM1,YMM2,YMM3/M256	Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/mem.
VXORPS	YMM{K}{Z},YMM,YMM/M256/M32BCST	AVX512VL,AVX512DQ	VXORPS YMM1{K1}{Z},YMM2,YMM3/M256/M32BCST	Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1.
VXORPS	ZMM{K}{Z},ZMM,ZMM/M512/M32BCST	AVX512DQ	VXORPS ZMM1{K1}{Z},ZMM2,ZMM3/M512/M32BCST	Return the bitwise logical XOR of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1.
VZEROALL		AVX	VZEROALL 	Zero all YMM registers.
VZEROUPPER		AVX	VZEROUPPER 	Zero upper 128 bits of all YMM registers.
WAIT			WAIT 	Check pending unmasked floating-point exceptions.
WBINVD			WBINVD 	Write back and flush Internal caches; initiate writing-back and flushing of external caches.
WRFSBASE	R32	FSGSBASE	WRFSBASE R32	Load the FS base address with the 32-bit value in the source register.
WRFSBASE	R64	FSGSBASE	WRFSBASE R64	Load the FS base address with the 64-bit value in the source register.
WRGSBASE	R32	FSGSBASE	WRGSBASE R32	Load the GS base address with the 32-bit value in the source register.
WRGSBASE	R64	FSGSBASE	WRGSBASE R64	Load the GS base address with the 64-bit value in the source register.
WRMSR			WRMSR 	Write the value in EDX:EAX to MSR specified by ECX.
WRPKRU			WRPKRU 	Writes EAX into PKRU.
XABORT	IMM8	RTM	XABORT IMM8	Causes an RTM abort if in RTM execution
XACQUIRE		HLE	XACQUIRE 	A hint used with an “XACQUIRE-enabled“ instruction to start lock elision on the instruction memory operand address.
XADD	R/M16,R16		XADD R/M16,R16	Exchange r16 and r/m16; load sum into r/m16.
XADD	R/M32,R32		XADD R/M32,R32	Exchange r32 and r/m32; load sum into r/m32.
XADD	R/M64,R64		XADD R/M64,R64	Exchange r64 and r/m64; load sum into r/m64.
XADD	R/M8,R8		XADD R/M8,R8	Exchange r8 and r/m8; load sum into r/m8.
XBEGIN	REL16	RTM	XBEGIN REL16	Specifies the start of an RTM region. Provides a 16-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
XBEGIN	REL32	RTM	XBEGIN REL32	Specifies the start of an RTM region. Provides a 32-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort.
XCHG	AX,R16		XCHG AX,R16	Exchange r16 with AX.
XCHG	EAX,R32		XCHG EAX,R32	Exchange r32 with EAX.
XCHG	R/M16,R16		XCHG R/M16,R16	Exchange r16 with word from r/m16.
XCHG	R/M32,R32		XCHG R/M32,R32	Exchange r32 with doubleword from r/m32.
XCHG	R/M64,R64		XCHG R/M64,R64	Exchange r64 with quadword from r/m64.
XCHG	R/M8,R8		XCHG R/M8,R8	Exchange r8 (byte register) with byte from r/m8.
XCHG	R16,AX		XCHG R16,AX	Exchange AX with r16.
XCHG	R16,R/M16		XCHG R16,R/M16	Exchange word from r/m16 with r16.
XCHG	R32,EAX		XCHG R32,EAX	Exchange EAX with r32.
XCHG	R32,R/M32		XCHG R32,R/M32	Exchange doubleword from r/m32 with r32.
XCHG	R64,R/M64		XCHG R64,R/M64	Exchange quadword from r/m64 with r64.
XCHG	R64,RAX		XCHG R64,RAX	Exchange RAX with r64.
XCHG	R8,R/M8		XCHG R8,R/M8	Exchange byte from r/m8 with r8 (byte register).
XCHG	RAX,R64		XCHG RAX,R64	Exchange r64 with RAX.
XEND		RTM	XEND 	Specifies the end of an RTM code region.
XGETBV			XGETBV 	Reads an XCR specified by ECX into EDX:EAX.
XLAT	M8		XLAT M8	Set AL to memory byte DS:[(E)BX + unsigned AL].
XLATB			XLATB 	Set AL to memory byte DS:[(E)BX + unsigned AL].
XOR	AL,IMM8		XOR AL,IMM8	AL XOR imm8.
XOR	AX,IMM16		XOR AX,IMM16	AX XOR imm16.
XOR	EAX,IMM32		XOR EAX,IMM32	EAX XOR imm32.
XOR	R/M16,IMM16		XOR R/M16,IMM16	r/m16 XOR imm16.
XOR	R/M16,IMM8		XOR R/M16,IMM8	r/m16 XOR imm8 (sign-extended).
XOR	R/M16,R16		XOR R/M16,R16	r/m16 XOR r16.
XOR	R/M32,IMM32		XOR R/M32,IMM32	r/m32 XOR imm32.
XOR	R/M32,IMM8		XOR R/M32,IMM8	r/m32 XOR imm8 (sign-extended).
XOR	R/M32,R32		XOR R/M32,R32	r/m32 XOR r32.
XOR	R/M64,IMM32		XOR R/M64,IMM32	r/m64 XOR imm32 (sign-extended).
XOR	R/M64,IMM8		XOR R/M64,IMM8	r/m64 XOR imm8 (sign-extended).
XOR	R/M64,R64		XOR R/M64,R64	r/m64 XOR r64.
XOR	R/M8,IMM8		XOR R/M8,IMM8	r/m8 XOR imm8.
XOR	R/M8,R8		XOR R/M8,R8	r/m8 XOR r8.
XOR	R16,R/M16		XOR R16,R/M16	r16 XOR r/m16.
XOR	R32,R/M32		XOR R32,R/M32	r32 XOR r/m32.
XOR	R64,R/M64		XOR R64,R/M64	r64 XOR r/m64.
XOR	R8,R/M8		XOR R8,R/M8	r8 XOR r/m8.
XOR	RAX,IMM32		XOR RAX,IMM32	RAX XOR imm32 (sign-extended).
XORPS	XMM,XMM/M128	SSE	XORPS XMM1,XMM2/M128	Return the bitwise logical XOR of packed single-precision floating-point values in xmm1 and xmm2/mem.
XRELEASE		HLE	XRELEASE 	A hint used with an “XRELEASE-enabled“ instruction to end lock elision on the instruction memory operand address.
XRSTOR	MEM		XRSTOR MEM	Restore state components specified by EDX:EAX from mem.
XRSTORS	MEM		XRSTORS MEM	Restore state components specified by EDX:EAX from mem.
XSAVE	MEM		XSAVE MEM	Save state components specified by EDX:EAX to mem.
XSAVEC	MEM		XSAVEC MEM	Save state components specified by EDX:EAX to mem with compaction.
XSAVEOPT	MEM	XSAVEOPT	XSAVEOPT MEM	Save state components specified by EDX:EAX to mem, optimizing if possible.
XSAVES	MEM		XSAVES MEM	Save state components specified by EDX:EAX to mem with compaction, optimizing if possible.
XSETBV			XSETBV 	Write the value in EDX:EAX to the XCR specified by ECX.
XTEST		HLE,RTM	XTEST 	Test if executing in a transactional region
